\mainmatter

# (PART) Foundational ideas {-}

# Some important facts about distributions

In linguistics and psychology, typical data-sets involve either *discrete* dependent measures such as acceptability ratings on a Likert scale (for example, ranging from 1 to 7), and binary grammaticality judgements, or *continuous* dependent measures such as reading times or reaction times in milliseconds and EEG signals in microvolts. 

Whenever we fit a model using one of these types of dependent measures, we make some assumptions about how these measurements were generated. In particular, we usually assume that our observed measurements are coming from a particular *distribution*. The normal distribution is an example that may be familiar to the reader.  

In this chapter, we will learn how to make explicit the assumptions about the distribution associated with out data; we will also learn to visualize distributions. In order to do this, we need to understand the concept of a random variable, and some very basic notions of probability theory. As will become apparent in this chapter, it is extremely useful to be able to think about data in terms of the underlying random variable producing the data. We consider the two cases, discrete and continuous, separately.

We will explain the terms *random variable* and *distribution* below through examples. But it is useful to define the notion of random variable formally.

A random variable, which will be denoted by a variable such as $Y$, is defined as a function from a sample space of possible outcomes $S$ to the real number system: 

\begin{equation}
Y : S \rightarrow \mathbb{R}
\end{equation}

The random variable associates to each outcome $\omega$ in the sample space $S$ ($\omega \in S$) exactly one number $Y(\omega) = y$. $S_Y$ will represent a set that contains all the $y$'s (all the possible values of $Y$, which we call the support of $Y$). We can compactly write: $y \in S_Y$. 

Every random variable $Y$ has associated with it a probability mass (distribution)  function (PMF, PDF). I.e., PMF is used for discrete distributions and PDF for continuous distributions. The PMF/PDF maps every element of $S_Y$ to a value between 0 and 1.

\begin{equation}
p_Y : S_Y \rightarrow [0, 1] 
\end{equation}

Probability mass functions (discrete case) and probability density functions (continuous case) are functions that assign probabilities (discrete case) or the relative likelihood (continuous case) to all events in a sample space.

The meanings of the terms PMF, PDF, likelihood, etc., will become clear as we discuss examples below. The reader to should revisit the above definition themselves when we discuss concrete examples of random variables below.



## Discrete random variables: An example using the Binomial distribution

Imagine that our data come from a grammaticality judgement task (participants see or hear sentences and have to decide whether these are grammatical or ungrammatical), and that the responses from participants are  a sequence of 1's and 0's, where 1 represents the judgment "grammatical", and 0 represents the judgement "ungrammatical". Assume also that each response, coded as 1 or 0, is generated independently from the others. We can simulate the outcome of such an experiment (i.e., a sequence of 1s and 0s) in R. Let's generate the outcome of 20 such experiments with a sample size of 10 (i.e., each experiment provides us with 10 responses). For each experiment, we count the number of 1s (or number of "successes").

```{r echo=FALSE}
rbinom(10,n=20,prob=0.5)
```

Outcomes such as this one (i.e., number of successes for a variable with two possible outcomes)  follow a probability distribution p(Y). In more technical terms, we can say that the number of successes in each of the $20$ simulated experiments above is being generated by a *discrete random variable* $Y$ which has associated with it a probability distribution $p(Y)$ called the **Binomial distribution**.^[When an experiment consists of only a single trial (i.e., we can have a total number of only 0 or 1 successes), $p(Y)$ is called a **Bernoulli distribution**.]   

For discrete random variable, the probability distribution $p(Y)$ is called a **probability mass function** (PMF). The PMF defines the probability of each possible outcome. In the above example, with $n=10$ trials, there are 11 possible outcomes:  $0,\dots,10$ successes. Which of these outcomes is most probable depends on a numerical value called a *parameter* in the Binomial distribution that represents the probability of success. We will call this parameter $\theta$.  The left-hand side plot in Figure \@ref(fig:binomplot) shows an example of a Binomial PMF with $10$ trials and the parameter $\theta$ with value $0.5$. Setting $\theta$ to 0.5 leads to a PMF where the most probable outcome is 5 successes out of 10. If we had set $\theta$ to, say 0.1, then the most probable outcome would be 1 success out of 10; and if we had set $\theta$ to 0.9, then the most probable outcome would be 9 successes out of 10.

As we will see later, when we analyze data, a primary goal will be to compute a so-called *estimate* of the parameter (here, $\theta$). In real-life situations, the value of the $\theta$ parameter will be unknown and unknowable; the data will allow us to compute a "guess" about the unknown value of of the parameter. We will call this guess the estimate of the parameter. 

```{r, binomplot,echo=FALSE,fig.cap="Probability mass functions of a binomial distribution assuming 10 trials, with 50%, 10%, and 90% probability of success." }
op<-par(mfrow=c(1,3),pty="s")
plot(0:10,dbinom(0:10,size=10,prob=0.5),
     xlab="possible outcomes",
     ylab="probability",
     main=expression(paste(theta,"=0.5",sep="")))
plot(0:10,dbinom(0:10,size=10,prob=0.1),xlab="possible outcomes",
     ylab="probability",main=expression(paste(theta,"=0.1",sep="")))
plot(0:10,dbinom(0:10,size=10,prob=0.9),xlab="possible outcomes",
     ylab="probability",
     main=expression(paste(theta,"=0.9",sep="")))
```

```{block2, type='rmdnote'}
to-do bar or line graphs above, instead of points
```



The probability mass function for the binomial is written as follows. 

\begin{equation}
\hbox{Binomial}(k|n,\theta) = 
\binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}

Here, $n$ represents the total number of trials, $k$ the number of successes, and $\theta$ the probability of success. The term $\binom{n}{k}$, pronounced n-choose-k, represents the number of ways in which one can choose $k$ successes out of $n$ trials. For example, 1 success out of 10 can occur in 10 possible ways: the very first trial could be a 1, the secone trial could be a 1, etc.
The term $\binom{n}{k}$ expands to $\frac{n!}{k!(n-k)!}$. In `R`, it is computed using the function `choose(n,k)`, with $n$ and $k$ representing positive integer values.

### The mean and variance of the Binomial distribution

It is possible to analytically compute the mean and variance of the PMF associated with the Binomial random variable $Y$. Without getting into the details of how these are derived mathematically, we just state here that the mean of $Y$ (also called the expectation, conventionally written $E[Y]$) and variance of $Y$ (written $Var(Y)$) of a Binomial distribution with parameter $\theta$ and $n$ trials are $E[Y] = n\theta$ and $Var(Y) = n\theta (1-\theta)$, respectively. 

Of course, we always know $n$ (because we decide on the number of trials ourselves), but in real experimental situations we never know the true value of $\theta$. But $\theta$ can be estimated from the data. From the observed data, we can compute the estimate of $\theta$, $\hat \theta=k/n$. The quantity $\hat \theta$ is the observed proportion of successes, and is called the **maximum likelihood estimate** of the true (but unknown mean). Once we have estimated $\theta$ in this way, we can also obtain an estimate (also a maximum likelihood estimate) of the variance by computing $n\hat\theta (1-\hat\theta)$. These estimates are then used for statistical inference. 

What does the term "maximum likelihood estimate" mean? The term **likelihood** refers to the value of the Binomial distribution function  for a particular value of $\theta$, once we have observed some data. For example, suppose you record $n=10$ trials, and observe $k=7$ successes. What is the probability of observing $7$ successes out of $10$? We need the binomial distribution to compute this value:

\begin{equation}
\hbox{Binomial}(k=7|n=10,\theta) = 
\binom{10}{7} \theta^{7} (1-\theta)^{10-7}
\end{equation}

Once we have observed the data, both $n$ and $k$ are fixed. The only variable in the above equation now is $\theta$: the above function is now only dependent on the value of $\theta$. When the data are fixed, the probability mass function is only dependent on the value of the parameter $\theta$, and is called a **likelihood function**. It is therefore often expressed as a function of $\theta$:

$p( y | \theta ) = p( k=7, n=10 | \theta) = \mathcal{L}(\theta)$

The vertical bar notation above should be read as saying that, given some data $y$ (which in  the binomial case will be $k$ "successes" in $n$ trials), the function returns a value for different values of $\theta$. 

If we now plot this function for all possible values of $\theta$, we get the plot shown in Figure \@ref(fig:binomlik).

```{r, binomlik,echo=FALSE,fig.cap="The likelihood function for 7 successes out of 10." }
theta<-seq(0,1,by=0.001)
plot(theta,dbinom(7,size=10,prob=theta),
     xlab=expression("theta"),ylab="probability",
     main="Likelihood function",type="l")
abline(v=0.7)
text(0.7,0.1,"Max. value at: \n 0.7")
```

```{block2, type='rmdnote'}
DS comment: do we want to show the code for computing all likelihood values? (maybe this comes later?)
```


What is important about this plot is that it shows that, given the data, the maximum point is at the point $0.7$, which corresponds to the estimated mean using the formula shown above: $k/n = 7/10$. Thus, the maximum likelihood estimate (MLE) gives us the most likely value of the parameter $\theta$ given the data. It is crucial to note here that the phrase "most likely" does not mean that the MLE from a *particular* sample of data invariably gives us an accurate estimate of $\theta$. For example, if we run our experiment for $10$ trials and get $1$ success out of $10$, the MLE is $0.10$. We could have happened to observe only one success out of ten even if the true $\theta$ were $0.5$. The MLE would however give an accurate estimate of the true parameter $\theta$ as $n$ approaches infinity.

### What information does a probability distribution provide?

What good is a probability mass function? We consider this question next.

#### Compute the probability of a particular outcome (discrete case only)

The Binomial distribution shown in Figure \@ref(fig:binomplot) already shows the probability of each possible outcome under a different value for $\theta$. In R, there is a built-in function that allows us to calculate the probability of $k$ successes out of $n$, given a particular value of $k$ (this number constitutes our data), the number of trials $n$, and given a particular value of $\theta$; this is the ```dbinom``` function. For example, the probability of 5 successes out of 10 when $\theta$ is 0.5 is:

```{r dnormexample1,echo=TRUE}
dbinom(5,size=10,prob=0.5)
```

The probabilities of success when $\theta$ is 0.1 or 0.9 can be computed by replacing 0.5 above by each of these probabilities. One can just do this by giving ```dbinom``` a vector of probabilities:

```{r}
dbinom(5,size=10,prob=c(0.1,0.9))
```

#### Compute the cumulative probability of k or less (more) than k successes 

Instead of the probability of obtaining a given number of successes we could be interested in knowing the cumulative probability of obtaining 1 or less, or 2 or less. We can obtain this information with the ```dbinom``` function, through a simple summation procedure:

```{r cdfbinom1,echo=TRUE}
## the cumulative probability of obtaining
## 0, 1, or 2 successes out of 10,
## with theta=0.5:
dbinom(0,size=10,prob=0.5)+dbinom(1,size=10,prob=0.5)+
  dbinom(2,size=10,prob=0.5)
```

Mathematically, we could write the above summation as: 

\begin{equation}
\sum_{k=0}^2 \binom{n}{k} \theta^{k} (1-\theta)^{n-k} 
\end{equation}

An alternative to the cumbersome addition in the R code above is this more compact statement, which closely mimics the above mathematical expression:

```{r}
sum(dbinom(0:2,size=10,prob=0.5))
```

R has a built-in function called ```pbinom``` that does this summation for us.  If we want to know the probability of $2$ or less successes as in the above example, we can write:

```{r pbinomexample1,echo=TRUE}
pbinom(2,size=10,prob=0.5,lower.tail=TRUE)
```

The specification ```lower.tail=TRUE``` ensures that the summation goes from $2$ to numbers smaller than $2$ (which lie in the lower tail of the distribution in Figure \@ref(fig:binomplot)). If we wanted to know what the probability is of obtaining $2$ or more successes out of $10$, we can set ```lower.tail``` to ```FALSE```:

```{r pbinomexample2,echo=TRUE}
pbinom(2,size=10,prob=0.5,lower.tail=FALSE)
```

The cumulative distribution function or CDF can be plotted by computing the cumulative probabilities for any value $k$ or less than $k$, where $k$ ranges from $0$ to $10$ in our running example. The CDF is shown in Figure \@ref(fig:binomcdf).


```{r, binomcdf,echo=FALSE,fig.cap="The cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success." }
#op<-par(mfrow=c(1,2),pty="s")
plot(0:10,pbinom(0:10,size=10,prob=0.5),
     xlab="Possible outcomes k",
     ylab="Prob. of k or less successes",
     main="Cumulative distribution function")
```

#### Compute the inverse of the cumulative distribution function (the quantile function)

We can also find out the value of the variable $k$ (the quantile) such that the probability of obtaining $k$ or less than $k$ successes is some specific probability value $p$. If we switch the x and y axes of Figure \@ref(fig:binomcdf), we obtain another very useful function, the inverse CDF. 

```{r, eval=FALSE,binominvcdf,echo=FALSE,fig.cap="The inverse cumulative distribution function for a binomial distribution assuming 10 trials, with 50% probability of success." }

plot(0:10~pbinom(0:10,size=10,prob=0.5),
     ylab="Possible outcomes k",
     xlab="Prob. of k or less successes",
     main="Cumulative distribution function")
```

The inverse of the CDF (known as the quantile function in R because it returns the quantile, the value k) is available in R as the function ```qbinom```. The usage is as follows: to find out what the value $k$ of the outcome is such that the probability of obtaining $k$ or less successes is $0.37$, type:

```{r qbinomexample,echo=TRUE}
qbinom(0.37,size=10,prob=0.5)
```

#### Generate random data from a $\hbox{Binomial}(n,\theta)$ distribution

We can generate random simulated data from a Binomial distribution by specifying the number of trials and the probability of success $\theta$. In `R`, we do this as follows:

```{r rbinomexample,echo=TRUE}
rbinom(10,size=1,prob=0.5)
```

The above code generates a sequences of $1$'s and $0$'s. Repeatedly run the above code; you will get different sequences each time. For each generated sequence, one can calculate the number of successes by just summing up the vector, or computing its mean and multiplying by the number of trials, here $10$:


```{r rbinomexamplemean,echo=TRUE}
y<-rbinom(10,size=1,prob=0.5)
mean(y)*10 ; sum(y)
```

## Continuous random variables: An example using the Normal distribution  

We will now revisit the idea of a random variable using a continuous distribution. Imagine that you have a vector of reading time data $y$ measured in milliseconds and coming from (i.e., generated by) a Normal distribution. The so-called probability density function (PDF) of the Normal distribution is defined as follows:

\begin{equation}
Normal(y|\mu,\sigma)=f(y)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{(y-\mu)^2}{2\sigma^2} \right)
\end{equation}

Here, $\mu$ is the mean, and $\sigma$ is the standard deviation of the Normal distribution that the reading times have been sampled from.

We can visualize the Normal distribution for particular values of $\mu$ and $\sigma$ as a PDF (using ```dnorm```), a CDF (using ```pnorm```), and the inverse CDF (using ```qnorm```). See Figure \@ref(fig:normdistrn). 

In the figure, the PDF gives us the so-called *density* for each possible value of our data $y$; "density" here is not the probability, rather is it giving us a non-negative number that is the value of the function $f(y)$ in the equation immediately above. We will return to the concept of density when we do an exercise on *maximum likelihood estimation*.

The CDF tells us the probability of observing a value like y or some value less than that (written: $P(Y<y)$); and the inverse CDF gives us the quantile $y$ such that $P(Y<y)$ is some specific value between 0 and 1. 

The PDF, CDF, and inverse CDF are three different ways of looking at the same information. 

```{r, normdistrn,echo=FALSE,fig.cap="The PDF, CDF, and inverse CDF for the $Normal(\\mu=0,\\sigma=1)$." }
op<-par(mfrow=c(1,3),pty="s")
plot(function(y) dnorm(y,mean=0,sd=1), -3, 3,
      main = "PDF of Y ~ Normal(0,1)",
              ylab="density",xlab="y")
plot(function(y) pnorm(y,mean=0,sd=1), -3, 3,
      main = "CDF of Y ~ Normal(0,1)",
              ylab="Prob(Y<y)",xlab="y")
plot(function(y) qnorm(y,mean=0,sd=1), 0, 1,
      main = "Inverse CDF of Y ~ Normal(0,1)",
              ylab="y",xlab="P(Y<y)")
```

One important fact about the normal distribution is that 95% of the probability mass is covered by approximately plus/minus 1.96 times the standard deviation about the mean. Thus,  the range $\mu\pm 1.96\times \sigma$ will cover approximately 95% of the area under the curve. We will approximate this by talking about $\mu\pm 2\times \sigma$.

As in the discrete example, the PDF, CDF, and inverse of the CDF allow us to ask questions like:

* **What is the probability of observing values between $a$ and $b$ from a Normal distribution with mean $\mu$ and standard deviation $\sigma$**? We can compute the probability of the random variable lying between 1 and minus infinity:

```{r pnormexample}
pnorm(1,mean=0,sd=1)-pnorm(-Inf,mean=0,sd=1)
```

```{r echo=FALSE}
## function for plotting area under curve:
plot.prob<-function(x,
                           x.min,
                           x.max,
                           prob,
                           mean,
                           sd,
                           gray.level,main){

        plot(x,dnorm(x,mean,sd), 
                     type = "l",xlab="",
             ylab="",main=main)
        abline(h = 0)

## shade X<x    
    x1 = seq(x.min, qnorm(prob), abs(prob)/5)
    y1 = dnorm(x1, mean, sd)

    polygon(c(x1, rev(x1)), 
            c(rep(0, length(x1)), rev(y1)), 
            col = gray.level)
}

shadenormal<- 
function (prob=0.5,
          gray1="black",
          x.min=-6,
          x.max=abs(x.min),
          x = seq(x.min, x.max, 0.01),
          mean=0,
          sd=1,main="P(X<0)") 
{

     plot.prob(x=x,x.min=x.min,x.max=x.max,
               prob=prob,
                      mean=mean,sd=sd,
     gray.level=gray1,main=main)     
}
```

```{r echo=FALSE}
shadenormal(prob=0.84134,main="P(X<1)")
```


Notice here that the probability of any point value in a PDF is always 0. This is because the probability in a continuous probability distribution is the area under the curve, and the area at any point on the x-axis is always 0. The implication here is that we can only ask about probabilities between two different points; e.g., the probability that $Y$ lies between $a$ and $b$, or $P(a<Y<b)$. Also, notice that $P(a<Y<b)$ and $P(a\leq Y\leq b)$ will be the same probability, because of the fact that $P(Y=a)$ or $P(Y=b)$ both equal 0.

* **What is the quantile $q$ such that the probability is $p$ of observing that value $q$ or something less (or more) than it**? For example, we can work out the quantile $q$ such that the probability of observing $q$ or something less than it is 0.975, in the Normal(500,100) distribution. Formally, we would write this as $P(Y<a)$.

```{r qnormexample}
qnorm(0.975,mean=500,sd=100)
```

The above output says that the probability that the random variable is less than $q=696$ is 97.5%.

* **Generating simulated data**. Given a vector of $n$ independent and identically distributed data $y$, i.e., given that each data point is being generated independently from  $Y \sim Normal(\mu,\sigma)$ for some values of the parameters $\mu,\sigma$, the maximum likelihood estimates for the expectation and variance are

\begin{equation}
\bar{y} =  \frac{\sum_{i=1}^n y_i}{n} 
\end{equation}

\begin{equation}
Var(y) = \frac{\sum_{i=1}^n (y_i-
\bar{y})^2}{n}
\end{equation}


Let's simulate some sample data from a Normal distribution and compute estimates of the mean and variance.   
Let's generate $10$ data points using the ```rnorm``` function, and then compute the mean and variance from the simulated data:

```{r rnormexample}
y<-rnorm(10,mean=500,sd=100)
mean(y);var(y)
```

Again, depending on the sample size, the sample mean and sample variance *from a particular sample* may or may not be close to the true values of the respective parameters, despite the fact that these are *maximum likelihood estimates*. 

One other important detail about probability distributions is that one can have a joint distribution defined for more than one random variable. We consider this situation next. 

## Other common distributions

Here, we briefly present some of the distributions that we will need or use in this book.

### The t-distribution

This continuous distribution, written $t(n-1)$, take as a parameter the degrees of freedom (roughly, the sample size) minus one.  As the degrees of freedom increase to infinity, the distribution approaches the Normal distribution with mean 0 and standard deviation 1.

The pdf is: 

\begin{equation}
f(x) = \frac{\Gamma((n+1)/2)}{\sqrt{n\pi} \Gamma(n/2)} (1+x^2/n)^{-(n+1)/2} \hbox{Support: } x \in (-\infty, \infty)
\end{equation}

Its mean is $0$ if $n>1$ and variance $\frac{n}{n-2}$ if $n>2$.

The t-distribution becomes the Cauchy distribution if $n=2$. The Cauchy distribution has  no mean or variance defined for it.

There are four functions in R that serve the same purpose as the dnorm, pnorm, qnorm, rnorm functions for the Normal:
dt, pt, qt, rt.


### Gamma dsitribution

The distribution is written $Gamma(a,\lambda)$, and takes two parameters, $a$ and $\lambda$. 

The pdf is: 

\begin{equation}
f(x) = \frac{1}{\Gamma(a)}(\lambda x)^ae^{-\lambda x}\frac{1}{x} \hbox{Support: } x \in (0, \infty)
\end{equation}

The mean is $\frac{a}{\lambda}$  and the variance is $\frac{a}{\lambda^2}$.

### Exponential 

The Exponential, written $Exp(\lambda)$, takes the parameter $\lambda$ and has the PDF:

\begin{equation}
f(x) = \lambda e^{-\lambda x} \hbox{Support: } x \in (0, \infty)
\end{equation}

Its mean is $\frac{1}{\lambda}$ and variance $\frac{1}{\lambda^2}$.

## Bivariate and multivariate distributions

So far, we have only discussed univariate distributions. It is also possible to specify distributions with two or more dimensions. 

Understanding bivariate (and, more generally, multivariate) distributions, and knowing how to simulate data from such distributions, is vital for us because linear mixed models crucially depend on such distributions. If we want to understand linear mixed models, we have to understand multivariate distributions.

### Example 1: Discrete bivariate distributions

Starting with the discrete case, consider the discrete bivariate distribution shown below. These are data from an experiment  where, inter alia, in each trial a Likert acceptability rating and a response accuracy (to a yes-no question) were recorded (the data are from a study by @AnnaLphd, used with permission here).

Figure \@ref(fig:bivardiscrete) shows the *joint probability mass function* of two random variables X and Y. The random variable X consists of 7 possible values (this is the 1-7 Likert response scale), and the random variable Y is response accuracy, with 0 representing incorrect responses, and 1 representing correct responses. 

```{r bivardiscrete,message="FALSE",warning=FALSE,results="asis",fig.cap="Example of a discrete bivariate distribution. In these data, in every trial, two pieces of information were collected: Likert responses and yes-no question responses. The random variable X represents Likert scale responses on a scale of 1-7. and the random variable Y represents 0, 1 (incorrect, correct) responses to comprehension questions."}
agrmt<-read.csv("data/agrmt_discrete_binomial.csv")
rating0<-table(subset(agrmt,accuracy==0)$rating)
rating1<-table(subset(agrmt,accuracy==1)$rating)

ratingsbivar<-data.frame(rating0=rating0,rating1=rating1)

ratingsbivar<-ratingsbivar[,c(2,4)]
colnames(ratingsbivar)<-c(0,1)
library (MASS)

## function from bivariate package:
f <- cbvpmf (ratingsbivar)
plot (f, TRUE,
       arrows=FALSE)
```

One can also display the figure as a table. 

```{r}
probs<-attr(f,"p")
t(probs)
```

For each possible value of X and Y, we have a joint probability. Given such a bivariate distribution, there are two useful quantities we can compute: the *marginal* distributions ($p_{X}$ and $p_Y$), and the *conditional* distributions ($p_{X|Y}$ and $p_{Y|X}$).  
The table below shows the joint probability mass function $p_{X,Y}(x,y)$.

\begin{table}[!htbp] 
\begin{center}
\begin{tabular}{c|ccccccc}
$p_{X,Y}$ & x=1 & x=2 & x=3 & x=4 & x=5 & x=6 & x=7\\
\hline
y = 0 & 0.018 & 0.023 & 0.040 & 0.043 & 0.063 & 0.049 & 0.055\\
y = 1 & 0.031 & 0.053 & 0.086 & 0.096 &  0.147 & 0.153 &  0.142\\
\end{tabular}
\end{center}
\caption{The joint PMF for two random variables $X$ and $Y$.}\label{discretebivartable}
\end{table}

The marginal  distribution $p_Y$ is defined as follows. $S_{X}$ is the support of X, i.e., all the possible values of X.

\begin{equation}
p_{Y}(y)=\sum_{x\in S_{X}}p_{X,Y}(x,y).\label{eq-marginal-pmf}
\end{equation}	

Similarly, the marginal distribution $p_X$ is defined as:

\begin{equation}
p_{X}(x)=\sum_{y\in S_{Y}}p_{X,Y}(x,y).\label{eq-marginal-pmf2}
\end{equation}	

$p_Y$ is easily computed, by summing up the rows; and $p_X$ by summing up the columns. You can see why this is called the marginal distribution; the result appears in the margins of the table.

```{r}
#P(Y)
(PY<-rowSums(t(probs)))
sum(PY) ## sums to 1
#P(X)
(PX<-colSums(t(probs)))
sum(PX) ## sums to 1
```

The marginal probabilities sum to 1, as they should. The table below shows the marginal probabilities.

\small
\begin{table}[!htbp]
\begin{center}
\begin{tabular}{c|ccccccc|c}
$p_{X,Y}$ & x=1 & x=2 & x=3 & x=4 & x=5 & x=6 & x=7 & P(Y)\\
\hline
y = 0 & 0.018 & 0.023 & 0.040 & 0.043 & 0.063 & 0.049 & 0.055 &  \textbf{0.291}\\
y = 1 & 0.031 & 0.053 & 0.086 & 0.096 &  0.147 & 0.153 &  0.142 &  \textbf{0.709}\\
\hline
P(X) & \textbf{0.049} & \textbf{0.077} & \textbf{0.126} & \textbf{0.139} & \textbf{0.210} & \textbf{0.202} & \textbf{0.197}\\
\end{tabular}
\end{center}
\caption{The joint and marginal distributions of X and Y.}\label{discretebivartable2}
\end{table}
\normalsize

Notice that to compute the marginal distribution of X, one is summing over all the Ys; and to compute the marginal distribution of Y, one sums over all the X's. We say that we are *marginalizing out* the random variable that we are summing over. One can visualize the two marginal distributions using barplots. 

```{r marginalprobs,fig.cap="Marginal distributions of X and Y.",echo=FALSE}
op<-par(mfrow=c(1,2),pty="s")
names(PX)<-1:7
barplot(PX,main="P(X)")
barplot(PY,main="P(Y)")
```

For computing conditional distributions, recall that the  conditional distribution of a random variable $X$ given that $Y=y$, where $y$ is some specific (fixed) value, is:

\begin{equation}
p_{X\mid Y} (x\mid y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}	\quad \hbox{provided } p_Y(y)=P(Y=y)>0
\end{equation}

As an example, let's consider how $p_{X\mid Y}$ would be computed.
The possible values of $y$ are $0,1$, and so we have to find the conditional distribution (defined above) for each of these values. I.e., we have to find $p_{X\mid Y}(x\mid y=0)$, and $p_{X\mid Y}(x\mid y=1)$.

Let's do the calculation for $p_{X\mid Y}(x\mid y=0)$.

\begin{equation}
\begin{split}
p_{X\mid Y} (1\mid 0) =& \frac{p_{X,Y}(1,0)}{p_Y(0)}\\
	=&  \frac{0.018}{0.291}\\
	=& `r fractions(0.018/0.291)`
\end{split}	
\end{equation}

This conditional probability value will occupy the cell X=1, Y=0 in the table below summarizing the conditional probability distribution $p_{X|Y}$. In this way, one can fill in the entire table, which will then represent the conditional distributions $p_{X|Y=0}$ and $p_{X|Y=1}$. The reader may want to take a few minutes to complete the table.  

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{c|ccccccc}
	& x=1 & x=2 & x=3 & x=4 & x=5 & x=6 & x=7\\ 
\hline	
$p_{X\mid Y}(x\mid y=0)$  & `r fractions(0.018/0.291)` &  & & & & & \\
$p_{X\mid Y}(x\mid y=1)$  &  &  & & & & &  \\
\end{tabular}
\end{center}
\caption{The conditional probability distribution of X given Y.}
\label{XgivenY}
\end{table}

Similarly, one can construct a table that shows $p_{Y|X}$.

### Example 2: Continuous bivariate distributions

Consider now the continuous bivariate case; this time, we will use simulated data. Consider two normal random variables $X$ and $Y$, each of which coming from, for example, a Normal(0,1) distribution, with some correlation $\rho$ between the two random variables. 

A bivariate distribution for two random variables $X$ and $Y$, each of which comes from a normal distribution, is expressed in terms of the means and standard deviations of each of the two distributions, and the correlation $\rho$ between them. The standard deviations and correlation are expressed in a special form of a $2\times 2$ matrix called a variance-covariance matrix $\Sigma$. If $\rho_u$ is the correlation between the two random variables, and $\sigma _{x}$ and $\sigma _{y}$ the respective standard deviations, the variance-covariance matrix is written as:

\begin{equation}\label{eq:covmatfoundations}
\Sigma
=
\begin{pmatrix}
\sigma _{x}^2  & \rho\sigma _{x}\sigma _{y}\\
\rho\sigma _{x}\sigma _{y}    & \sigma _{y}^2\\
\end{pmatrix}
\end{equation}

The off-diagonals of this matrix contain the covariance between  $X$ and $Y$. 

The joint distribution of $X$ and $Y$ is defined as follows:

\begin{equation}\label{eq:jointpriordistfoundations}
\begin{pmatrix}
  X \\ 
  Y \\
\end{pmatrix}
\sim 
\mathcal{N}_2 \left(
\begin{pmatrix}
  0 \\
  0 \\
\end{pmatrix},
\Sigma
\right)
\end{equation}

The joint PDF is written with reference to the two variables $f_{X,Y}(x,y)$. It has the property that the area under the curve sums to 1. Formally, we would write this as a double integral: we are summing up the area under the curve for both dimensions X and Y (hence two integrals).

\begin{equation}
\iint_{S_{X,Y}} f_{X,Y}(x,y)\, dx dy = 1
\end{equation}

Here, the terms $dx$ and $dy$ express the fact that we are summing the area under the curve along the X axis and the Y axis.

The joint CDF would be written as follows. The equation below gives us the probability of observing a value like $(u,v)$ or some value smaller than that (i.e., some $(u',v')$, such that $u'<u$ and $v'<v$.

\begin{equation}
\begin{split}
F_{X,Y}(u,v) =& P(X<u,Y<v)\\
             =& \int_{-\infty}^u \int_{-\infty}^v f_{X,Y}(x,y)\, dy dx \hbox{ for } (x,y)\in \mathbb{R}^2\\
\end{split}
\end{equation}

As an aside, notice that the support for the normal distribution ranges from minus infinity to plus infinity.  There can however be other PDFs with a more limited support; an example would be a normal distribution whose pdf $f(x)$ is such that the lower bound is truncated at, say, 0. In such a case, the area under the range $\int_{-\infty}^0 f(x) \, dx$ will be 0 because the range lies outside the support of the truncated normal distribution. 

A visualization will help. The figures below show a bivariate distribution with correlation zero (Figure \@ref(fig:zerocor)), a positive (Figure \@ref(fig:poscor)) and a negative correlation (Figure \@ref(fig:negcor)).

```{r zerocor,echo=FALSE,fig.cap="A bivariate Normal distribution with zero correlation. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot."}
f1 <- nbvpdf (0, 0, 1, 1, 0)
#f1 %$% matrix.variances
plot (f1, all=TRUE, n=20,
      main="No correlation")
```

```{r poscor,echo=FALSE,fig.cap="A bivariate Normal distribution with a positive  correlation of 0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot."}
f3 <- nbvpdf (0, 0, 1, 1, -0.6)
#f1 %$% matrix.variances
plot (f3, all=TRUE, n=20,
      main="Correlation -0.6")
```


```{r negcor,echo=FALSE,fig.cap="A bivariate Normal distribution with a negative  correlation of -0.6. Shown are four plots: the top-right plot shows the three-dimensional bivariate density, the top-left plot the contour plot of the distribution (seen from above). The lower plots show the cumulative distribution function from two views, as a three-dimensional plot and as a contour plot."}
f2 <- nbvpdf (0, 0, 1, 1, 0.6)
#f1 %$% matrix.variances
plot (f2, all=TRUE, n=20,
      main="Correlation 0.6")
```

In this book, we will make use of such multivariate distributions a lot, and it will soon become important to know how to generate simulated bivariate or multivariate data that is correlated. So let's look at how to generate simulated data next.

### Generate simulated bivariate (multivariate) data 

Suppose we want to generate 100 correlated pairs of data, with correlation $\rho=0.6$. The two random variables have mean 0, and standard deviations 5 and 10 respectively. 

Here is how we would generate such data. First, define a variance-covariance matrix; then, use the multivariate analog of the `rnorm` function, `mvrnorm`, to generate $100$ data points.

```{r}
library(MASS)
## define a variance-covariance matrix:
Sigma<-matrix(c(5^2,5*10*.6,5*10*.6,10^2),
              byrow=FALSE,ncol=2)
## generate data:
u<-mvrnorm(n=100,
           mu=c(0,0),
           Sigma=Sigma)
head(u)
```

A plot confirms that the simulated data are positively correlated.

```{r}
plot(u[,1],u[,2])
```

As an exercise, try changing the correlation to $0$ or to $-0.6$, and then plot the bivariate distribution that results.

One final useful thing to notice about the variance-covariance matrix is that it can be decomposed into the component standard deviations and an underlying correlation matrix. For example, consider the matrix above:

```{r}
Sigma
```

One can decompose the matrix as follows. The matrix can be seen as the product of a diagonal matrix of the standard deviations and the correlation matrix:

```{r}
## sds:
(sds<-c(5,10))
## diagonal matrix:
(sd_diag<-diag(sds))
## correlation matrix:
(corrmatrix<-matrix(c(1,0.6,0.6,1),ncol=2))
```

Given these two matrices, one can reassemble the variance-covariance matrix:

```{r}
sd_diag%*%corrmatrix%*%sd_diag                   
```

There is a built-in convenience function, `sdcor3cov` in the `SIN` package that does this calculation, taking the vector of standard deviatio ns (not the diagonal matrix) and the correlation matrix to yield the variance-covariance matrix:

```{r}
SIN::sdcor2cov(stddev=sds,corr=corrmatrix)
```

We will be using this function a lot when simulating data from hierarchical models.

## Likelihood and maximum likelihood estimation

We now turn to an important topic: the idea of likelihood, and of maximum likelihood estimation. Consider as a first example the discrete case, using the Binomial distribution.

Suppose we toss a fair coin 10 times, and count the number of heads; we do this experiment once. Notice below that we set the probability of success to be 0.5. This is because we are assuming that we tossed a fair coin.

```{r}
(x<-rbinom(1,size=10,prob=0.5))
```

The *probability* of obtaining this value depends on the parameter we set for $\theta$ in the PMF for the binomial distribution. Here are some possible values for $\theta$ and the resulting probabilities:

```{r}
dbinom(x,size=10,prob=0.1)
dbinom(x,size=10,prob=0.3)
dbinom(x,size=10,prob=0.5)
dbinom(x,size=10,prob=0.8)
dbinom(x,size=10,prob=1.0)
```

The value of $\theta$ that gives us the highest probability will be called the **maximum likelihood estimate**. The function ```dbinom```  (which is a function of $\theta$) is also called a likelihood function, and the maximum value of this function is called the maximum likelihood estimate. We can graphically figure out the maximal value of the ```dbinom``` likelihood function here by plotting the value of the function for all possible values of $\theta$, and checking which is the maximal value:

```{r}
theta<-seq(0,1,by=0.01)
plot(theta,dbinom(x,size=10,prob=theta),type="l",
     ylab="likelihood/probability")
abline(v=x/10)
```

It should be clear from the figure that the maximum value corresponds to the proportion of heads: `r  x`/10. This value is called the maximum likelihood estimate (MLE). We can obtain this MLE of $\theta$, which  maximizes the likelihood, by computing:

\begin{equation}
\hat \theta = \frac{x}{n}
\end{equation}

where $n$ is sample size, and $x$ is the number of successes. For the analytical derivation of this result, see the Linear Modeling lecture notes:
https://github.com/vasishth/LM



The likelihood function in a continuous case is similar to that of the discrete example above, but there is one crucial difference, which we will just get to below. 

Consider the following example. Suppose we have one data point from a Normal distribution, with mean 0 and standard deviation 1:

```{r}
(x<-rnorm(1))
```

The likelihood function for this data point is going to depend on two parameters, $\mu$ and $\sigma$. For simplicity, let's assume that $\sigma$ is known to be 1 and that only $\mu$ is unknown. In this situation, the value of $\mu$ that maximizes the likelihood will be the MLE. As before, we can graphically find the MLE by plotting the likelihood function:

```{r}
mu<-seq(-3,3,by=0.01)
plot(mu,dnorm(x,mean=mu,
              sd=1),type="l")
abline(v=x)
```

The maximum point in this function will always be the sample mean from the data; the sample mean is the MLE. In the above case, the mean of the single data point `r x` is the number itself. If we had two data points from a Normal(0,1) distribution, then the likelihood function would be defined as follows. First, let us simulate two data points:

```{r}
(x1<-rnorm(1))
(x2<-rnorm(1))
```

These two data points are independent of each other. Hence, to obtain the joint likelihood, we will have to multiply the likelihoods of each of the numbers, given some value for $\mu$:

```{r eval=FALSE}
mu<-1
dnorm(x1,mean=mu)*dnorm(x2,mean=mu)
```


In order to plot the joint likelihood, we need to write a function:

```{r}
normallik<-function(mu=NULL){
  dnorm(x1,mean=mu)*dnorm(x2,mean=mu)
}
## test:
normallik(mu=1)
```

Now, we can plot the likelihood function for these two data points:

```{r}
mu<-seq(-3,3,by=0.01)
plot(mu,normallik(mu),type="l")
abline(v=mean(c(x1,x2)))
```

Notice that the maximum value of this joint likelihood is the mean of the two data points.

For the normal distribution, where $X \sim N(\mu,\sigma)$, we can get MLEs of $\mu$ and $\sigma$ by computing:

\begin{equation}
  \hat \mu = \frac{1}{n}\sum x_i = \bar{x}  
\end{equation}

and

\begin{equation}
	\hat \sigma ^2 = \frac{1}{n}\sum (x_i-\bar{x})^2
\end{equation}

you will sometimes see the ``unbiased'' estimate (and this is what R computes) but for large sample sizes the difference is not important:

\begin{equation}
  \hat \sigma ^2 = \frac{1}{n-1}\sum (x_i-\bar{x})^2
\end{equation}

Now we come to a crucial difference between the discrete and continuous cases discussed above. The ```dbinom``` is the PMF but is also a likelihood function, when seen as a function of $\theta$. The ```dbinom``` gives us  the *probability* of a particular outcome (given some value of $\theta$). This is not the case in the continuous PDF. For example, the ```dnorm``` function doesn't give us the **probability** of a point value, but rather the **density** or **likelihood**. For this reason, we will make a distinction between the words probability and likelihood; in day to day parlance the two are used interchangeably, but here these two terms have a technical meaning that we will stick to.

A final point to note is that a likelihood function is not a PDF; the area under the curve does not need to sum to 1. 

### The importance of the MLE 

One significance of the MLE is that, having assumed a particular underlying PDF, we can estimate the (unknown) parameters (the mean and variance) of the distribution that we assume to have generated our particular data. For example, in the Binomial case, we have a formula for computing the MLEs of the mean and variance; for the Normal distribution, we have a formula for computing the MLE of the mean and the variance.


What are the distributional properties of the mean **under repeated sampling**? This is a question that forms the basis for hypothesis testing in the frequentist paradigm. So we turn to this topic in the next chapter.



## Summary of useful R functions relating to univariate distributions

Table \@ref(tab:dpqrfunctions) summarizes the different functions relating to univariate PMFs and PDFs, using the Binomial and Normal as examples.

Table: (\#tab:dpqrfunctions) Important R functions relating to two univariate distributions, the Normal and  the Binomial.

                                              Discrete              Continuous
  ------------------------------------ ---------------------- ----------------------
  Example:                              Binomial(n,$\theta$)   Normal($\mu,\sigma$)
  Likelihood function                          dbinom                 dnorm
  Prob Y=y                                     dbinom                always 0
  Prob $Y\geq y, Y\leq y, y_1<Y<y_2$           pbinom                 pnorm
  Inverse CDF                                  qbinom                 qnorm
  Generate simulated data                      rbinom                 rnorm

Other distributions, such as the t-distribution, the Uniform, Exponential, Gamma, Beta, etc., have their own set of d-p-q-r functions in R. The appendix summarizes the properties of the  distributions that we will need in this book.

## Summary of random variable theory

We can summarize the above informal concepts very compactly if we re-state them in mathematical form. A mathematical statement has the advantage not only of brevity but also of reducing ambiguity.  

Formally, a random variable $Y$ is defined as a function from a sample space of possible outcomes $S$ to the real number system: 

\begin{equation}
Y : S \rightarrow \mathbb{R}
\end{equation}

The random variable associates to each outcome $\omega \in S$ exactly one number $Y(\omega) = y$. $S_Y$ is all the $y$'s (all the possible values of $Y$, the support of $Y$). I.e., $y \in S_Y$. 

Every random variable $Y$ has associated with it a probability mass (distribution)  function (PMF, PDF). I.e., PMF is used for discrete distributions and PDF for continuous distributions. The PMF/PDF maps every element of $S_Y$ to a value between 0 and 1.

\begin{equation}
p_Y : S_Y \rightarrow [0, 1] 
\end{equation}

Probability mass functions (discrete case) and probability density functions (continuous case) are functions that assign probabilities or relative frequencies to all events in a sample space.

The expression 

\begin{equation}
 Y \sim f(\cdot)
\end{equation}

\noindent
will be used to mean that the random variable $Y$ has pdf/pmf $f(\cdot)$.
For example, if we say that $Y \sim Binomial(n,\theta)$, then we are asserting that the PMF is:

\begin{equation}
\hbox{Binomial}(k|n,\theta) = 
\binom{n}{k} \theta^{k} (1-\theta)^{n-k}
\end{equation}

If we say that $Y\sim Normal(\mu,\sigma)$, we are asserting that the PDF is

\begin{equation}
Normal(y|\mu,\sigma)= \frac{1}{\sqrt{2\pi \sigma^2}} \exp \left(-\frac{y-\mu)^2}{2\sigma^2} \right)
\end{equation}

The **cumulative distribution function** or CDF is defined as follows: 

For discrete distributions, the probability that $Y$ is less than $a$ is written:

\begin{equation}
P(Y<a) = F(Y<a) =\sum_{-\infty}^{a} f(y)
\end{equation}

For continuous distributions, the summation symbol $\sum$ above becomes the summation symbol for the continuous case, which is the integral $\int$. The upper and lower bounds are marked by adding a subscript and a superscript on the integral. For example, if we want the area under the curve between points a and b for some function $f(y)$, we write $\int_b^a f(y)\, dy$. So, if we want the probability that $Y$ is less than $a$, we would write:

\begin{equation}
P(Y<a) = F(Y<a) =\int_{-\infty}^{a} f(y)\, dy
\end{equation}

The above integral is simply summing up the area under the curve between the points $-\infty$ and $a$; this gives us the probability of observing $a$ or a value smaller than $a$.

A final point here is that we can go back and forth between the PDF and the CDF. If the PDF is $f(y)$, then the CDF that allows us to compute quantities like $P(Y<b)$ is just the integral:

\begin{equation}
F(Y<b)=\int_{-\infty}^b f(y)\, dy
\end{equation}

The above is simply computing the area under the curve $f(y)$, ranging from $b$ to $-\infty$.

Because differentiation is the opposite of integration (this is called the Fundamental Theorem of Calculus), if we differentiate the CDF, we get the PDF back: 

\begin{equation}
d(F(y))/dy=f(y)
\end{equation}

In bivariate distributions, the joint CDF is written $F_{X,Y}(a,b)=P(X\leq a, Y\leq b)$, where $-\infty < a,b<\infty$. The *marginal distributions* of $F_X$ and $F_Y$ are the CDFs of each of the associated random variables. The CDF of $X$:

\begin{equation}
F_X(a) = P(X\leq a) = F_X(a,\infty)	
\end{equation}

The CDF of $Y$:

\begin{equation}
F_Y(b) = P(Y\leq b) = F_Y(\infty,b)	
\end{equation}

$f(x,y)$ is the *joint PDF* of $X$ and $Y$. Every joint PDF satisfies

\begin{equation}
f(x,y)\geq 0\mbox{ for all }(x,y)\in S_{X,Y},
\end{equation}
and
\begin{equation}
\iintop_{S_{X,Y}}f(x,y)\,\mathrm{d} x\,\mathrm{d} y=1.
\end{equation}
	
where $S_{X,Y}$ is the joint support of the two random variables.

If X and Y are jointly continuous, they are individually continuous, and their PDFs are:

\begin{equation}
\begin{split}
P(X\in A) = & P(X\in A, Y\in (-\infty,\infty))	\\
= & \int_A \int_{-\infty}^{\infty} f(x,y)\,dy\, dx\\
= & \int_A f_X(x)\, dx
\end{split}	
\end{equation}

\noindent
where

\begin{equation}
f_X(x) = \int_{-\infty}^{\infty} f(x,y)\, dy	
\end{equation}

Similarly:

\begin{equation}
f_Y(y) =  \int_{-\infty}^{\infty} f(x,y)\, dx		
\end{equation}

	
	
## Further reading

For readers interested in the mathematics needed for statistics, the books by @fox2009mathematical, @gill2006essential, and @moore2013mathematics are useful. The essential matrix algebra needed for statistics is discussed in @fieller.  Accessible introductions to probability theory are @morin2016probability and @blitzstein2014introduction. @kerns contains a very well-written and freely available general introduction to random variable theory and statistics, but assumes the reader knows the basics of calculus.

## Exercises {#sec:Foundationsexercises}

### Practice using the ```pnorm``` function {#sec:Foundationsexercisespnorm}

#### Part 1 {#sec:FoundationsexercisespnormPart1}

```{r echo=FALSE}
## DATA GENERATION
mean.val<-round(rnorm(1,mean=100,sd=100),digits=0)
sd.val<-round(rnorm(1,mean=100,sd=1.5),digits=0)

q1<-round(rnorm(1,mean=100,sd=100)+50,digits=0)
q2<-round(rnorm(1,mean=100,sd=100)-100,digits=0)

if(q1>q2){
sol<-pnorm(q1,mean=abs(mean.val),sd=abs(sd.val))-pnorm(q2,mean=abs(mean.val),sd=abs(sd.val))
} else {
  sol<-pnorm(q2,mean=abs(mean.val),sd=abs(sd.val))-pnorm(q1,mean=abs(mean.val),sd=abs(sd.val))
}
```

Given a normal distribution with mean ```r abs(mean.val)``` and standard deviation ```r abs(sd.val)```, use the pnorm function to calculate the probability of obtaining values between ```r format(q1)``` and ```r format(q2)``` from this distribution.

#### Part 2 {#sec:FoundationsexercisespnormPart2}

```{r echo=FALSE}
mu<-round(rnorm(1,mean=51,sd=2),digits=0)
sigma<-round(runif(1,min=2,max=4),digits=0)

q<-round(runif(1,min=40,max=50))
q1<-round(runif(1,min=51,max=60))


p1<-round(pnorm(q,mean=mu,sd=sigma),digits=3)
p2<-round(1-pnorm(q,mean=mu,sd=sigma),digits=3)
p3<-round(1-pnorm(q1,mean=mu,sd=sigma),digits=3)
```

Calculate the following probabilities. 
Given a normal distribution with mean ```r mu``` and standard deviation ```r sigma```, what is the probability of getting 

- a score of ```r q``` or less
- a score of ```r q``` or more
- a score of ```r q1``` or more

#### Part 3 {#sec:FoundationsexercisespnormPart3}

```{r echo=FALSE}
mu<-round(runif(1,min=45,max=55),digits=0)
sigma<-round(runif(1,min=2,max=10),digits=0)

p1<-round(pnorm(mu-5,mean=mu,sd=sigma),digits=3)
p2<-round(pnorm(mu+3,mean=mu,sd=sigma)-pnorm(mu-3,mean=mu,sd=sigma),digits=3)
p3<-round(1-pnorm(mu+1,mean=mu,sd=sigma),digits=3)
```

Given a normal distribution with mean ```r mu``` and standard deviation ```r sigma```, what is the probability of getting 

- a score of ```r mu-5``` or less.
- a score between ```r mu-3``` and ```r mu+3```.
- a score of ```mu+1``` or more.

### Practice using the ```qnorm``` function {#sec:Foundationsexercisesqnorm}

#### Part 1 {#sec:FoundationsexercisesqnormPart1}

```{r echo=FALSE}
prob1<-round(runif(1,min=0,max=1),digits=2)
if(prob1<=0.5){
 prob2<-round(runif(1,min=0.55,max=0.99),digits=2)
} else {
    prob2<-prob1
    prob1<-round(runif(1,min=0.01,max=0.40),digits=2)}

lower<-round(qnorm(prob1,mean=1,sd=1),digits=3)
upper<-round(qnorm(prob2,mean=1,sd=1),digits=3)
```

Consider a normal distribution with mean 1 and standard deviation 1.

Compute the lower and upper boundaries such that:

- the area (the probability) to the left of the lower boundary is ```r prob1```. 
- the area (the probability) to the left of the upper boundary is ```r prob2```.

#### Part 2 {#sec:FoundationsexercisesqnormPart2}

```{r echo=FALSE}
mu<-round(runif(1,min=50,max=60),digits=3)
sigma<-round(runif(1,min=0.5,max=1.5),digits=3)
probs<-c(.80,.85,.90,.95,.99)
prob<-sample(probs,1)
tailprob<-(1-prob)/2
q1<-round(qnorm(tailprob,mean=mu,sd=sigma),digits=3)
q2<-round(qnorm(tailprob,mean=mu,sd=sigma,lower.tail=FALSE),digits=3)
```

Given a normal distribution with mean ```r mu``` and standard deviation ```r sigma```. There exist two quantiles, the lower quantile q1 and the upper quantile q2, that are equidistant from the mean ```r mu```, such that the area under the curve of the Normal probability between q1 and q2  is ```r prob*100```%. Find q1 and q2. 



### Practice using ```qt``` {#sec:Foundationsexercisesqt}

```{r echo=FALSE}
n<-round(runif(1,min=140,max=170))
mu<-round(runif(1,min=120,max=180),0)
sigma<-round(runif(1,min=30,max=80),0)
x<-rnorm(n,mean=mu,sd=sigma)
sample.sd<-round(sd(x),3)
sample.mean<-round(mean(x),digits=3)
estimated.se<- round(sample.sd/sqrt(n),3)
crit.t<-abs(round(qt(0.025,df=n-1),3))
lower<-round(sample.mean-crit.t*estimated.se,digits=3)
upper<-round(sample.mean+crit.t*estimated.se,digits=3)
```

Take an independent random sample of size ```r n``` from a normal distribution
with mean ```r mu```, and standard deviation ```r sigma```. Next, we are going to pretend we don't know the population parameters (the mean and standard deviation). We compute the MLEs of the mean and standard deviation using the data and get the sample mean ```r sample.mean``` and the sample standard deviation ```r sample.sd```. 

- Compute the estimated standard error using the sample standard deviation provided above. 
- What are your degrees of freedom for the relevant t-distribution?
- Calculate the **absolute** critical t-value for a 95\% confidence interval using the relevant degrees of freedom you just wrote above.
- Next, compute the lower bound of the 95\% confidence interval using the estimated standard error and the critical t-value.
- Finally, compute the upper bound of the 95\% confidence interval using the estimated standard error and the critical t-value.

### Maximum likelihood estimation 1 {#sec:FoundationsexercisesMLE1}

```{r echo=FALSE}
x<-round(rnorm(1,mean=12,sd=5),3)
mn<-seq(9,12,by=1)
ans1<-round(dnorm(x,mean=mn[4],sd=5),3)
ans2<-round(dnorm(x,mean=mn[3],sd=5),3)
ans3<-round(dnorm(x,mean=mn[2],sd=5),3)
ans4<-round(dnorm(x,mean=mn[1],sd=5),3)
```


The function ```dnorm``` gives the likelihood given a data point (or multiple data) and a value for the mean and the standard deviation (sd). Using ```dnorm```, compute 

- the likelihood of the data point ```r x``` assuming a mean of ```r mn[4]``` and standard deviation 5.
- the likelihood of the data point ```r x``` assuming a mean of ```r mn[3]``` and standard deviation 5.
- the likelihood of the data point ```r x``` assuming a mean of ```r mn[2]``` and standard deviation 5.
- the likelihood of the data point ```r x``` assuming a mean of ```r mn[1]``` and standard deviation 5.

### Maximum likelihood estimation 2 {#sec:FoundationsexercisesMLE2}

```{r echo=FALSE}
x<-round(rnorm(10,mean=500,sd=10),0)
mn<-mean(x)
loglik<-round(sum(dnorm(x,mean=mn,sd=5,log=TRUE)),3)
loglik2<-round(sum(dnorm(x,mean=mn-2,sd=5,log=TRUE)),3)
```

You are given $10$ independent and identically distributed data points that are assumed to come from a Normal distribution with unknown mean and unknown standard deviation:

```{r}
x
```

The function ```dnorm``` gives the likelihood given  multiple data points and a value for the mean and the standard deviation. The log-likelihood can be computed by typing ```dnorm(...,log=TRUE)```.

The product of the likelihoods for two independent data points can be computed like this: Suppose we have two independent and identically distributed data points 5 and 10. Then, assuming that the Normal distribution they come from has mean 10 and standard deviation 2, the joint likelihood of these is:

```{r}
dnorm(5,mean=10,sd=2)*dnorm(10,mean=10,sd=2)
```

It is easier to do this on the log scale, because then one can add instead of multiplying. This is because $\log(x\times y)= \log(x) + \log(y)$.  For example:

```{r}
log(2*3)
log(2) + log(3)
```

So the joint log likelihood of the two data points is:

```{r}
dnorm(5,mean=10,sd=2,log=TRUE)+dnorm(10,mean=10,sd=2,log=TRUE)
```

Even more compactly:

```{r}
sum(dnorm(c(5,10),mean=10,sd=2,log=TRUE))
```

- Given the 10 data points above, calculate the maximum likelihood estimate (MLE) of the expectation.
- The sum of the log-likelihoods of the data x, using as the mean the MLE from the sample, and standard deviation 5.
- What is the sum of the log-likelihood if the mean used to compute the log-likelihood is ```r mn-2```? 
- Which value for the mean, the MLE or ```r mn-2```, gives the higher log-likelihood? 


### Generating bivariate data {#sec:Foundationsexercisesbivar}

Generate 50 data points from two random variables X and Y, where $X\sim Normal(50,100)$ and $Y\sim Normal(100,20)$. The correlation between the random variables is  0.7. Plot the simulated data points from Y against those from X.

### Generating multivariate data {#sec:Foundationsexercisesmultivar}

The bivariate case can be generalized to more than two dimensions.
Generate 50 data points from three random variables X, Y, and Z, where $X\sim Normal(50,100)$, $Y\sim Normal(100,20)$, and $Z\sim Normal(200,50)$. The correlation between the random variables X and Y is  0.5, between X and Z is 0.2, an between Y and Z is 0.7. Here, you will have to define a $3\times 3$ variance covariance matrix, with the pairwise covariances in the off-diagonals. Plot the simulated data points as two-dimensional figures: Y against X, Y against Z, and X against Z.

