# Linear models and linear mixed models

The t-tests discussed in chapter 2 can be re-cast as linear models. The two-sample t-test corresponds to the simple linear model, and the paired t-test to the linear mixed model. This connection between the two seemingly different types of statistical models allows us to go beyond the narrow confines of the t-test and to investigate more sophisticated and more complicated research questions.

## From the t-test to the linear (mixed) model

We begin with the now-familiar @grodner self-paced reading data; this design was first presented in chapter 2. Load the data and compute the means for the raw reading times by condition:

```{r}
library(lingpsych)
data("df_gg05e1")
gg05e1 <- df_gg05e1
means <- round(with(gg05e1, tapply(rawRT,
  IND = condition,
  mean
)))
means
```

As predicted by theory [@grodner], object relatives (labeled objgap here) are read slower than subject relatives (labeled subjgap).

As discussed in the previous chapter, a paired t-test can be done to evaluate whether we have evidence against the null hypothesis that object relatives and subject relatives have identical reading times. However, we have to aggregate the data by subjects and by items first. 

```{r}
bysubj <- aggregate(rawRT ~ subject +
  condition,
mean,
data = gg05e1
)
byitem <- aggregate(rawRT ~ item +
  condition,
mean,
data = gg05e1
)
t.test(rawRT ~ condition,
  paired = TRUE, bysubj
)$statistic
t.test(rawRT ~ condition,
  paired = TRUE, byitem
)$statistic
```

What these two t-tests show is that both by subjects and by items, there is strong evidence against the null hypothesis that the object and relatives have identical reading times. 

Interestingly, exactly the same t-values can be obtained by running the following commands, which  implement a kind of linear model called the *linear mixed model*:

```{r}
m0rawsubj <- lmer(rawRT ~ condition + (1 | subject), bysubj)
summary(m0rawsubj)$coefficients

m0rawitem <- lmer(rawRT ~ condition + (1 | item), byitem)
summary(m0rawitem)$coefficients
```

The signs of the t-values are the opposite to that of the paired t-tests above; the reason  for that will presently become clear.

Our goal in this chapter is to understand the above model involving the `lmer` function, using the familiar paired t-test as a starting point.

For now, consider only the by-subject analysis. 
Given the sample means shown above for the two conditions, 
we can rewrite our best guess about how the object and subject relative clause reading time distributions were generated:

- Object relative reading time: $Normal(471,\hat\sigma)$
- Subject relative reading time: $Normal(369,\hat\sigma)$

Here, $\hat\sigma$ is the estimated standard deviation; it is assumed to be the same for the two relative clause conditions. Right now the focus is only on the two estimates mean reading times for the two conditions.

The above two statements about the underlying pdf that produced the two relative clause conditions' reading times can also be rewritten with respect to the mean reading time of the object relative and the difference between the two conditions (the reasons for this will become clear presently):

- Object relative: $Normal(471-102\times 0,\hat\sigma)$
- Subject relative: $Normal(471-102\times 1,\hat\sigma)$


Note that the two distributions for object and subject relative clauses (RCs) are assumed to be independent. This assumed independence is expressed by the fact that we define two separate Normal distributions, one for object relatives and the other for subject  relatives. We saw earlier that this assumption of independence does not hold in our data because, in the aggregated data, each subject delivers a data point for subject relatives and another data point for object relatives. However, for now we will ignore this detail; we will fix this  shortcoming later.

The interesting point to notice here is that the mean for the object and subject relatives' distributions can be rewritten as a sum of two terms. A completely equivalent way to express the fact that object relatives are coming from a $Normal(471,\hat\sigma)$ is to say that each object relative data point can be described by the following equation:

\begin{equation}
y = 471 + -102 \times 0 + \varepsilon \hfill \hbox{ where } \varepsilon \sim Normal(0,\hat\sigma)
\end{equation}

Similarly, the subject relative's distribution can be written as being generated from:

\begin{equation}
y = 471 - 102 \times 1 + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,\hat\sigma)
\end{equation}

In these data,  the parameter $\hat\sigma$ is estimated to be $213$ ms. How do we know what this estimate is? This parameter's estimate can be derived from the by-subject t-test output above: The observed t-value is 

\begin{equation}
t_{obs}= \frac{\bar{y}-0}{s/\sqrt{n}} 
\end{equation}

Solving for $s$:

\begin{equation}
 s = \bar{y} \times \sqrt{n}/t_{obs} = -102 \times \sqrt{42}/-3.109  =  213
\end{equation}

So, based on the data, our model for the relative clause data consists of two equations:

Object relatives:

\begin{equation}
y = 471 -102\times 0 + \varepsilon \hfill \hbox{ where } \varepsilon \sim Normal(0,213)
\end{equation}

Subject  relatives:

\begin{equation}
y = 471 - 102\times 1 + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,213)
\end{equation}

The above statements describe a *generative process* for the data. 

Given such a statement about the generative process, we can express the estimated mean reading times for each RC type as follows. The term $\varepsilon$ has mean 0; we stipulate this when we specify that $\varepsilon \sim Normal(0,\sigma)$.

Mean object relative reading times:

\begin{equation}
\hbox{Mean OR RT}= 471 -102\times 0
\end{equation}

Mean subject relative reading times:

\begin{equation}
\hbox{Mean SR RT} = 471 - 102\times 1 
\end{equation}

There is a function in R, the `lm()` function, which expresses the above statistical model, and prints out exactly the same numerical values that are shown above:

```{r}
summary(m0raw <- lm(rawRT ~ condition, bysubj))$coefficients
```


The linear model function `lm()` prints out two coefficients, $471$ and  $-102$, that express the mean reading times for object and subject relative data, using a simple coding scheme: object relatives are coded as 0, and subject relatives are coded as 1. This coding scheme is not visible to the user, but is represented internally in R.  However, the user can see the coding for each condition level by typing:

```{r}
## make sure that the condition column is of type factor:
bysubj$condition <- factor(bysubj$condition)
contrasts(bysubj$condition)
```

We will discuss contrast coding in  detail in a later chapter, but right now the simple 0,1 coding above---called treatment contrasts---is enough for our purposes.

Thus, what the linear model above gives us is two numbers: the estimated mean object relative reading time (471), and the estimated *difference* between object and subject relative (-102). We can extract the two coefficients by typing:

```{r}
round(coef(m0raw))
```

In the vocabulary of linear modeling, the first number is called the *intercept*, and the second one is called the *slope*. 
Note that the meaning of the intercept and slope depends on the ordering of the factor levels. We can make the sample mean of the  subject relative represent  the intercept:

```{r}
## reverse the factor level ordering:
bysubj$condition <- factor(bysubj$condition,
  levels = c("subjgap", "objgap")
)
contrasts(bysubj$condition)
```

Now, the intercept is the mean of the subject relatives, and the slope is the difference between object and subject relatives reading times. Note that  the  sign of the t-value has changed---the sign depends on the contrast coding. 

```{r}
m0raw <- lm(rawRT ~ condition, bysubj)
summary(m0raw)$coefficients
```

Let's switch back to the original  factor level ordering:

```{r}
bysubj$condition <- factor(bysubj$condition,
  levels = c("objgap", "subjgap")
)
contrasts(bysubj$condition)
m0raw <- lm(rawRT ~ condition, bysubj)
summary(m0raw)$coefficients
```

In mathematical form, the model can now be stated as a single equation:

\begin{equation}
rawRT = \hat\beta_0 + \hat\beta_1 condition + \varepsilon
\end{equation}

where 

- condition is a 0,1 coded vector, with object relatives coded as 0, and subject relatives coded as 1.
- $\hat\beta_0$ is the estimated mean for the object relative (which is coded as 0)
- $\hat\beta_1$ is the amount by which the object relative mean must be 
changed to obtain the estimated mean for the subject relative.
- $\varepsilon$ is the noisy variation from trial to trial around the means for the two conditions, represented by $Normal(0,213)$.  That is, $\hat\sigma$ is 213 ms.

The null hypothesis of scientific interest here is always with reference to the slope, that the difference in means between the two relative clause types $\beta_1$ is:

$H_0: \beta_1 = 0$

The t-test value printed out in the linear model is simply the familiar t-test formula in action:

\begin{equation}
t_{obs} = \frac{\hat\beta_1 - 0}{SE}
\end{equation}

The  intercept also has a null hypothesis associated with it, namely that 
$H_0: \beta_0 = 0$. However, this null hypothesis test is of absolutely no interest for us. This hypothesis test is reported by the `lm()` function only because the intercept is needed  for technical reasons, to be discussed in the contrast coding chapter.

The *contrast coding* mentioned above determines the meaning of the $\beta_0$ and $\beta_1$ parameters:

```{r}
bysubj$condition <- factor(bysubj$condition,
  levels = c("objgap", "subjgap")
)
contrasts(bysubj$condition)
```

When discussing linear models, we will make a distinction between the unknown true means $\beta_0, \beta_1$ and the estimated mean from the data $\hat\beta_0, \hat\beta_1$. The estimates that we have from the data are:

- Estimated mean object relative processing time: $\hat\beta_0=`r round(coef(m0)[1])`$
.
- Estimated mean subject relative processing time: $\hat\beta_0+\hat\beta_1=`r round(coef(m0raw)[1])`+`r round(coef(m0raw)[2])`=`r round(coef(m0raw)[1])+(round(coef(m0raw)[2]))`$.

The underlying statistical model expressed in terms of the unknown parameters is stated in terms of $\beta_0, \beta_1, \sigma$:

\begin{equation}
rawRT = \beta_0 + \beta_1 condition + \varepsilon
\end{equation}

where $\varepsilon \sim Normal(0,\sigma)$.

The null hypotheses are always stated in terms of these unknown parameters. Thus, a careful distinction should be made between the true unknown parameters, and their estimates from the data; the latter are designated by a hat over the parameter: $\hat\cdot$.

## Sum coding

We have established so far that the mathematical form of the model is:

\begin{equation}
rawRT = \beta_0 + \beta_1 condition + \varepsilon
\end{equation}

An important change that we will make now is to the contrast coding of the `condition` vector. First, recode the levels of the condition column as shown below.

```{r}
## new contrast coding:
bysubj$cond <- ifelse(bysubj$condition == "objgap", 1, -1)
```

Now, the two conditions  are coded not as 0, 1 but as -1 and +1. This new coding is stored in a new column called `cond` (of course, one can call it anything one likes). It is always a good idea to quickly carry out a sanity check to ensure that the coding is correct. One way to do this is by comparing the `cond` column with the `condition` column. Many a data analysis has gone awry because the researcher neglected to check that their coding was set up the way they wanted it!

```{r}
xtabs(~ cond + condition, bysubj)
```

With  this coding, the $\beta$ parameters have a different meaning:

```{r}
m1raw <- lm(rawRT ~ cond, bysubj)
round(coef(m1raw))
```

- The intercept now represents the grand mean processing time: the estimate from the data is $\hat \beta_0=`r round(coef(m1raw)[1])`$.
- The estimated mean object relative processing time is now: $\hat\beta_0+\hat\beta_1\times 1=`r round(coef(m1raw)[1])`+`r round(coef(m1raw)[2])`=`r round(coef(m1raw)[1])+round(coef(m1raw)[2])`$.
- The estimated mean subject relative processing time is: $\hat\beta_0+\hat\beta_1\times (-1)=`r round(coef(m1raw)[1])`-`r round(coef(m1raw)[2])`=`r round(coef(m1raw)[1])-round(coef(m1raw)[2])`$.

This kind of parametrization is called *sum-to-zero contrast* or more simply *sum contrast* coding. This is the coding we will use most frequently in this book.  Contrast coding will be elaborated on in a later chapter; there, the advantages of sum coding over treatment coding will become clear. For now, it is sufficient to understand that one can *reparametrize* the model using different kinds of contrast coding, and that such a reparametrization impacts the interpretation of the parameters.

With sum coding, the null hypothesis for the slope is

\begin{equation}
H_0: \mathbf{1\times} \mu_{obj} + (\mathbf{-1}) \times\mu_{subj} = 0   
\end{equation}

The sum contrast coding of $+1$ standing for object relatives and $-1$ standing for subject relatives in the linear model directly refer to the $\pm 1$ coefficients in the null hypothesis above.
Now the model, using the estimates based on the data, is as follows. 

Object relative reading times:

\begin{equation}
rt = 420\mathbf{\times 1} + 51\mathbf{\times 1} + \varepsilon
\end{equation}

Subject relative reading times:

\begin{equation}
rt = 420\mathbf{\times 1} + 51\mathbf{\times (-1)} + \varepsilon
\end{equation}

One could write it in a single line as:

\begin{equation}
rt = 420 + 51\times condition + \varepsilon
\end{equation}

The numerical values in the vector $\varepsilon$ are called the residuals; these number represent the amount by which the observed data deviate from the numerical point values in the above model. 

For example, suppose that a data point for a subject relative condition is 400 ms. The model predicts an average reading time of 420-51=369 ms for all subjects. The residual for that subject's data point would be 400-369= 31. Another example: suppose that a data point for an object relative condition is 200 ms. The model predicts the object relative to be 420+51=471. The residual for that data point would then be 200-471=-271. Thus, the residual is the amount of the discrepancy between the model's predicted reading time and the actually observed reading time.

## Checking model assumptions

It is an assumption of the linear model that the residuals are (approximately) normally distributed, That is what the statement $\varepsilon\sim Normal(0,\sigma)$ implies. It is important to check that model assumptions are approximately satisfied; this is because the null hypothesis significance testing procedure requires approximate normality of residuals.

Here is how we can check whether this normality assumption is met. First, extract the vector of residuals:

```{r}
## extract residuals:
res_m1raw <- residuals(m1raw)
```

Then, graphically compare the residuals to the quantiles of the standard normal distribution ($Normal(0,1)$); see Figure \@ref(fig:residualsplot1).

```{r residualsplot1,message=FALSE,warning=FALSE,results="asis",fig.cap="The residuals of the model `m1raw` plotted against the quantiles of a standard normal distribution."}
qqnorm(res_m1raw)
```

When the normality assumption is met, the residuals will align with the quantiles of the standard normal distribution, resulting in a straight diagonal line. When the normality assumption is not met, the line will tend to curve away from the diagonal. 

There are formal statistical tests for checking whether a distribution is normal, but these tests are so stringent that even a sample from the standard normal might not pass this test. This is why statistics textbooks like @venablesripley suggest using a graphical comparison of quantiles to establish approximate normality. 

In the above case, a log transform of the data improves the normality of residuals; see Figure \@ref(fig:residualsplot2).  We will discuss transformation in detail later in this book; for now, it is sufficient to note that  for continuous data that consists of all-positive values (here, reading times), a log transform will often be the appropriate transform.

```{r}
m1log <- lm(log(rawRT) ~ cond, bysubj)
```

```{r residualsplot2,echo=FALSE,fig.cap="The residuals of the model `m0log` plotted against the quantiles of a standard normal distribution."}
qqnorm(residuals(m1log))
```


The estimates of the parameters are now in the log scale:

- The estimated grand mean processing time: $\hat\beta_0=`r round(coef(m1log)[1],4)`$.
- The estimated mean object relative processing time: $\hat\beta_0+\hat\beta_1=`r round(coef(m1log)[1],4)`+`r round(coef(m1log)[2],4)`=`r round(coef(m1log)[1],4)+round(coef(m1log)[2],4)`$.
- The estimated mean subject relative processing time: $\hat\beta_0-\hat\beta_1=`r round(coef(m1log)[1],4)`-`r round(coef(m1log)[2],4)`=`r round(coef(m1log)[1],4)-round(coef(m1log)[2],4)`$.

The model does not change, only the scale does:

\begin{equation}
\log rt = \beta_0 + \beta_1 condition  + \varepsilon 
\end{equation}

where $\varepsilon \sim Normal(0,\sigma)$.

Now, the intercept and slope can be used to compute the reading time in the two conditions.
Note that because $exp(log(rt))=rt$, to get the estimates on the raw ms scale, we just need to exponentiate both sides of the equation:

\begin{equation}
exp(\log rt) = exp( \beta_0 + \beta_1 condition)
\end{equation}

This approach gives us the following estimates on the ms scale:

- Estimated object relative reading time: $exp(\hat\beta_0+\hat\beta_1)=exp(`r round(coef(m1log)[1],4)`+`r round(coef(m1log)[2],4)`)=`r round(exp(round(coef(m1log)[1],4)+round(coef(m1log)[2],4)))`$.
- Estimated subject relative reading time: $exp(\hat\beta_0-\hat\beta_1)=exp(`r round(coef(m1log)[1],4)`-`r round(coef(m1log)[2],4)`=`r round(exp(round(coef(m1log)[1],4)-round(coef(m1log)[2],4)))`$.

The exponentiated values are medians, not means. We use the median here because the mean in the log-transformed data depends on the standard deviation.

The difference in reading time is 417-352=65 ms. If we had fit the model to raw reading times, the difference would have been:

```{r}
m1raw <- lm(rawRT ~ cond, bysubj)
```

- Estimated mean object relative reading time: $\hat\beta_0+\hat\beta_1=`r round(coef(m1raw)[1],2)`+`r round(coef(m1raw)[2],2)`=`r round(coef(10raw)[1],2)+round(coef(m1raw)[2],2)`$.
- Estimated mean subject relative reading time: $\hat\beta_0-\hat\beta_1=`r round(coef(m1raw)[1],2)`-`r round(coef(m1raw)[2],2)`=`r round(coef(m1raw)[1],2)-round(coef(m1raw)[2],2)`$.

The difference in the means on the raw scale is 102 ms.
The larger estimate based on the  raw scale is less realistic, and we will see later that the large difference between the two conditions is driven by a few extreme, influential values.


## From the paired t-test to the linear  mixed  model

One important point to notice is that the observed t-value of the  paired t-test and the t-test printed out by the simple linear model don't match:

```{r}
summary_ttest(t.test(rawRT ~ condition, bysubj, 
                     paired = TRUE))
round(summary(m1raw)$coefficients, 2)[, c(1:3)]
```

This is because the linear model is equivalent to the unpaired (i.e., two sample) t-test:

```{r}
summary(lm(rawRT ~ condition, bysubj))

summary_ttest(t.test(rawRT ~ condition, bysubj,
  paired = FALSE
))
```


As discussed earlier, the paired t-test's analog in the linear modeling framework is the linear mixed model with varying intercepts. We turn next to this extension of the simple linear model. The command corresponding to the   paired t-test in the  linear modeling framework is as follows. Notice that we are working with the aggregated data, as in the paired t-test. Later, we will switch to the unaggregated data; that will be the correct analysis of these data.

```{r}
m0raw_lmer <- lmer(rawRT ~ condition + (1 | subject), bysubj)
summary(m0raw_lmer)$coefficients
```

To understand the connection between the paired t-test and the above command, it is necessary to consider how a paired t-test is assembled.  

First, some background knowledge from random variable theory. If you have two random variables that have   correlation $\rho$, the variance of the difference between the two random variables is:

\begin{equation}
Var(Y_1-Y_2)=Var(Y_1) + Var(Y_2) - 2\times Cov(Y_1, Y_2)
\end{equation}

$Cov(Y_1, X2)$ is the covariance between the two  random variables and  is defined  as:

\begin{equation}
Cov(Y_1, Y_2) = \rho \sqrt{Var(Y_1)}\sqrt{Var(Y_2)}
\end{equation}

You can find the proofs of the above assertions in books like @rice1995mathematical.

As discussed earlier, a paired t-test is used when you have paired data from subject $i=1,...,n$ in two conditions; assume that they are labeled conditions 1 and 2. Let's write the data as two vectors $Y_{1}, Y_{2}$. Because the pairs of data points are coming from the same subject, they are correlated with some correlation $\rho$; recall Figure \@ref(fig:averageorsr). Assume that both conditions 1 and 2 have standard deviation $\sigma$. 

To make this discussion concrete, let's generate some simulated bivariate data that are correlated. At this point, you may want to review chapter 1's section on bivariate/multivariate distributions. Assume that $\sigma = 1$, $\rho=0.5$, and that the data are balanced. 

```{r}
samplesize <- 12
mu <- c(.3, .2)
rho <- 0.5
stddev <- 1
Sigma <- matrix(stddev, nrow = 2, ncol = 2) + diag(2)
Sigma <- Sigma / 2
Sigma

## simulated data:
y <- mvrnorm(n = samplesize, mu = mu, Sigma = Sigma, 
             empirical = TRUE)
head(y)

n <- samplesize
y1 <- y[, 1]
y2 <- y[, 2]
y1
y2
```

To carry out the paired t-test, 
we need to know the variance of $Y_1-Y_2$ because the t-statistic will be:

\begin{equation}
T = \frac{(\mu_{Y_1} - \mu_{Y_2})-0}{\sqrt{Var(Y_1 - Y_2)/n}}
\end{equation}

Now,

\begin{equation}
Var(Y_1 - Y_2) = \sigma^2 + \sigma^2 - 2 \rho \sigma\sigma = 2\sigma^2 (1-\rho)
\end{equation}

Now let's compute the t-statistic using the above formula. Let the actual data vectors be $y_1, y_2$.

\begin{equation}
t_{obs} = \frac{(mean(y_1) - mean(y_2))-0}{\sqrt{Var(y_1 - y_2)/n}}
\end{equation}

This simplifies to:

\begin{equation}
t_{obs} = \frac{(mean(y_1) - mean(y_2))-0}{\sqrt{2\sigma^2(1-\rho)/n}}
\end{equation}

Now compare the paired t-test output and the by-hand calculation:

```{r}
summary_ttest(t.test(y1, y2, paired = TRUE))

round(((mean(t1) - mean(y2))-0) / sqrt((2 * stddev^2 * (1 - rho)) / n),2)
```

The two t-values are identical.

The linear mixed model we present next will fit exactly the same model as the paired t-test above. We will prove below that the linear mixed model above and the paired t-test are exactly the same model.  

Suppose we have $i$ subjects and two conditions, labeled 1 and 2. For now, assume that each subject sees each condition only once (e.g., the by-subjects aggregated English relative clause data), so we have two data points from each subject. In other words, the data are paired.

Then, for condition 1, the dependent variable can be described by the equation:

$y_{i1} = \beta_0 + u_{0i} + \varepsilon_{i1}$

Here, $\beta_0$ is the  mean  reading time, and $\varepsilon$ is the usual residual error term. The interesting new term is $u_{0i}$. This is a numerical value that constitutes an adjustment to the intercept $\beta_0$, for subject $i$. That is, if some subject is slower than $\beta_0$, $u_{0i}$ will be a positive number; if a subject is faster than $\beta_0$, then that subject's adjustment $u_{0i}$ will be negative in sign; and if a subject has exactly the same reading time as the intercept $\beta_0$, then $u_{0i}$ for that subject will be exactly 0. 

Similarly, for condition 2, the dependent variable is described  by the equation:

$y_{i2} = \beta_0 + \delta + u_{0i} + \varepsilon_{i2}$

Here, $\delta$ is the additional time taken to process condition 2 (thus, this is the treatment contrast coding we saw earlier in this chapter). 

If we subtract the equation for condition 2 from the equation for condition 1, the resulting equation is:

$d_i=y_{i1} - y_{i2}= \delta + (\varepsilon_{i1}-\varepsilon_{i2})$

The $u_{0i}$ terms cancel out.
The expectation or mean value of $d_i$ is $\delta$ because the expectation of the  $\varepsilon$ terms  is 0 (we set up the model such that  $\varepsilon  \sim Normal(0,\sigma)$).

Now, assuming that the error terms are correlated with correlation $\rho$, the result presented at the beginning of this section applies:

\begin{equation}
Var(y_{i1}-y_{i2}) = \sigma^2 + \sigma^2-2\rho\sigma^2=2\sigma^2(1-\rho)
\end{equation}

The generative distribution for $d_i$, the pairwise differences in the two conditions, is

\begin{equation}
d_i \sim Normal(\delta, \sqrt{2\sigma^2(1-\rho)})
\end{equation}

But that is exactly the same  standard deviation as the one used in the paired t-test. 

The above result proves that the paired t-test will deliver exactly the same t-score as the linear mixed model with the additional term $u_{0i}$. It is called a linear *mixed* model because of the new adjustment to the intercept $\beta_0$, the term $u_{0i}$ for each subject. This term is called a *varying intercept* by subject. It is called a varying intercept because the $u_{0i}$ adjustment leads to the intercept for a particular subject becoming the sum of a fixed, unvarying term and a term that varies by subject, $\beta_0 + u_{0i}$. Compare this to the fixed intercept $\beta_0$ in the simple linear model that corresponds to the unpaired t-test. In the linear mixed model, the intercept term is different for each subject because of the $u_{0i}$ term.  

Let's check that the linear mixed model delivers exactly the same t-value as our paired t-test above. We will use our simulated data. In the code below, the term  `(1|subj)` is the adjustment by subject to the intercepts---the term $u_{0i}$ above.

```{r}
dat <- data.frame(y = c(y1, y2), 
                  condition = rep(letters[1:2], 
                                  each = n), 
                  subj = rep(1:n, 2))
dat$condition <- factor(dat$condition)
## sum coding
dat$cond<-ifelse(dat$condition=="a",1,-1)
## sanity check!
xtabs(~cond+condition,dat)

summary(m2 <- lmer(y ~ cond + (1 | subj), dat))
```

The t-statistic from the linear mixed model is exactly the same as that from the paired t-test.

Now we have the necessary background for investigating the linear mixed models in detail.

## Linear mixed models

We return to our subject and object relative clause data from English (Grodner and Gibson, Expt 1). First load the data as usual, define relative clause type as a sum-coded predictor, and create a new column called `so` that represents the contrast coding ($\pm 1$ sum contrasts), and  a column that holds log-transformed reading time. 

```{r}
gg05e1$so <- ifelse(gg05e1$condition == "objgap", 1, -1)
gg05e1$logrt <- log(gg05e1$rawRT)
```

Recall that these data have multiple measurements from each subject for each condition:

```{r}
t(xtabs(~ subject + condition, gg05e1))
```

We can visualize the different reading times of each of the 42 subjects as in Figure \@ref(fig:bysubjrt).

```{r bysubjrt,echo=FALSE,message=FALSE,out.width='98%', fig.show = "hold", fig.cap = "The subject (coded -1) vs. object (coded +1) relative clause reading times for each subject. Also shown are linear models fit separately to the data for each subject."}
gg_xyplot <- function(x, y, formula, shape, size, data) {
  ggplot(data = data, aes(
    x = data[, x],
    y = data[, y]
  )) +
    facet_wrap(formula) +
    geom_smooth(method = "lm") +
    geom_point(color = "blue", shape = shape, size = size) +
    theme(panel.grid.minor = element_blank()) +
    theme_bw() +
    #     scale_x_discrete(breaks=c("-1","0.5","0","0.5","1"),labels=c("OR","", "", "", "SR"))+
    scale_x_continuous(breaks = round(seq(-1, 1, by = 2), 1)) +
    ylab(y) +
    xlab("condition (-1: SR, +1: OR)")
}

gg_xyplot(
  x = "so", y = "logrt", ~subject,
  shape = 1, size = 3,
  data = gg05e1
)
```

It's clear from this figure that different subjects  have different effects of the  relative clause manipulation: some slopes are positive sloping, some are flat, and some are negatively sloping. There is between-subject variability in the relative clause effect.

Given these differences between subjects, you could fit a separate linear model for each subject, collect together the slopes for each subject, and then check if the slopes are significantly different from zero. There is a function in the package `lme4` that computes separate linear models for each subject: `lmList`.

```{r}
lmlist_m1 <- lmList(logrt ~ so | subject, gg05e1)
```

One can extract the intercept and slope estimates for each subject. For example, for subject 1:

```{r}
lmlist_m1$`1`$coefficients
```

Figure \@ref(fig:lmlistplot) shows the individual fitted linear models for each subject, as well as the  fit of a simple linear model `m1log` for all the data taken together (the gray line); this figure shows how each subject deviates in intercept and slope from the model `m1log`'s intercept and slope.

```{r lmlistplot,echo=FALSE, fig.cap="The thick gray line represents the intercept and slope estimate from the simple linear model m1log fit at the beginning of the chapter. The black lines are the individual linear model estimates of the intercept and slope for each subject, where each linear model is fit separately for each subject using the lmList function in the lme4 package."}
plot(gg05e1$so,
  gg05e1$logrt,
  axes = FALSE,
  xlab = "condition",
  ylab = "log rt"
)
axis(1,
  at = c(-1, 1),
  labels = c("SR", "OR")
)
axis(2)

subjects <- 1:42

for (i in subjects) {
  abline(lmlist_m1[[i]])
}

abline(lm(gg05e1$logrt ~ gg05e1$so), lwd = 3, col = "gray")
```


To find out if there is an effect of relative clause type, we simply need to check whether the slopes of the individual subjects' fitted lines taken together are significantly different from zero. A one-sample t-test will achieve this:


```{r}
summary_ttest(t.test(coef(lmlist_m1)[2]))
```


The above test is *exactly* the same as the paired t-test and the varying intercepts linear mixed model that we fit in the last chapter using the by-subject aggregated data. The only difference is that we are now using log reading times.

```{r}
bysubj <- aggregate(log(rawRT) ~ subject + condition,
  mean,
  data = gg05e1
)

colnames(bysubj)[3] <- "logrt"
```

Compare the paired t-test and linear mixed model results (the t-values):

```{r}
summary_ttest(t.test(logrt ~ condition, bysubj, paired = TRUE))

## compare with linear mixed model:
summary(lmer(
  logrt ~ condition + (1 | subject),
  bysubj
))$coefficients[2, ]
```

The t-test we carried out using the slopes from the `lmList` function is called *repeated measures regression*. This repeated measures regression model is now largely of historical interest, and useful only for understanding the linear mixed model, which will be our  standard approach.  

We turn next to three main types of linear mixed model; other variants will be introduced in later chapters. 

### Model type 1: Varying intercepts

We have already encountered this model. In the model shown below,  the statement 

\begin{equation}
(1 \mid \hbox{subject}) 
\end{equation}

adjusts the grand mean estimates of the intercept by a term (a number) for each subject. That was the term called $u_{0i}$ earlier in the chapter.


```{r}
m0_lmer <- lmer(logrt ~ so + (1 | subject), gg05e1)
```

Notice that we did not aggregate the data this time.

Here is the abbreviated output from model `m0_lmer`:

```
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 0.09983  0.3160  
 Residual             0.14618  0.3823  
Number of obs: 672, groups:  subject, 42

Fixed effects:
            Estimate Std. Error t value
(Intercept)  5.88306    0.05094 115.497
so           0.06202    0.01475   4.205
```

One thing to notice in the present example is that the coefficients (intercept and slope) of the fixed effects of the above model are identical to those in the linear model:

```{r}
m0_lm <- lm(logrt ~ so, gg05e1)
summary(m0_lm)
```


What is different between  the linear model and the linear mixed model is the standard error. In the latter, the standard error is determined by more than one source of variance, as we explain below.

The intercept adjustments for each subject can be viewed by typing:

```{r}
## first 10 subjects' intercept adjustments:
ranef(m0_lmer)$subject[, 1][1:10]
```
Figure \@ref(fig:latticeplotranefs) shows another way to summarize the adjustments to the grand mean intercept by subject. The error bars represent 95\% confidence intervals.

```{r latticeplotranefs,fig.cap="The varying intercepts by subject plotted using the dotplot in the lattice library. Also shown are 95% confidence intervals of each estimate."}
library(lattice)
print(dotplot(ranef(m0_lmer, condVar = TRUE)))
```

### The formal statement of the varying intercepts model

The model `m0_lmer` above prints out the following type of linear model.  $i$ indexes subject, and $j$ indexes items. 

Because this is a Latin square design, once we know the subject id and the item id, we know which subject saw which condition:


```{r}
subset(gg05e1, subject == 1 & item == 1)
```

The mathematical form of the linear mixed model is:

\begin{equation}
y_{ij} = \beta_0 + u_{0i}+\beta_1\times so_{ij} + \varepsilon_{ij}
\end{equation}

The *only* new thing here beyond the linear model we  saw earlier is the by-subject adjustment to the intercept. These by-subject adjustments to the intercept $u_{0i}$ are assumed by `lmer` to come from a normal distribution centered around 0: 

\begin{equation}
u_{0i} \sim Normal(0,\sigma_{u0})
\end{equation}

The ordinary linear model `m0_lm` has one intercept $\beta_0$ for all subjects, whereas this linear mixed model with varying intercepts `m0_lmer` has a different intercept ($\beta_0 + u_{0i}$) for each subject $i$.

Figure \@ref(fig:varyingintercepts) visualizes the adjustments for each subject to the intercepts.


```{r varyingintercepts,echo=FALSE,fig.cap="A visualization of the linear mixed model with varying intercepts for each subject (black lines). The model is `m0_lmer`. Also shown as a gray line is the fitted line corresponding to the simple linear model `m0`, without varying intercepts."}
a <- fixef(m0_lmer)[1]
newa <- a + ranef(m0_lmer)$subj

ab <- data.frame(newa = newa, b = fixef(m0_lmer)[2])

plot(gg05e1$so, gg05e1$logrt, xlab = "condition", ylab = "log rt", axes = F)
axis(1, at = c(-1, 1), labels = c("SR", "OR"))
axis(2)

for (i in 1:42) {
  abline(a = ab[i, 1], b = ab[i, 2])
}

abline(lm(logrt ~ so, gg05e1), lwd = 3, col = "gray")
```


An important point is that in this  model there are two variance components or sources of variance (cf. the linear model, which had only one):

- Between-subject variability in the intercept: $u_0 \sim Normal(0,\sigma_{u0})$
- Within-subject variability: $\varepsilon \sim Normal(0,\sigma)$

These two standard deviations  determine the standard error of the $\beta_1$ slope parameter.

### Model type 2: Varying intercepts and varying slopes, without a correlation

Unlike the figure associated with the `lmlist_m1` model above, which also involves fitting separate models for each subject, the model `m0_lmer` assumes *different intercepts* for each subject *but the same slope*. 

We can choose to fit different intercepts as well as different slopes for each subject. To achieve this, assume now that each subject's slope is also adjusted:

\begin{equation}
y_{ij} = \beta_0 + u_{0i}+(\beta_1+u_{1i})\times so_{ij} + \varepsilon_{ij}
\end{equation}

That is, we additionally assume that $u_{1i} \sim Normal(0,\sigma_{u1})$. The `lmer` notation for fitting separate intercepts and slopes is `(1+so||subject)`. The double vertical bar syntax will be explained soon.

```{r}
m1_lmer <- lmer(logrt ~ so + (1 + so || subject), gg05e1)
```

The output of this model shows that now there are not two but three sources of variability. These are:

- Between-subject variability in the intercept: $u_0 \sim Normal(0,\sigma_{u0})$
- Between-subject variability in the slope: $u_1 \sim Normal(0,\sigma_{u1})$
- Within-subject variability: $\varepsilon \sim Normal(0,\sigma)$

The model output reports the estimates of the three standard deviations:

- $\hat\sigma_{u0}=0.317$
- $\hat\sigma_{u1}=0.110$
- $\hat\sigma = 0.365$.

```
Random effects:
 Groups    Name        Variance Std.Dev.
 subject   (Intercept) 0.1006   0.317   
 subject.1 so          0.0121   0.110   
 Residual              0.1336   0.365   
Number of obs: 672, groups:  subject, 42

Fixed effects:
            Estimate Std. Error t value
(Intercept)   5.8831     0.0509  115.50
so            0.0620     0.0221    2.81
```

Figure \@ref(fig:varyinginterceptsslopes) visualizes these fits for each subject. As before, the gray line shows the model with a single intercept and slope, i.e., the simple linear model `m0_lm`):

```{r varyinginterceptsslopes,echo=FALSE,fig.cap="A visualization of the linear mixed model with varying intercepts as well as varying slopes for each subject (black lines). The model is `m1_lmer`. Also shown as a gray line is the fitted line corresponding to the simple linear model, without varying intercepts, `m0_lm`."}
a <- fixef(m1_lmer)[1]
b <- fixef(m1_lmer)[2]

newa <- a + ranef(m1_lmer)$subject[1]
newb <- b + ranef(m1_lmer)$subject[2]

ab <- data.frame(newa = newa, b = newb)

plot(gg05e1$so, gg05e1$logrt,
  xlab = "condition",
  ylab = "log rt", axes = F,
  main = "varying intercepts and slopes for each subject"
)
axis(1, at = c(-1, 1), labels = c("SR", "OR"))
axis(2)

for (i in 1:42) {
  abline(a = ab[i, 1], b = ab[i, 2])
}

abline(lm(logrt ~ so, gg05e1), lwd = 3, col = "gray")
```

#### Comparing lmList model with the varying intercepts model

Compare this `m1_lmer` model with the `lmlist_m1` model we fitted earlier. Figure \@ref(fig:lmlistlmercomparison) visualizes the difference between the two models.

```{r lmlistlmercomparison,echo=FALSE,fig.cap="The lmList model (left) vs. the varying intercepts and varying slopes model (right)."}
op <- par(mfrow = c(1, 2), pty = "s")

plot(gg05e1$so, gg05e1$logrt,
  axes = F, xlab = "condition",
  ylab = "log rt", main = "ordinary linear model"
)
axis(1, at = c(-1, 1), labels = c("SR", "OR"))
axis(2)

subjects <- 1:42

lmlistcoef <- coef(lmlist_m1)
a_lmlist <- lmlistcoef$`(Intercept)`
b_lmlist <- lmlistcoef$so

for (i in subjects) {
  abline(a = a_lmlist[i], b = b_lmlist[i])
}

abline(lm(logrt ~ so, gg05e1), lwd = 3, col = "gray")

a <- fixef(m1_lmer)[1]
b <- fixef(m1_lmer)[2]

newa <- a + ranef(m1_lmer)$subj[1]
newb <- b + ranef(m1_lmer)$subj[2]

ab <- data.frame(newa = newa, b = newb)

plot(gg05e1$so, gg05e1$logrt,
  axes = FALSE,
  main = "varying intercepts and slopes",
  xlab = "condition", ylab = "log rt"
)
axis(1, at = c(-1, 1), labels = c("SR", "OR"))
axis(2)

for (i in 1:42) {
  abline(a = ab[i, 1], b = ab[i, 2])
}

abline(lm(logrt ~ so, gg05e1), lwd = 3, col = "gray")
```

What is striking in Figure \@ref(fig:lmlistlmercomparison) is that in the linear mixed model plot, each subject's estimated best fit line is "gravitating" towards the gray line which represents the simple linear model fit. Compare the lines to those from the `lmList` fits; there, each subject's fitted line is based only on that subject's data, the lines don't gravitate towards the gray line. This gravitation in the linear mixed model of individual subjects' intercepts and slopes towards the grand mean intercepts and slopes  is called shrinkage. What is happening here is that the estimates for the intercept and slope for each subject is informed by two sources of information: the average behavior of all the subjects taken together, and the individual subjects' measurements. By contrast, in the `lmList` model, the individual subjects' fits are only informed by the individual subjects' data.

#### Visualizing random effects

As before, it is instructive to visualize the by-subjects adjustments to the intercept and slope. See Figure \@ref(fig:ranefsm1lmer).

```{r ranefsm1lmer,echo=FALSE,fig.cap="The varying intercepts and varying slopes in the model `m1_lmer`."}
print(dotplot(ranef(m1_lmer, condVar = TRUE)))
```

What Figure \@ref(fig:ranefsm1lmer) is showing is wide variability in the mean reading times between subjects, but relatively smaller variation in the slope between subjects.

#### The formal statement of the varying intercepts and varying slopes linear mixed model

Here is the formal statement of the varying intercept and varying slopes model. Again, i indexes subjects, j items.

\begin{equation}
y_{ij} = \beta_0 + u_{0i}+(\beta_1+u_{1i})\times so_{ij} + \varepsilon_{ij}
\end{equation}

There are three variance components, which partition the variation in the data to between-subjects and within-subjects variability:

- Between-subjects variability in the intercept: $u_0 \sim Normal(0,\sigma_{u0})$
- Between-subjects variability in the slope: $u_1 \sim Normal(0,\sigma_{u1})$
- Within-subject variability: $\varepsilon \sim Normal(0,\sigma)$

#### Crossed random effects for subjects and for items

The varying intercepts and varying slopes model  doesn't capture all the sources of variance yet. The items also contribute sources of variance: just like subjects, items may also vary in their  reading times or in the extent to which the reading times are impacted by condition. I other words, they might have different intercepts and slopes. 

Notice that in the Latin square design (which is standard in psycholinguistics or linguistic research) each subject sees all the items, but only one condition from each item. In such cases, we say that subjects and items are crossed.

```{r}
head(xtabs(~ subject + item, gg05e1))
```

A linear mixed model with crossed subject and items random effects can be defined with the following syntax:

```{r}
m2_lmer <- lmer(logrt ~ so + (1 + so || subject) +
  (1 + so || item), gg05e1)
```

Analogously to the preceding example, now there are five variance components:

```
Random effects:
 Groups    Name        Variance Std.Dev.
 subject   (Intercept) 0.10090  0.3177  
 subject.1 so          0.01224  0.1106  
 item      (Intercept) 0.00127  0.0356  
 item.1    so          0.00162  0.0402  
 Residual              0.13063  0.3614  
Number of obs: 672, groups:  subject, 42; item, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)   5.8831     0.0517  113.72
so            0.0620     0.0242    2.56
```

The item intercept and slope adjustments can be visualized as well. Notice that in the present data, there is a lot less item-level variation; this is often the case in planned experiments in sentence processing, where the experimental items are carefully constructed to vary as little as possible. 

```{r m2lmervarintvarsl,echo=FALSE,fig.cap="Varying intercepts and varying slopes by item in the model `m2_lmer`."}
print(dotplot(ranef(m2_lmer, condVar = TRUE))$item)
```

In the above models, there is an assumption that there is no correlation between the varying intercept and slope adjustments by subject, and no correlation between the varying intercept and slope adjustments by item. It is possible that the intercept and slope adjustments are in fact correlated.  We turn to this model next.

### Model type 3: Varying intercepts and varying slopes, with correlation

A correlation can be introduced between the intercept and slope adjustments by using a single vertical bar instead of two vertical bars in the random effects structure:

```{r}
m3_lmer <- lmer(
  logrt ~ so + (1 + so | subject) + (1 + so | item),
  gg05e1
)
```

To understand what this model is doing, we have to recall what a bivariate/multivariate distribution is.

```
Random effects:
 Groups   Name        Variance Std.Dev. Corr
 subject  (Intercept) 0.10103  0.3178       
          so          0.01228  0.1108   0.58
 item     (Intercept) 0.00172  0.0415       
          so          0.00196  0.0443   1.00 <= degeneracy
 Residual             0.12984  0.3603       
Number of obs: 672, groups:  subject, 42; item, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)   5.8831     0.0520  113.09
so            0.0620     0.0247    2.51
```


The correlations (0.58 and 1.00) you see in the model output above are the correlations between the varying intercepts and slopes for subjects and for items. The variance covariance matrix for items is called "degenerate". This means that, because its correlation is 1, the matrix cannot be inverted. 

When the correlation is +1 or -1 or near these numbers, this means that the optimizer in `lme4` is unable to estimate the correlation parameter. This is almost always due to there not being enough data. If you are in such a situation, you are better off not trying to estimate this parameter with the data you have, and instead fitting one of the simpler models. We will return to this point when discussing model selection.  For further discussion, see @barr2013,  @BatesEtAlParsimonious, and @hannesBEAP.

#### Formal statement of varying intercepts and varying slopes linear mixed model with correlation

As usual, i indexes subjects, j items. The vector `so` is the sum-coded factor levels: +1 for object relatives and -1 for subject relatives. The only new thing in this model is the item-level effects, and the specification of the variance-covariance matrix for subjects and items, in order to include the correlation parameters.

\begin{equation}
y_{ij} = \beta_0 + u_{0i} + w_{0j} + (\beta_1 + u_{1i} + w_{1j}) \times so_{ij} + \varepsilon_{ij}
\end{equation}

where $\varepsilon_{ij} \sim Normal(0,\sigma)$ and 

\begin{equation}\label{eq:covmatLM}
\Sigma_u
=
\begin{pmatrix}
\sigma _{u0}^2  & \rho _{u}\sigma _{u0}\sigma _{u1}\\
\rho _{u}\sigma _{u0}\sigma _{u1}    & \sigma _{u1}^2\\
\end{pmatrix}
\quad 
\Sigma _w
=
\begin{pmatrix}
\sigma _{w0}^2  & \rho _{w}\sigma _{w0}\sigma _{w1}\\
\rho _{w}\sigma _{w0}\sigma _{w1}    & \sigma _{w1}^2\\
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jointpriordistLM}
\begin{pmatrix}
  u_0 \\ 
  u_1 \\
\end{pmatrix}
\sim 
\mathcal{N} \left(
\begin{pmatrix}
  0 \\
  0 \\
\end{pmatrix},
\Sigma_{u}
\right),
\quad
\begin{pmatrix}
  w_0 \\ 
  w_1 \\
\end{pmatrix}
\sim 
\mathcal{N}\left(
\begin{pmatrix}
  0 \\
  0 \\
\end{pmatrix},
\Sigma_{w}
\right)
\end{equation}


#### Visualizing the random effects

One can visualize the correlation between intercepts and slopes by subjects. The positive correlation of 0.58 between subject intercept and slope adjustments implies that slower subjects show larger effects. However, the dotplot below doesn't show a convincing indication that such a correlation exists: 

```{r}
print(dotplot(ranef(m3.lmer, condVar = TRUE))$subject)
```

The correlation pattern is easier to see if we plot the slope adjustments against the intercept adjustments. See Figure \@ref(fig:m3lmervarintsl).

```{r m3lmervarintsl,fig.cap="The varying intercepts and varying slopes by subject in the model `m3_lmer`."}
plot(ranef(m3_lmer)$subject[, 1], ranef(m3_lmer)$subject[, 2],
  xlab = "Intercept adjustments (subjects)",
  ylab = "Slope adjustments (subjects)"
)
```

When we talk about hypothesis testing, we will look at what inferences we can draw from this correlation.

Figure \@ref(fig:m3lmervarintsl) is a 
dotplot showing the item-level effects. The plot suggests a perfect correlation between intercept and slope adjustments, but as mentioned above, this correlation is a result of a failure to estimate the correlation. It is not meaningful.

```{r m3lmervarintsl,echo=FALSE,fig.cap="Varying intercepts and varying slopes by item from the model `m3_lmer`."}
print(dotplot(ranef(m3_lmer, condVar = TRUE))$item)
```

For these data, the appropriate model would be one without correlations for items:

```{r}
m4_lmer <- lmer(
  logrt ~ so + (1 + so | subject) + (1 + so || item),
  gg05e1
)
summary(m4_lmer)
```

```{r echo=FALSE}
b0<-round(summary(m4_lmer)$coefficients[1,1],2)
b1<-round(summary(m4_lmer)$coefficients[2,1],2)
tval<-round(summary(m4_lmer)$coefficients[2,3],2)

```

The double vertical bar for the item random effects is the syntax in `lme4` for removing the correlation term that the model was unable to estimate. For the @grodner data, model `m4_lmer` is the one we would report in a paper. Specifically, the wording that would be appropriate is the following: "The estimate of the relative clause effect was `r exp(b0+b1)-exp(b0-b1)` ms, with a t-value of `r tval`. We can therefore reject the null hypothesis of no difference between the two relative clause types." 

In a balanced data set like this one (i.e., with no missing data), the t-test on the slope is sufficient for drawing inferences about the null hypothesis test. However, when there is missing data (this happens for example in eyetracking data), the likelihood ratio test discussed in the next chapter is the more general method.  

## Shrinkage in linear mixed models

The estimate of the effect for each participant computed from a linear mixed model "gravitates" towards the grand mean effect compared to when we fit a separate linear model to each subject's data. 
This is called "shrinkage" in linear mixed models. We say that the individual-level estimates are shrunk towards the mean slope.
The less data we have from a given subject, the greater the shrinkage. Figure \@ref(fig:shrinkage) illustrates this. 

```{r shrinkage,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "The figures show linear model fits (the grand mean estimates) for three subjects with unusually large intercepts and slopes; shown are the simple linear model fit on all the data (gray line), the `lmList` model fit to the individual subject's data (black line), and the linear mixed model fit (the broken line). In all three subjects' models, the linear mixed model estimates are shrunk towards the grand mean (gray line) estimates.", tidy = FALSE}
op<-par(mfrow=c(1,3),pty="s")
coefs<-coef(lmlist_m1)
intercepts<-coefs[1]
colnames(intercepts)<-"intercept"
slopes<-coefs[2]

plot(jitter(gg05e1$so,.5),
        gg05e1$logrt,axes=FALSE,
        xlab="Condition",ylab="rt (log ms)",
        main="Subject 28's estimates",
     col="white")
axis(1,at=c(-1,1),labels=c("SR","OR"))
axis(2)
## no pooling estimate:
abline(intercepts$intercept[28],slopes$so[28])
## partial pooling
u<-ranef(m1_lmer)$subject
u0<-u$`(Intercept)`
u1<-u$so
b0<-summary(m1_lmer)$coefficients[1,1]
b1<-summary(m1_lmer)$coefficients[2,1]

abline(b0+u0[28],b1+u1[28],lty=2)
## complete pooling:
abline(lm(log(rawRT)~so,gg05e1),lwd=3,col="gray")

plot(jitter(gg05e1$so,.5),
        log(gg05e1$rawRT),axes=FALSE,
        xlab="Condition",ylab="rt (log ms)",
        main="Subject 36's estimates",col="white")
axis(1,at=c(-1,1),labels=c("SR","OR"))
axis(2)

## no pooling estimate:
abline(intercepts$intercept[36],slopes$so[36])
## partial pooling:
abline(b0+u0[36],b1+u1[36],lty=2)
## complete pooling:
abline(lm(log(rawRT)~so,gg05e1),lwd=3,col="gray")

plot(jitter(gg05e1$so,.5),
        log(gg05e1$rawRT),axes=FALSE,
        xlab="Condition",ylab="rt (log ms)",
        main="Subject 37's estimates",col="white")
axis(1,at=c(-1,1),labels=c("SR","OR"))
axis(2)
## no pooling estimate:
abline(intercepts$intercept[37],slopes$so[37])
## partial pooling:
abline(b0+u0[37],b1+u1[37],lty=2)
## complete pooling:
abline(lm(log(rawRT)~so,gg05e1),lwd=3,col="gray")
```


#### Shrinkage in action: when data are missing

The importance and value of shrinkage becomes clear once we simulate a situation where there is some missing data. Missingness can happen in experiments, either due to lost measurements (arising from computer error or programming errors), or some other reason.  To see what happens when we have missing data, let's randomly delete some data from one subject. We will randomly delete 50% of subject 37's data:

```{r}
set.seed(4321)
## choose some data randomly to remove:
rand <- rbinom(1, n = 16, prob = 0.5)
```

Here are subject 37's reading times (16 data points):

```{r}
gg05e1[which(gg05e1$subject == 37), ]$rawRT
```

Now, we randomly delete half the data:

```{r}
gg05e1$deletedRT <- gg05e1$rawRT
gg05e1[which(gg05e1$subject == 37), ]$deletedRT <-
  ifelse(rand, NA,
    gg05e1[which(gg05e1$subject == 37), ]$rawRT
  )
```


Now subject 37's estimates are going to be even more inaccurate than with 16 data points, because they are based on much less data (even one extreme value can strongly influence the mean):


```{r}
subset(gg05e1, subject == 37)$deletedRT
```

```{r echo=FALSE}
## original no pooling estimate:
lmlist_m1_old <- lmList(log(rawRT) ~ so | subject, gg05e1)
coefs_old <- coef(lmlist_m1_old)
intercepts_old <- coefs_old[1]
colnames(intercepts_old) <- "intercept"
slopes_old <- coefs_old[2]
## subject 37's original estimates:
intercepts_old$intercept[37]
slopes_old$so[37]
```

```{r echo=FALSE}
## on deleted data:
lmlist_m1_deleted <- lmList(log(deletedRT) ~ so | subject, gg05e1)
coefs <- coef(lmlist_m1_deleted)
intercepts <- coefs[1]
colnames(intercepts) <- "intercept"
slopes <- coefs[2]
## subject 37's new estimates on deleted data:
intercepts$intercept[37]
slopes$so[37]
```

Now fit the hierarchical model and examine subject 37's estimates on undeleted vs. deleted data. Figure \@ref(fig:shrinkage2) illustrates the difference between the different models.

```{r shrinkage2,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "The figure shows linear model fits (the grand mean estimates) for subject 37. When using `lmList`, deleting data leads to very different estimates; but using `lmer`, deleting half the data from this subject hardly affects the individual subject's estimates.", tidy = FALSE}
plot(jitter(gg05e1$so,.5),
        log(gg05e1$rawRT),axes=FALSE,
        xlab="Condition",ylab="rt (log ms)",
        main="Subject 37's estimates",ylim=c(4.5,10),
     col="white")
axis(1,at=c(-1,1),labels=c("SR","OR"))
axis(2)

## deleted data estimate lmList:
abline(intercepts$intercept[37],slopes$so[37])
## original data estimate lmList:
abline(intercepts_old$intercept[37],slopes_old$so[37],lwd=4)

## partial pooling, original data:
u0_old<-u0
u1_old<-u1
b0_old<-b0
b1_old<-b1

m1_lmer_deleteddata<-lmer(log(deletedRT)~so + (1+so||subject),gg05e1)
b0<-summary(m1_lmer_deleteddata)$coefficients[1,1]
b1<-summary(m1_lmer_deleteddata)$coefficients[2,1]
u<-ranef(m1_lmer_deleteddata)$subject
u0<-u$`(Intercept)`
u1<-u$`so`

## LMM on deleted data:
abline(b0+u0[37],b1+u1[37],lty=2,col="gray",lwd=1)
## LMM on original data:
abline(b0_old+u0_old[37],b1_old+u1_old[37],lty=2,col="gray",lwd=3)

## complete pooling:
abline(lm(log(rawRT)~so,gg05e1),lwd=3,col="gray")

legend(x=-1,y=10,
       lty=c(1,1,2,2,1),
       lwd=c(1,4,1,2,3),
       col=c("black","black","gray","gray","gray"),
       legend=c("lmList  deleted data",
                "lmList  original data",
                "lmer  deleted data",
                "lmer  original data",
                "linear model original data"))

```

What we see in Figure \@ref(fig:shrinkage2) is that the estimates from the hierarchical model are barely affected by the missingness, but the estimates from the `lmList` model are heavily affected by the missingness.

The upshot of shrinkage i that linear mixed models will give you more robust estimates (think Type M error!) compared to models which fit data separately for each subject. This property of shrinkage is one reason why linear mixed models are so important in cognitive science, and why they should be the standard workhorse for data analysis. 

## Summary 



## Further reading

## Exercises {#sec:LMExercises1}

```{exercise, LMsttest}
By-subjects t-test in a two-condition design
```

Load the data-set below:

```{r}
data("df_gibsonwu")
```

The data consist of a repeated measures experiment (self-paced reading) involving Chinese relative clauses. The experiment compares two conditions which are labeled obj-ext and subj-ext. The column subj refers to subject id, and the column item refers to item id. The column labeled rt refers to reading time in milliseconds at a critical word in the sentence. This is a standard Latin square design. 

Using rt as a dependent variable, carry out the appropriate by-subjects and by-items t-test to evaluate the null hypothesis that there is no difference between the two conditions. Write down all the R commands needed to do the appropriate t-tests, and the resulting t-values and p-values. State whether we can reject the null hypothesis given the results of the t-tests; explain why.

```{exercise, LMMGibsonWu}
A linear mixed model for the two-condition design
```

Now,  fit a linear mixed model (call it M1). Recode the column called Type as  sum contrasts ($\pm 0.5$).  

```{r echo=FALSE}
df_gibsonwu$so <- ifelse(df_gibsonwu$type == "obj-ext", -1 / 2, 1 / 2)
```

Assume varying intercepts for subjects and varying intercepts for items (varying intercepts are sometimes called random intercepts). Write down the linear mixed models command, and  write down the fixed-effects estimates (intercept and slope) along with their standard errors. State whether we can reject the null hypothesis given the results of the t-value shown in the linear mixed model output; explain why.

```{exercise, LMMGibsonWuLogRT}
Raw RTs vs. Log RTs
```

Repeat the t-test and the linear mixed model with log reading times as the dependent measure, instead of raw reading times. Does the conclusion change compared to the analyses using raw reading times?

```{exercise, LMMGibsonWuPower}
Power analysis on the Gibson and Wu data
```

```{r echo=FALSE}
crit_subj <- aggregate(rt ~ subj + type, mean,
  data = df_gibsonwu
)
d <- subset(crit_subj, type == "subj-ext")$rt -
  subset(crit_subj, type == "obj-ext")$rt
stddev <- sd(d)
n <- length(d)
SE <- round(stddev / sqrt(n))
```

The researcher wants to achieve 80% statistical power in a future study that replicates the Gibson and Wu study. Based on the available data above, she determines that the standard error (note: not the standard deviation!) of the difference in means between the two conditions is `r SE`.  She has reason to believe that the true  difference in means is $30$ ms. What is the number of participants  needed to  achieve approximately 80% power? Use the power.t.test function to compute your answer. Write down the `power.t.test` function specification you used, as well as the number of participants needed, based on the output of the power.t.test function. 

```{exercise, LMMGibsonWuresiduals}
Residuals in a linear mixed model for the two-condition design
```
The plot below shows the distribution of the residuals from model M (fit on raw reading times) plotted against the standard normal distribution with mean 0 and  standard  deviation 1. Explain what the plot tells us  about one of the model assumptions of the linear mixed model M that we fit earlier.

(You can ignore the numbers below the plot.)

```{r echo=FALSE,message=FALSE,results="asis"}
M <- lmer(
  rt ~ so + (1 | subj) + (1 | item),
  df_gibsonwu
)

qqnorm(residuals(M))
```

```{exercise, LMMGibsonWuContrastCoding}
Understanding contrast coding
```

Using only your estimates of the intercept and the slope in model M's fixed effects output (the model fit on raw reading times), write down the mean of the  condition labeled obj-ext in the data, and the mean of the condition labeled subj-ext.

```{exercise, LMMGibsonWuFE}
Understanding the fixed-effects output
```

Suppose that the model M's output for the fixed effects analysis were as follows. `so` is the sum-coded contrast specification shown above.

```{r echo=FALSE}
results <- summary(M)$coefficients
results[2, 3] <- 2
results[2, 2] <- NA
```

```{r}
results
```

What is the value of the standard error of the slope (`so`), which is labeled NA above?

```{exercise, LMMGibsonWuNHST}
Understanding the null hypothesis test
```

A researcher fits a linear mixed model a new data set to compare the reading times between two conditions (a) and (b), just like in the  above study. Her hypothesis is that the mean for condition (a) is larger than the  mean for (b). She observes that condition (a) has sample mean 500 ms, and condition (b) has  sample mean 450 ms.  She also establishes from the linear mixed model that the t-value is 1.94. The approximate p-value associated with  this t-value is `r round(2*pnorm(-1.94),3)`. Answer the following: (A) Do we have evidence against the null hypothesis, and (B) do we have evidence for the particular research hypothesis that the researcher has? 

The researcher runs the  same analysis as above on  a new data-set that has the same design as above, and now gets a p-value of 0.001. What is the approximate t-value here (assume that the standard normal distribution can be used to approximate the t-value)?  Now she has stronger evidence than in the above case where the p-value was `r round(2*pnorm(-1.94),3)`. What does she have stronger evidence for? Does she have stronger evidence for the null hypothesis, or stronger evidence in favor of her particular research hypothesis (or both)?
