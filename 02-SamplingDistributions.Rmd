# Hypothetical repeated sampling and the t-test

This chapter introduces some of the foundational ideas behind hypothesis testing in the frequentist framework. The key idea is that of hypothetical repeated sampling. When we fit a model to a given data-set, we are assuming that this is one of  potentially infinite numbers of exact repetitions of an experiment. We will leverage some amazing  properties of these exact repetitions in order to draw inferences from our particular data. The key idea to understand here is the central limit theorem, which we will discuss below. Before we do that, it is useful to introduce some terminology relating to typical experiment designs in linguistics and psychology.

## Some terminology surrounding typical experiment designs in linguistics and psychology

An experiment typically involves taking a *random sample* from some population. For example, we could take a sample of participants and record their reading times for two types of sentences, for example easy and difficult sentences. How one operationalizes easy and difficult is not important at this point. However, to make the  discussion concrete, consider this well-known example from psycholinguistics. We could be comparing reading times in active vs. passive sentences:

Active sentence: The man threw the ball.

Passive sentence: The ball was thrown by the man.

One claim in the literature is that, due to their simpler structure, active sentences are easier to process than passive sentences. So this is an example of a simpler vs. complex sentence. If we want to investigate whether actives are easier to process than passives, we can, as an example, compare the total reading times for each sentence type (there are some problems that arise here because the sentences don't have the same length, so the comparison is not really fair; these issues we will discuss later). 

Technically, we are supposed to randomly choose participants; in practice, this randomization rarely happens. Instead we take whatever participants we get,  such as university students who happen to apply to participate in an experiment! So, random sampling is an assumption in all the discussions in this chapter, but in practice this assumption may or may not be perfectly met.

One design decision that the researcher must take is whether to collect only one response from each participant in each condition, or whether we collect multiple measures from each participant. If we collect only one data point from each participant, we will say that the data points are *independent* of each other. When we collect multiple measurements from each participant, we will say that we have *dependent* data; such multiple measurements are also called *repeated measures*. With repeated measures, there will be some correlation between the data-points because there are  multiple measurements from a common source.

In psycholinguistics, repeated measures experiments are the norm. Furthermore, it is common to use a so-called *Latin Square* design. The term Latin Square refers to the fact that the ordering of the conditions forms a square (Table \@ref(tab:LatinSquare)); the word Latin apparently only refers to the fact the symbols for each condition were from the Latin script.

```{r LatinSquare,echo=FALSE,results="asis"}
latsq<-data.frame(rbind(letters[1:2],letters[2:1]))
colnames(latsq)<-c("Group 1","Group 2")

kableExtra::kable(latsq,
                  digits = 2, booktabs = TRUE, 
                  vline = "", # format="latex",
  caption = "The Latin Square with two conditions labeled a and b.")
```

A characteristic property of the Latin Square is that, as shown in Table \@ref(tab:LatinSquare), each condition appears in each row exactly once. The Latin square generalizes beyond two conditions easily. Suppose we have four conditions; then, the Latin Square is as in Table \@ref(tab:LatinSquare4). Similarly, if there are eight conditions, the Latin Square table would have the form shown in Table \@ref(tab:LatinSquare8). 


```{r LatinSquare4,echo=FALSE,results="asis"}
latsq4<-data.frame(rbind(letters[1:4],
                        letters[c(2:4,1)],
                  letters[c(3:4,1:2)],
                  letters[c(4,1:3)]))
colnames(latsq4)<-c("Group 1","Group 2","Group 3","Group 4")

kableExtra::kable(latsq4,
                  digits = 2, booktabs = TRUE, 
                  vline = "", # format="latex",
  caption = "A Latin Square design with four conditions labeled a, b, c, and d.")
```


```{r LatinSquare8,echo=FALSE,results="asis"}
latsq8<-data.frame(rbind(letters[1:8],
                        letters[c(2:8,1)],
                  letters[c(3:8,1:2)],
                  letters[c(4:8,1:3)],
                  letters[c(5:8,1:4)],
                  letters[c(6:8,1:5)],
                  letters[c(7:8,1:6)],
                  letters[c(8,1:7)]
                  ))
colnames(latsq8)<-paste(rep("G",8),1:8,sep="")

kableExtra::kable(latsq8,
                  digits = 2, booktabs = TRUE, 
                  vline = "", # format="latex",
  caption = "A Latin Square design with eight conditions labeled a, b, c, d, e, f, g, and h. The column names representing the eight groups are abbreviated as G1-G8.")
```

As an aside, it is rarely a good idea to design such complex experiments as the one shown in Table \@ref(tab:LatinSquare8). The more conditions you have in an experiment, the more decisions you will have to make about how to analyze the data, the more complex the model will get, and the harder the interpretation of the data. Later in this course, we will demonstrate the price one has to pay for setting up overly complex designs. This point is mentioned here because it is a common beginner error to want to have as many conditions as possible in a single experiment. In fact, the first experiment a student of ours once proposed to us was a 180 condition design. 

The Latin Square design is attractive for psycholinguistics and psychology because it has some important optimality properties. Experiment design and the properties of different designs is a whole field in itself in statistics, but we won't get into these details.  

The Latin square design can be extended so that each participant sees repeated sets of each condition. For example, in a two-condition design, we may decide to create multiple (say 4) sets of items: item 1 will have two instances of conditions a and b; item 2 to 4 will each have two instances of conditions a and b. The Latin square then looks like the one shown in Table \@ref(tab:LatinSquarerep2).

```{r LatinSquarerep2,echo=FALSE,results="asis"}
latsqrep2<-data.frame(cbind(item=1:4,rep(letters[1:2],2),rep(letters[2:1],2)))

colnames(latsqrep2)<-c("item","Group 1","Group 2")

kableExtra::kable(latsqrep2,
                  digits = 2, booktabs = TRUE, 
                  vline = "", # format="latex",
  caption = "The Latin Square with two conditions labeled a and b, and four items.")
```

Now, when each incoming participants is randomly assigned to Group 1 or Group 2, they will see two instances of each condition, but---crucially--each instance of each item will be seen only once. This has the advantage that we obtain repeated measures from each participant for each condition, leading to more accurate estimates from each participant for each condition; and we obtain measurements from multiple items as well. However, each participant sees each item only once; this can be important in cases where showing the same item to the participant more than once could bias their response. For example, if we show the same word twice to a participant, they might process the second instance of that word more easily, biasing the average response to that word. There can however be situations where it is appropriate/necessary to show the same item multiple times to a participant. In most of the designs considered in this book, participants will see one item only once.

Having repeated measurements from participants and from items allows us to generalize beyond the specific participants and items that we have in the experiment.

To come closer to a realistic experiment design, suppose that we have two participants (this is just for illustration---normally we will have many more than two participants). Then, the data frame will look like this:

```{r}
condition<-c(rep(letters[1:2],2),rep(letters[2:1],2))
item<-rep(1:4,2)
subj<-rep(1:2,each=4)
df_example<-data.frame(subj,item,condition)
df_example
```

As a consequence of this design, we say that we have a fully crossed subjects (participants) and items design: each participant sees exactly one item:

```{r}
xtabs(~subj+item,df_example)
```

You can also check that each subject sees two instances of each condition:

```{r}
xtabs(~subj+condition,df_example)
```

Notice also that there is exactly one measurement from each item for each condition. This is because only two subjects are involved here. If there were 10 subjects, there would be five instance of each condition for each item.

```{r}
xtabs(~item+condition,df_example)
```

So, one consequence of the above repeated measurements design is that there will, in general, be repeated measurements not just from subjects but also from items. Thus, items can also be seen as producing dependent data.  Later on, we will see that this design has far-reaching consequences regarding the type of data analysis one can and should do. 

This is a very brief introduction to experiment design; but the reader should keep in mind that these are the kinds of designs we will primarily focus on in this book. 
We now turn to the idea of hypothetical repeated sampling, and the central limit theorem.

## The central limit theorem using simulation

Suppose we collect some data, which can be represented by a vector $y$; this is a *single sample*. Given data $y$, and assuming for concreteness that the underlying likelihood is a $Normal(\mu=500,\sigma=100)$, the sample mean and standard deviation, $\bar{y}$ and $s$  give us an estimate of the unknown parameters mean $\mu$ and the standard deviation $\sigma$ of the distribution from which we assume that our data come from. Figure \@ref(fig:normalsample) shows the distribution of a particular sample, where the number of data points is $n=1000$. Note that in this example the parameters are specified by us, so they are not unknown; in a real data-collection situation, the sample mean and standard deviation are all we have as estimates of the parameters.

```{r normalsample,out.width='75%', fig.show = "hold", fig.cap = "A sample of data y of size n=1000, from the distribution  Normal(500,100). The vertical line shows the true mean of the distribution we are sampling from.", tidy = FALSE}
## sample size:
n<-1000
##  independent and identically distributed sample:
y<-rnorm(n,mean=500,sd=100)
## histogram of data:
hist(y,freq=FALSE)
## true value of mean:
abline(v=500,lwd=2)
```

Suppose now that you had not a single sample of size 1000 but many repeated samples. This isn't something one can normally do in real life; we often run a single experiment or, at most, repeat the same experiment once. However, one can simulate repeated sampling easily within R. Let us take 100 repeated samples like the one above, and save the samples in a matrix containing $n=1000$ rows and 2000 columns, each column representing an experiment:

```{r}
mu <- 500
sigma <- 100
## number of experiments:
k <- 2000
## store for data:
y_matrix <- matrix(rep(NA, n * k), ncol = k)
for (i in 1:k) {
  ## expt result with sample size n:
  y_matrix[, i] <- rnorm(n, mean = mu, sd = sigma)
}
```

Now, if we compute the means $\bar{y}_k$ of *each* of the $k=1,\dots,100$ experiments we just carried out, if certain conditions are met, these means will be normally distributed, with mean $\mu$  and standard deviation $\sigma/\sqrt{n}$. To understand this point, it is useful to first visualize the distribution of means and graphically summarize this standard deviation, which confusingly is called *standard error*.

```{r}
## compute means from each replication:
y_means <- colMeans(y_matrix)
## the mean and sd (=standard error) of the means
mean(y_means)
sd(y_means)
```

```{r sdsmnormal,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Sampling from a normal distribution (left); and the sampling distribution of the means under repeated sampling (right). The right-hand plot shows an overlaid normal distribution, and the standard deviation (standard error) as error bars.", tidy = FALSE}
op<-par(mfrow=c(1,2),pty="s")

for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from a \n Normal distribution",
     xlab="")
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}

hist(y_means,freq=FALSE,main="Sampling distribution of means",
     xlab="means under repeated sampling",
     ylim=c(0,0.15))
abline(v=mean(y_means),lty=2)
lines(seq(9,11,by=0.01),
      dnorm(seq(9,11,by=0.01),mean(y_means),sd(y_means)))
arrows(x0=mean(y_means)-sd(y_means),
      y0=0.04,
      x1=mean(y_means)+sd(y_means),
      y1=0.04,angle = 90,code=3)
lines(seq(490,510,by=0.01),
      dnorm(seq(490,510,by=0.01),
            mean(y_means),sd(y_means)))
```

The sampling distribution of means has a  normal distribution provided two conditions are met: (a) the sample size should be ``large enough'', and (b)  $\mu$ and $\sigma$ are defined for the probability density or mass function that generated the data. This fact is called the **central limit theorem** (CLT). The significance of the CLT for us as researchers is that from the summary statistics computed from a *single* sample, we can obtain an estimate of this distribution of means: $Normal(\bar{y},s/\sqrt{n})$.  




The statement that the sampling distribution of the means will be normal, with mean $\mu$  and standard deviation $\sigma/\sqrt{n}$, can be derived  formally through a surprisingly simple application of random variable theory. Suppose we gather independent and identically distributed data $y_1, \dots, y_n$, each of which is generated by a random variable $Y\sim Normal(\mu,\sigma)$.

When we compute the mean $\bar{y}$ for each sample, we are assuming that each of the means is coming from a random variable $\bar{Y}$, which is just a linear combination of values generated by instances of the random variable $Y$, which itself has a pdf with mean (expectation) $\mu$ and variance $\sigma^2$:

\begin{equation}
\bar{Y}=\frac{1}{n} \sum_{i=1}^n Y = \frac{1}{n}Y_1 + \dots + \frac{1}{n}Y_n
 \end{equation}

So, the expectation of $\bar{Y}$ is 

\begin{equation}
\begin{split}
E[\bar{Y}] =& E[\frac{1}{n}Y_1 + \dots + \frac{1}{n}Y_n]\\
=& \frac{1}{n} (E[Y] + \dots + E[Y])\\
=& \frac{1}{n} (\mu + \dots + \mu)\\
=& \frac{1}{n} n\mu \\
=& \mu \\
\end{split}
\end{equation}

And the variance of $\bar{Y}$ is

\begin{equation}
\begin{split}
Var(\bar{Y}) =& Var(\frac{1}{n}Y_1 + \dots + \frac{1}{n}
Y_n)\\
=& \frac{1}{n^2} Var(Y_1 + \dots + Y_n)\\
\end{split}
\end{equation}

The last line above arises because the variance of a random variable $Z$ multiplied by a constant $a$, $Var(aZ)$ is $a^2 Var(Z)$. Here, $a=1/n$, so $a^2 = 1/n^2$. 
Because $Y_1,\dots,Y_n$ are independent, we can compute the variance $Var(Y_1 + \dots + Y_n)$ by using the fact that the variance of the sum of independent random variables is the sum of their variances. This fact gives us:

\begin{equation} \label{sdsmderivation}
\begin{split}
\frac{1}{n^2} Var(Y_1 + \dots + Y_n) =& \frac{1}{n^2} (Var(Y) + \dots + Var(Y))\\
=&  \frac{1}{n^2}  n Var(Y)\\
=&  \frac{1}{n}  Var(Y)\\
=&  \frac{\sigma^2}{n}\\
\end{split}
\end{equation}

This derives the above result that the expectation (i.e., the mean) and variance of the sampling distribution of the sample means are

\begin{equation}
E[\bar{Y}] = \mu \quad Var(\bar{Y}) = \frac{\sigma^2}{n}
\end{equation}


The above means that we can estimate the expectation $\bar{Y}$ and the variance  of $\bar{Y}$ from a *single* sample. (Of course, whether these estimates are accurate or not is subject to variation, as we will see below).

The Central Limit Theorem, not proved here (for a proof, see p. 267 of @millermiller) can be  summarized as follows.

**Central Limit Theorem**

Let $f(Y)$ be the pdf of a random variable $Y$, and assume that the pdf has mean $\mu$ and variance $\sigma^2$. Then:

\begin{equation}
\bar{Y} \sim Normal(\mu,\sigma^2/n) \quad  E[Y]=\mu,Var(Y)=\sigma^2 \quad \hbox{ when n is large}
\end{equation}

For us, the practical implication of this result is huge. From a *single* sample of $n$ independent data points $y_1,\dots, y_n$, we can derive the distribution of *hypothetical* sample means under repeated sampling. That is, it becomes possible to say something about what the plausible and implausible values of the sample mean are under repeated sampling. This is the basis for all hypothesis testing and statistical inference in the frequentist framework that we will look at in this book.

Sometimes the central limit theorem is misunderstood to imply that the pdf $f(Y)$ that is assumed to generate the data is always going to be normal. It is important to understand that there are two pdfs we are talking about here. First, there is the pdf that the data were generated from; this need not be normal. For example, you could get data from a Normal, Exponential, Gamma, or other distribution. Second, there is the sampling distribution of the *sample mean* under repeated sampling. It is the sampling distribution that the central limit theorem is about, not the distribution that generated the data.

## Three examples of the sampling distribution

In the above discussion, the underlying pdf we sampled from above was a normal distribution. However, it need not be. Consider two examples first: the underlying pdf is an Exponential or a Gamma distribution. 

The Exponential distribution has a parameter $\lambda$ (parameterized in `R` as a `rate`, $1/\lambda$); its mean is $\lambda$ and its variance is $1/\lambda^2$. The sampling distribution is normal, even though the underlying distribution is an Exponential; see Figure \@ref(fig:sdsmexp).

```{r sdsmexp,echo=FALSE,out.width='85%', fig.show = "hold", fig.cap = "Sampling from an exponential.", tidy = FALSE}
## Sampling from an Exponential:
## number of experiments:
k<-2000
y_matrix<-matrix(rep(NA,n*k),ncol=k)
for(i in 1:k){
  y_matrix[,i]<-rexp(n,rate=1/10)
}

op<-par(mfrow=c(1,2),pty="s")
for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from an \n  Exponential distribution",
     xlab="",ylim=c(0,0.15))
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}

## compute means from each replication:
y_means<-colMeans(y_matrix)
hist(y_means,freq=FALSE,main="Sampling distribution of means \n (Data sampled \n from Exponential)",
     xlab="means under repeated sampling",
     ylim=c(0,2))
abline(v=mean(y_means),lty=2)
lines(seq(9,11,by=0.01),
      dnorm(seq(9,11,by=0.01),mean(y_means),sd(y_means)))
```

A further example is samples from a Gamma distribution. The Gamma distribution has two parameters, a and b, and is written Gamma(a,b).
In R, the parameters a and b are called shape and rate, respectively. The mean of the Gamma distribution is $\frac{a}{b}$ and the variance is $\frac{a}{b^2}$. Suppose we sample from a Gamma distribution with shape parameter chosen arbitrarily to be 1. The distribution of means is again going to be approximately normal; see Figure \@ref(fig:sdsmgamma).


```{r sdsmgamma,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Sampling from a Gamma distribution.", tidy = FALSE}
## Sampling from a Gamma:
## number of experiments:
k<-2000
y_matrix<-matrix(rep(NA,n*k),ncol=k)
for(i in 1:k){
  y_matrix[,i]<-rgamma(n,shape=1,rate=1)
}

op<-par(mfrow=c(1,2),pty="s")
for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from a \n Gamma distribution",
     xlab="")
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}
## compute means from each replication:
y_means<-colMeans(y_matrix)
hist(y_means,freq=FALSE,main="Sampling distribution of means \n (Data sampled from Gamma)",
     xlab="means under repeated sampling")
abline(v=mean(y_means),lty=2)
lines(seq(.9,1.1,by=0.01),
      dnorm(seq(.9,1.1,by=0.01),mean(y_means),sd(y_means)))
```

As a final example, consider what happens if sample from a distribution, the Cauchy, that doesn't have any mean or variance defined for it. 

```{r sdsmcauchy,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Sampling from a Cauchy distribution.", tidy = FALSE}
## Sampling from a Cauchy:
## number of experiments:
k<-2000
y_matrix<-matrix(rep(NA,n*k),ncol=k)
for(i in 1:k){
  y_matrix[,i]<-rcauchy(n)
}

op<-par(mfrow=c(1,2),pty="s")
for(i in 1:k){
  if(i==1){
plot(density(y_matrix[,i]),main="Samples from a \n Cauchy distribution",
     xlab="")
  } else {
lines(density(y_matrix[,i]),lty=2)
  }
}
## compute means from each replication:
y_means<-colMeans(y_matrix)
hist(y_means,freq=FALSE,main="Sampling distribution of means \n (Data sampled from Cauchy)",
     xlab="means under repeated sampling")
abline(v=median(y_means),lty=2)
lines(seq(.9,1.1,by=0.01),
      dnorm(seq(.9,1.1,by=0.01),mean(y_means),sd(y_means)))
```

As Figure \@ref(fig:sdsmcauchy) illustrates, when the mean and variance for the likelihood are undefined, the central limit theorem doesn't hold. In the rest of this book, we will always assume that the data are coming from a distribution that has a mean and variance defined for it.

## The confidence interval, and what it's good for

Once we have sample of data $y$, and once the sample mean $\bar{y}$ and the $SE = s/\sqrt{n}$ have been computed, it is common to define a so-called 95% confidence interval:

\begin{equation}
\bar{y} \pm 2 SE
\end{equation}

Because the sampling distribution of means is normally distributed, and because 95% of the area under the curve is covered by two times the standard deviation of the normal distribution,  
the upper and lower bounds of the interval defined by the interval $\bar{y} \pm 2 SE$ covers approximately 95\% of the area under the curve in the sampling distribution. 

This interval is usually computed after estimating the sample mean and  standard error from a data-set, and is called the *confidence interval* (CI). It has the following meaning: If you take samples repeatedly and compute the CI each time, 95\% of those CIs will contain the true population mean $\mu$. To understand this point, one can simulate this situation. This time we will do 1000 repeated experiments instead of 100. 

```{r}
mu <- 500
sigma <- 100
n <- 1000
nsim <- 1000
lower <- rep(NA, nsim)
upper <- rep(NA, nsim)
for (i in 1:nsim) {
  y <- rnorm(n, mean = mu, sd = sigma)
  lower[i] <- mean(y) - 2 * sd(y) / sqrt(n)
  upper[i] <- mean(y) + 2 * sd(y) / sqrt(n)
}

## check how many CIs contain mu:
CIs <- ifelse(lower < mu & upper > mu, 1, 0)
## approx. 95% of the CIs contain true mean:
round(mean(CIs), 2)
```

Figure \@ref(fig:ciplot) visualizes the coverage properties of the confidence interval in 100 simulations; by coverage we mean here the proportion of cases where the true $\mu$ is contained in the CI.

```{r ciplot,echo=FALSE,out.width='75%', fig.show = "hold", fig.cap = "Illustration of the meaning of a  95 percent confidence interval (CI). The thicker bars represent the CIs which do not contain the true mean.", tidy = FALSE}
## define function for SE
se <- function(x)
      {
        y <- x[!is.na(x)] # remove the missing values, if any
        sqrt(var(as.vector(y))/length(y))
}
## define function for CI:
ci <- function (scores){
m <- mean(scores,na.rm=TRUE)
stderr <- se(scores)
len <- length(scores)
upper <- m + 2 * stderr 
lower <- m - 2 * stderr 
return(data.frame(lower=lower,upper=upper))
}

nsim<-100
## sample size:
n<-1000
lower <- rep(NA,nsim)
upper <- rep(NA,nsim)

store <- rep(NA,nsim)

for(i in 1:nsim){ 
  y <- rnorm(n,mean=mu,sd=sigma)
  lower[i] <- ci(y)$lower
  upper[i] <- ci(y)$upper
  if(lower[i]<mu & upper[i]>mu){
    store[i] <- TRUE} else {
      store[i] <- FALSE}
}

## need this for the plot below:
cis <- cbind(lower,upper)

main.title<-"95% CIs in 100 repeated samples"

line.width<-ifelse(store==FALSE,2,1)
cis<-cbind(cis,line.width)
y<-seq(450,550,by=1)
x<-1:length(y)
plot(x,y,type="n",xlab="i-th repeated sample",
     ylab="y",main=main.title)
abline(500,0,lwd=2)
x0<-x
x1<-x
arrows(x0,y0=cis[,1],
       x1,y1=cis[,2],length=0,lwd=cis[,3])
```

### Confidence interals are often misinterpreted

The confidence interval is widely misinterpreted, i.e., as representing the range of plausible value of the $\mu$ parameter. This is the wrong interpretation because $\mu$ is a point value by assumption, it doesn't have a pdf associated with it. The frequentist CI is defined with reference to the sampling distribution of the mean under repeated sampling, not the probability distribution of $\mu$.
By contrast, the Bayesian credible interval does have this interpretation. 
In most modeling settings that the authors have encountered in their work, the frequentist confidence interval and Bayesian credible interval have very similar widths, with the Bayesian interval being slightly wider depending on the prior specifications. But these similarities in the intervals do not change the fact that they have different meanings.

Given the convoluted meaning of the CI, and the impossibility of interpreting a single CI, it is reasonable to ask: what good is a CI? One can treat the CI as a summary that tells us the width of the sampling distribution of the mean---the wider the sampling distribution, the more the implied variability under repeated sampling. The confidence interval can therefore be used to assess how uncertain we can be about the estimate of the sample mean under hypothetical repeated sampling. See @cumming2014new for a useful perspective relating to using confidence intervals for inference. As discussed later in the book, we will use the CI to informally assess uncertainty.

We turn next to the central ideas behind the hypothesis test. We begin with the humble one-sample t-test, which contains many subtleties and is well worth close study before we move on to the main topic of this book: linear mixed models.

## Hypothesis testing: The one sample t-test

With the central limit theorem and the idea of hypothetical repeated sampling behind us, we turn  now to one of the simplest statistical tests that one  can do with continuous data: the t-test. 

Due to its simplicity, it is tempting to take only a cursory look at the t-test and move on immediately to the linear (mixed) model. This would be a mistake. The humble t-test is surprising in many ways, and holds several important lessons for us. There  are subtleties in this test, and a close connection to the linear mixed model. For these reasons, it is worth slowing down and spending some time understanding this test. Once the t-test is clear, more complex statistical tests will be easier to follow, because the logic of these more complex tests will essentially be more of the same, or variations on this general theme. You will see later that t-test can be seen as an analysis of variance or ANOVA; and the paired t-test is exactly the linear mixed model with varying intercepts.

### The one-sample t-test

As in our running example, suppose we have a random sample $y$ of size $n$, and the data come from a $N(\mu,\sigma)$ distribution, with unknown parameters $\mu$ and $\sigma$. An assumption of the t-test is that the data points are independent in the sense discussed at the beginning of this chapter.
We can estimate $\mu$ from the sample mean $\bar{y}$, which we will sometimes also write as $\hat \mu$. We can also estimate $\sigma$ from the sample standard deviation $s$, which we can also write as  $\hat\sigma$. These estimates in turn  allow us to estimate the sampling distribution of the mean under (hypothetical) repeated sampling:

\begin{equation}
N(\hat\mu,\frac{\hat \sigma}{\sqrt{n}})
\end{equation}

It is important to realize here that the above sampling distribution is only as realistic as the estimates of the mean and standard deviation parameters---if those happen to be inaccurately estimated, then the sampling distribution is not realistic either.

Assume as before that we take an independent random sample of size $1000$ from a random variable $Y$ that is normally distributed, with mean 500 and standard deviation 100. As usual, begin by estimating the mean and SE:

```{r}
n <- 1000
mu <- 500
sigma <- 100
## generate simulated data:
y <- rnorm(n, mean = 500, sd = 100)
## compute summary statistics:
y_bar <- mean(y)
SE <- sd(y) / sqrt(n)
```

The null hypothesis significance testing (NHST) approach as practised in psychology and other areas is to set up a null hypothesis that $\mu$ has some fixed value. Just as an  example, assume that our null hypothesis is:

\begin{equation}
H_0: \mu = 450
\end{equation}

This amounts to assuming that the true sampling distribution of sample means is (approximately) normally distributed and centered around 450, with the standard error estimated from the data.


```{r echo=FALSE,fig.cap="The sampling distribution of the mean when the null hypothesis is that the mean is 450. Also shown is the observed sample mean."}
x <- seq(350, 650, by = 0.1)
plot(x, dnorm(x, mean = 450, sd = SE),
  type = "l",
  main = "The sampling distribution with mu=450",
  ylab = "density",
  yaxs="i"
)
points(y_bar, 0.001, col = "black", pch = 20)
text(x = y_bar, y = 0.01, label = "sample \n mean", col = "black")
```

The intuitive idea here is that 

- if the sample mean $\bar{y}$ is "near" the hypothesized $\mu$ (here, 450), the data are possibly (but by no means necessarily) consistent with the null hypothesis distribution.
- if the sample mean $\bar{y}$ is "far" from the hypothesized $\mu$, the data are inconsistent with the null hypothesis distribution.  

The terms "near" and "far" will be quantified by determining how many standard error units the sample mean is from the hypothesized mean. This way of thinking shifts the focus away from the sampling distribution above, towards the distance measured in standard error units from the null hypothesis mean.

The distance between the sample mean and the hypothesized mean can be written in SE units as follows. We will say that the sample mean is $t$ standard errors away from the hypothesized mean:

\begin{equation}
t \times SE = \bar{x} - \mu 
\end{equation}

If we divide both sides with the standard error, we obtain the so-called observed t-statistic:

\begin{equation}
t  = \frac{\bar{x} - \mu}{SE}
\end{equation}

This observed t-value, an expression of the distance between  the sample mean and  the hypothesized mean, becomes the basis for the statistical test. 

Notice that the t-value is a random variable: it is a transformation of $\bar{X}$, the random variable generating the sample means. The t-value can therefore be seen as  an instance of the following transformed random variable $T$:

\begin{equation}
T  = \frac{\bar{X} - \mu}{SE}
\end{equation}

This random variable has a pdf associated with it, the  t-distribution, which is defined in terms of the sample size $n$; the pdf is written $t(n-1)$. Under repeated sampling, the t-distribution is generated from this random variable $T$. 

Figure \@ref(fig:tdistrn) quickly  visualizes this: sample 10000 times from a $Y\sim Normal(\mu=450,\sigma=100)$. Assume that the null hypothesis is $\mu_0=450$; that is, the null hypothesis is in fact true in this case. The sample size is $n=5$. Then compute the t statistic each time, and plot the distribution of the t-values as a histogram. Plot a $t(n-1)$ distribution on top of this histogram to compare to the two distributions. A Normal(0,1) distribution is plotted as a broken line for comparison with the $t(df=4)$ distribution.

```{r tdistrn,fig.cap="The distribution of the t-value under repeated sampling assuming that the null hypothesis is true, compared with a t(n-1) distribution (solid line) and a Normal(0,1) distribution (broken line)."}
set.seed(4321)
nsim <- 10000
n <- 5
mu <- 450
## null hypothesis mean:
mu0 <- 450
sigma <- 100
tval <- rep(NA, nsim)
se <- sigma / sqrt(n)
for (i in 1:nsim) {
  y <- rnorm(n, mean = mu, sd = sigma)
  xbar <- mean(y)
  tval[i] <- (xbar - mu0) / se
}

hist(tval, freq = FALSE, main = "t-value distribution")
x <- seq(-4, 4, by = 0.001)
lines(x, dt(x, df = n - 1))
lines(x, dnorm(x), lty = 2)
```

We will compactly express the statement that "the observed t-value is assumed to be generated (under repeated sampling) from a t-distribution with  n-1 degrees of freedom" as:  

\begin{equation}
T \sim t(n-1)
\end{equation}

For large $n$, the pdf of the random variable $T$ approaches $N(0,1)$. This is illustrated in Figure  \@ref(fig:tnormal); notice that the t-distribution has fatter tails than the normal for small $n$, say $n<20$, but for larger n, the t-distribution and the normal become increasingly similar in shape. Incidentally,  when n=2, the t-distribution $t(1)$ is the Cauchy distribution we saw earlier; this distribution is characterized by fat tails, and has no mean or variance defined for it.


```{r tnormal,echo=FALSE,fig.cap="A visual comparison of the t-distribution (with degrees of freedom ranging from 1 to 50) with the standard normal distribution (N(0,1)). The dashed line represents the standard normal distribution, and the solid line the t-distribution with the relevant degrees of freedom."}
range <- seq(-4, 4, .01)

op <- par(mfrow = c(2, 3), mar = c(2, 2, 3, 2), pty = "s")


for (i in c(1, 2, 5, 15, 20, 50)) {
  plot(range, dnorm(range),
    type = "l", lty = 2,
    xlab = "", ylab = "",
    cex.axis = 1, cex.axis = 0.8,
    yaxs="i"
  )
  lines(range, dt(range, df = i), lty = 1, lwd = 1)
  mtext(paste("df=", i), cex = 1.2)
}
```

Thus, given a sample size $n$, we can define a t-distribution corresponding to the null hypothesis distribution. For large values of $n$, we could even use $N(0,1)$, although it is traditional in psychology and linguistics to always use the t-distribution no matter how large $n$ is.

The null hypothesis testing procedure proceeds as follows:

- Define the null hypothesis: in our example, the null  hypothesis was that $\mu = 450$.  This amounts to making a commitment about what fixed value we think the true underlying distribution of sample means is centered at. 
- Given data of size $n$, estimate $\bar{y}$, standard deviation $s$, and from that, estimate the standard error $s/\sqrt{n}$.  The standard error will be used  to describe the sampling distribution's standard  deviation.
- Compute the observed t-value:

\begin{equation}
t=\frac{\bar{y}-\mu}{s/\sqrt{n}}
\end{equation}

- Reject null hypothesis if the observed t-value is "large" (to be made more precise next).
- Fail to reject the null hypothesis, or (under some conditions, to be made clear later) even go so far as to accept the null hypothesis, if the observed t-value is "small".

What constitutes a large or small observed t-value?
Intuitively, the t-value from the sample is large when we end up far in *either* tail of the distribution. The two tails of the t-distribution will be referred to as the *rejection region*. The word *region* here refers to the real number line along the x-axis, under the tails of the distribution. The rejection region will go off to infinity on the outer sides, and is demarcated by a vertical line on the inner side of each tail. This is shown in Figure \@ref(fig:tails). It goes off to infinity because the support---the range of possible values---of the random variable that the t-distribution belongs to stretches from minus infinity to plus infinity. 

```{r tails,echo=FALSE,fig.cap="The rejection region in a t-distribution (sample size 20) assuming that the null hypothesis is true. The rejection region is the x-axis under the gray-colored area."}
n<-20
x <- seq(-5, 5, by = 0.1)
plot(x, dt(x, df = n-1),
  type = "l", main = "t(n-1)",
  ylab = "density",
  yaxs="i",
  xlab = "t-values"
)

lower <- qt(0.025, df = n-1)
upper <- qt(0.975, df = n-1)
abline(v = lower)
abline(v = upper)

x1 <- seq(upper, 20, 0.001)
y1 <- dt(x1, df = n-1)
polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)

x1 <- seq(-20, lower, 0.001)
y1 <- dt(x1, df = n-1)
polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)

points(1.8, 0.005, col = "black", pch = 20)
text(x = 1.8, y = 0.04, label = "observed t-value", col = "black")
text(0,.2,"fail-to-reject region")
```

The location of the vertical lines is determined by the so-called *absolute critical t-value* along the x-axis of the t-distribution.  This is the value such that the area under the curve in the tails to the left or right of the tails is 0.025. As discussed in chapter 1, this area under the curve represents the probability of observing a value as extreme as the critical t-value, or some value that is more extreme. Notice that if we ask ourselves what the probability is of observing some particular t-value (a point value), the answer must necessarily be $0$ (if you are unclear about why, re-read chapter 1). But we can ask the question: what is the absolute t-value, written $|t|$, such that $P(T>|t|)=0.05$?  That's the critical t-value. We call it the critical t-value because it demarcates the rejection region shown in Figure \@ref(fig:tails): we are adopting the convention that any observed t-value larger than this critical t-value allows us to reject the null hypothesis.

For a given sample size $n$, we can identify the rejection region by using the `qt` function, whose usage is analogous to the `qnorm` function discussed in chapter 1.

Because the shape of the t-distribution depends on the degrees of freedom (n-1), the absolute critical t-value beyond which we reject the null hypothesis will change depending on sample size. For large sample sizes, say $n>50$, the rejection point is about 2.

```{r}
abs(qt(0.025, df = 15))
abs(qt(0.025, df = 50))
```
Consider the observed t-value from our sample in our running example:

```{r}
## null hypothesis mean:
mu <- 450
(t_value <- (y_bar - mu) / SE)
```

This observed t-value is huge and is  telling you the distance of the sample mean from the null hypothesis mean $\mu$ in standard error units.

\begin{equation}
t=\frac{\bar{y}-\mu_0}{s/\sqrt{n}} \hbox{ or } t\frac{s}{\sqrt{n}}=\bar{y}-\mu_0
\end{equation}

For large sample sizes, if the absolute t-value $|t|$ is greater than  $2$, we will reject the null hypothesis. 

For a smaller sample size $n$, you can compute the exact critical t-value:

```{r}
qt(0.025, df = n - 1)
```

Why is this critical t-value negative in sign? That is because it is on the left-hand side of the t-distribution, which is symmetric.
The corresponding value on the right-hand side is:

```{r}
qt(0.975, df = n - 1)
```

These values are of course identical if we ignore the sign. This is why we always frame our discussion around the absolute t-value.

In R, the  built-in function `t.test` delivers the observed t-value. Given our running example, with the null hypothesis $\mu=450$, R returns the following:

```{r}
## observed t-value from t-test function:
t.test(y, mu = 450)$statistic
```

The default value for the null hypothesis mean $\mu$ in this function is 0; so if one doesn't define a null hypothesis mean, the statistical test is done with  reference to a null hypothesis that $\mu=0$. That is why this t-value does not match our calculation above: 

```{r}
t.test(y)$statistic
```

In the most common usage of the t-test, the null hypothesis mean will be $0$, because usually one is comparing a difference in means between two conditions or two sets of conditions. So the above line of code will work out correctly in those cases; but if you ever have a different null hypothesis mean than $0$, then you   have to specify it in  the `t.test` function.

So, the way that the t-test is used in psychology and related areas is to implement a *decision*: either reject the null hypothesis or fail to reject it. Whenever we do an experiment and carry out a t-test, we use the  t-test to make this binary decision: reject or fail to reject the null hypothesis. 

Recall that behind the t-test lies the  assumption  that the observed t-value  is coming from a random variable, $T\sim t(n-1)$. The particular t-value we observe from a particular data-set belongs to a distribution of t-values under hypothetical repeated sampling. Thus, implicit in the logic of the t-test---and indeed every frequentist statistical test---is the assumption that the experiment is in principle repeatable: the experiment can in principle be re-run as many times as we want, assuming we have the necessary resources and time. 

A quick simulation of t-values under repeated sampling makes this clear. Suppose that our null hypothesis mean is $450$, and our sample size  $n=100$. Assume that the data come from $Normal(\mu=450,\sigma=100)$. Thus, in this case the null hypothesis is in fact true. Let's do $10000$ simulations, compute the sample mean each time, and then store the observed t-value. The t-distribution that results is shown in Figure \@ref(fig:simt).

```{r simt,fig.cap="The distribution of t-values under repeated sampling. The null hypothesis is true."}
n <- 100
nsim <- 10000
tvals <- rep(NA, nsim)
for (i in 1:nsim) {
  y <- rnorm(n, mean = 450, sd = 100)
  SE <- sd(y) / sqrt(n)
  tvals[i] <- (mean(y) - 450) / SE
}
plot(density(tvals),
  main = "Simulated t-distribution",
  xlab = "t-values under repeated sampling"
)
```

What would the t-distribution look like if the null hypothesis were false? Assume now that the null hypothesis is that $\mu=450$ as before, but that in fact the true $\mu$ is 470. Now the null hypothesis is false. Figure \@ref(fig:simtnullfalse) shows the t-distribution under repeated sampling. The t-distribution is now centered around 2; why? This is because if we plug in the hypothesized mean (450) and the true mean (470) and the standard error ($100/\sqrt(100)=10$) into the equation for computing the t-value, the expected value of the t-distribution (its mean) is $2$.

\begin{equation}
\frac{470-450}{10} = 2
\end{equation}

```{r simtnullfalse,fig.cap="The distribution of t-values under repeated sampling. The null hypothesis is false, with the true mean being 470 (the null hypothesis is that the true mean is 450)."}
n <- 100
nsim <- 10000
tvals <- rep(NA, nsim)
for (i in 1:nsim) {
  y <- rnorm(n, mean = 470, sd = 100)
  SE <- sd(y) / sqrt(n)
  tvals[i] <- (mean(y) - 450) / SE
}
plot(density(tvals),
  main = "Simulated t-distribution",
  xlab = "t-values under repeated sampling"
)
```

This implicit idea of the experiment's repeatability leads to an important aspect of the t-test: once certain assumptions about the null hypothesis and the alternative hypothesis are fixed, we can using simulation to compute the proportion of times that the null hypothesis would be rejected under repeated sampling. One has to consider two alternative possible scenarios: the null is either true, or it is  false. In other words, this simulation-based approach allows us to study the t-test's ability (at least in theory) to lead the researchers to the correct decision under (hypothetical) repeated sampling. We turn to this issue next.

### Type I, II error, and power

When we do a hypothesis test using the t-test, the observed t-value will either fall in the rejection region, leading  us to reject the null hypothesis,
or it will land in the non-rejection region, leading us to fail to reject the null. For a particular experiment, that is a single, one-time event. 

So suppose  we have made our decision based on the observed t-value. Now, the null hypothesis can be either true or not true;  we don't know which of those two possibilities is the reality.
When we decide (based on  the observed t-value) that the  null  is true, we are asserting  that the parameter $\mu$ actually does have the  hypothesized value $\mu_0$; when we decide that the null is false, we are asserting that the parameter $\mu$ has some *specific* value $\mu_{alt}$ other than $\mu_0$.
We can represent these two alternative possible realities in a tabular form, as shown in Table \@ref(tab:type12). The two columns show the two possible worlds, one in which the null is true, and the other in  which it is false. The two rows show the two possible decisions we can take based on the observed t-value: reject the null or fail to reject it.

\begin{table}
\begin{tabular}{ccc}
        & \textbf{Possible world 1}      & \textbf{Possible world 2} \\  
\hline
 & $H_0$ TRUE: $\mu=\mu_0$  & $H_0$ FALSE $\mu=\mu_{alt}$ \\
\hline
Decision: `reject': & $\alpha$ & $1~-~\beta$ \\
                                     & Type I error                         & Power \\                                      
                                     & & \\
\hline
Decision: `fail to reject': & $1 - \alpha$ & $\beta$ \\                                    &                                 & Type II error\\
\hline
\end{tabular}
\caption{The possible realities (null is true or null is false) and the possible decisions (accept or reject null) we can take based on our observed t-value.} \label{tab:type12}
\end{table}

As the table shows, we can make two kinds of mistakes:

- Type I error or $\alpha$: Reject the null when it's true.
- Type II error or $\beta$: Accept the null when it's false.

In psychology and related areas, Type I error is usually fixed a priori at 0.05. This stipulated Type I error value is why the absolute critical t-value is kept at approximately $2$; if, following recommendations  from @benjamin2018redefine, we were to stipulate that the Type I  error be 0.005, then the critical t-value would have had to be set at:

```{r}
abs(qt(0.0025, df = n - 1))
```

This suggested change in convention hasn't been taken up yet in cognitive science, but this could well change one day.

Type II error, the probability of incorrectly accepting the null hypothesis when it is false with some particular value for the parameter $\mu$, is conventionally recommended [@powerbookcohen] to be kept at 0.20 or lower. This implies that the probability of correctly  rejecting a null hypothesis for some particular true value of $\mu$ is 1-Type II error. This probability, called statistical power, or just power, should then obviously be larger than 0.80. Again, there is nothing special about these stipulations; they are conventions that became the norm over time. 

Next, we will consider the trade-off between Type I and II error. For simplicity, assume that the standard error is 1, and the null hypothesis is that $\mu=0$. This means that the observed t-value is really the sample mean.

Consider the concrete situation where, in reality, the true value of $\mu$ is $2$. As mentioned above, the null hypothesis $H_0$ is that $\mu=0$. Now the $H_0$ is false because $\mu=2$ and not $0$. Type I  and II  error can be visualized graphically as shown in  Figure \@ref(fig:type12). 

```{r type12,echo=FALSE,fig.cap="A visualization of Type I  and II error. The dark-shaded tails of the left-hand side distribution represent Type I error; and  the light-colored shaded region of the right-hand side distribution represents Type II error. Power is the unshaded area under the curve in  the right-hand side distribution."}
## function for plotting the area under the curve:
plot.prob <- function(x,
                      x.min,
                      x.max,
                      prob,
                      mean,
                      sd,
                      gray.level, main) {
  plot(x, dnorm(x, mean, sd),
    type = "l", xlab = "",
    ylab = "", main = main
  )
  abline(h = 0)

  ## shade X<x
  x1 <- seq(x.min, qnorm(prob), abs(prob) / 5)
  y1 <- dnorm(x1, mean, sd)

  polygon(c(x1, rev(x1)),
    c(rep(0, length(x1)), rev(y1)),
    col = gray.level
  )
}

shadenormal <-
  function(prob = 0.5,
           gray1 = gray(0.3),
           x.min = -6,
           x.max = abs(x.min),
           x = seq(x.min, x.max, 0.01),
           mean = 0,
           sd = 1, main = "P(X<0)") {
    plot.prob(
      x = x, x.min = x.min, x.max = x.max,
      prob = prob,
      mean = mean, sd = sd,
      gray.level = gray1, main = main
    )
  }

shadenormal(prob = 0.025, main = "Type I, II error")

x1 <- seq(qnorm(0.975), 6, abs(0.975) / 5)
y1 <- dnorm(x1)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.3)
)

x <- seq(-6, 6, by = 0.1)
lines(x, dnorm(x, mean = 2), col = "black", lwd = 2)
abline(v = 2)
abline(v = -2)

x1 <- seq(-2, 2, 0.01)
y1 <- dnorm(x1, mean = 2)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)
```

To understand Figure \@ref(fig:type12), one has to consider two distributions side by side. First, consider the null hypothesis distribution, centered at 0. Under the null hypothesis distribution, the rejection region lies below the dark colored tails of the distributions. The area under the curve in these dark-colored tails is the Type I error (conventionally set at 0.05) that we decide on even before we conduct the experiment and collect the data. Because the Type I error is set at 0.05, and because the t-distribution is symmetric, the area under the curve in each tail is 0.025. The absolute critical t-value helps us demarcate the boundaries of the rejection regions through the vertical lines shown in the figure. These vertical lines play a crucial role in helping us understand Type II error and power. This becomes clear when we consider the distribution representing the alternative possible value of $\mu$, the distribution centered around 2. In this second distribution,  consider now the area under the curve  between the vertical lines demarcating the rejection region  under the  null. This area under the curve is the probability of accepting the null hypothesis when  the null hypothesis is false with some specific value (here, when $\mu$ has value 2). 

Some interesting observations follow. Suppose that the true effect is in fact $\mu=2$, as in the above illustration. Then,

- Simply decreasing Type I error to a smaller value like 0.005 will increase Type II error, which means that power (1-Type II error) will fall.
- Increasing sample size will squeeze the vertical lines closer to  each other because standard error will go down with increasing sample size. This will reduce Type II error, and therefore increase power. Decreasing sample size will have the opposite effect.
- If we design an experiment with a larger effect size, e.g., by setting up a stronger manipulation (concete examples will be discussed in this book later on), our Type II error will go down, and therefore power will go up. Figure \@ref(fig:highpower) shows a graphical visualization of a situation where the true effect size is $\mu=4$. Here, Type II error is much smaller compared to Figure \@ref(fig:type12), where $\mu=2$.


```{r highpower,echo=FALSE,fig.cap="The change in Type II error if the true effect has mean 4."}
## function for plotting the area under the curve:
plot.prob <- function(x,
                      x.min,
                      x.max,
                      prob,
                      mean,
                      sd,
                      gray.level, main) {
  plot(x, dnorm(x, mean, sd),
    type = "l", xlab = "",
    ylab = "", main = main
  )
  abline(h = 0)

  ## shade X<x
  x1 <- seq(x.min, qnorm(prob), abs(prob) / 5)
  y1 <- dnorm(x1, mean, sd)

  polygon(c(x1, rev(x1)),
    c(rep(0, length(x1)), rev(y1)),
    col = gray.level
  )
}

shadenormal <-
  function(prob = 0.5,
           gray1 = gray(0.3),
           x.min = -6,
           x.max = abs(x.min),
           x = seq(x.min, x.max, 0.01),
           mean = 0,
           sd = 1, main = "P(X<0)") {
    plot.prob(
      x = x, x.min = x.min, x.max = x.max,
      prob = prob,
      mean = mean, sd = sd,
      gray.level = gray1, main = main
    )
  }

shadenormal(prob = 0.025, main = "Type I, II error")

x1 <- seq(qnorm(0.975), 6, abs(0.975) / 5)
y1 <- dnorm(x1)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.3)
)

x <- seq(-6, 6, by = 0.1)
lines(x, dnorm(x, mean = 4), col = "black", lwd = 2)
abline(v = 4)
abline(v = 2)
abline(v = -2)

x1 <- seq(-2, 2, 0.01)
y1 <- dnorm(x1, mean = 4)

polygon(c(x1, rev(x1)),
  c(rep(0, length(x1)), rev(y1)),
  col = gray(0.8)
)
```

In summary, when we plan out an experiment, we are also required to specify the Type I and II error associated with the design. Both sources of error are within our control, at least to some extent.  The Type I error we decide to use will determine our critical t-value  and therefore our decision criterion for rejecting, failing to reject, or even (under certain conditions, to be discussed below) accepting the null hypothesis.  

The Type II error we decide on will determine the long-run probability of incorrectly ``accepting'' the null hypothesis; its inverse (1-Type II error), statistical power, will determine the long-run probability of correctly rejecting the null hypothesis under the assumption that the $\mu$ has some particular value other than the null hypothesis mean.

That's the theory anyway. In practice, researchers only rarely consider the power properties of their experiment design; the focus is almost exclusively on Type I error. The neglect of power in experiment design has had interesting consequences for theory development, as we will see later in this book. For a case study in psycholinguistics, see @VasishthMertzenJaegerGelman2018.

### How to compute power for the one-sample t-test

Power (which is 1-Type II error) is a function of three variables:

- the effect size
- the standard deviation
- the sample size.

There are two ways that one can compute power in connection with the t-test: either one can use the built-in R function, `power.t.test`, or one can use simulation. 


#### Power calculation using the `power.t.test` function

Suppose that we have an expectation that an effect size is 15 ms $\pm 5$ ms (this could be based on the predictions of a theoretical model, or prior data); suppose also that prior experiments show standard deviations ranging from 100 to 300 ms. Given a particular sample size, this is enough information to compute a power curve as a function of effect size and standard deviation. See Figure \@ref(fig:powercurve) and the associated code below.

```{r powercurve,fig.cap="An illustration of a power curve for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20."}
sds <- seq(100, 300, by = 1)
lower <- power.t.test(delta = 15 - 5, 
                      sd = sds, n = 10, 
                      strict = TRUE)$power
upper <- power.t.test(delta = 15 + 5, 
                      sd = sds, n = 10, 
                      strict = TRUE)$power
meanval <- power.t.test(delta = 15, 
                        sd = sds, n = 10,
                        strict = TRUE)$power

plot(sds, meanval,
  type = "l",
  main = "Power curve (n=10)\n
     using power.t.test",
  xlab = "standard deviation",
  ylab = "power",
  yaxs="i",
  ylim=c(0.05,0.07)
)
lines(sds, lower, lty = 2)
lines(sds, upper, lty = 2)
text(125, 0.053, "10")
text(150, 0.054, "15")
text(175, 0.056, "20")
```

#### Power calculations using simulation

An analogous calculation as the one shown above using the  `power.t.test` function can also be done using simulated data.
First, generate simulated data repeatedly for each possible combination of parameter values (here, effect size and standard deviation), and compute the proportion of significant effects for each parameter combination. This can be done by defining a function that takes as input the number of simulations, sample size, effect size, and standard deviation:

```{r}
compute_power <- function(nsim = 100000, n = 10,
                          effect = NULL,
                          stddev = NULL) {
  temp_power <- rep(NA, nsim)
  for (i in 1:nsim) {
    y <- rnorm(n, mean = effect, sd = stddev)
    temp_power[i] <- ifelse(abs(t.test(y)$statistic) 
                            > 2, 1, 0)
  }
  ## return power calculation:
  mean(temp_power)
}
```


Then, plot the power curves as a function of effect size and standard deviation, exactly as in Figure \@ref(fig:powercurve). Power calculations using simulations are shown in Figure \@ref(fig:powercurve2). It is clear that simulation-based power estimation is going to be noisy; this is because each  time we are generating simulated data and then carrying out a statistical  test on it. This is no longer a closed-form mathematical calculation as done in `power.t.test` (this function simply implements a formula for power calculation specified for this simple case). Because the power estimates will be noisy, we show a smoothed lowess line for each effect size estimate. 

```{r powercurve2,cache=TRUE,echo=FALSE,fig.cap="An illustration of a power curve using simulation, for 10 participants, as a function of standard deviation, and three estimates of the effect: 15, 10, and 20. The power curves are lowess-smoothed."}
sds <- seq(100, 300, by = 5)


nsim <- 100000
n <- 10
power_meanval <- power_lowerval <- power_upperval <- rep(NA, length(sds))
for (s in 1:length(sds)) {
  power_meanval[s] <- compute_power(
    nsim = nsim, n = n,
    effect = 15,
    stddev = sds[s]
  )
}
for (s in 1:length(sds)) {
  power_lowerval[s] <- compute_power(
    nsim = nsim, n = n,
    effect = 10,
    stddev = sds[s]
  )
}
for (s in 1:length(sds)) {
  power_upperval[s] <- compute_power(
    nsim = nsim, n = n,
    effect = 20,
    stddev = sds[s]
  )
}
plot(sds, lowess(power_upperval)$y,
  type = "l", lty = 2,
  xlab = "standard deviation", ylab = "power",
  main = "Power curve (n=10) \n using simulation", 
  ylim = c(0.05,0.12),
    yaxs = "i"
  )
lines(sds, lowess(power_lowerval)$y, lty = 2)
lines(sds, lowess(power_meanval)$y, lty = 1)
```

In the above example, simulation-based power calculation is overkill, and completely unnecessary because we have `power.t.test`. However, the technique shown above will be extended and will become our bread-and-butter method once we switch to power calculations for complicated linear mixed models. There, no closed form calculation can be done to compute power, at least not without oversimplifying the model; simulation will be the only practical way to calculate power.

It is important to appreciate the fact that power is a *function*; it isn't a single number. Because we can never be sure what the true effect size is, or what the true standard deviation is, power functions (power as a function of plausible values for the relevant parameters) are much more useful than single numbers.  

### The p-value

Continuing with our t-test example, the `t.test` function in R will not only print out a t-value as shown above, but also a probability known as a *p-value*. This is the probability of obtaining the observed t-value that we obtained, or some value more extreme than that, conditional on the assumption that the null hypothesis is true. 

We can compute the p-value "by hand". This can be computed, as done earlier, simply by calculating the area under the curve that lies beyond the absolute observed t-value on either side of the t-distribution. It is standard practice to take the tail probability on both sides of the t-distribution. 

```{r}
(abs_t_value <- abs(t.test(y, mu = 450)$statistic))

2 * pt(abs_t_value, df = n - 1, lower.tail = FALSE)
```

The area from both sides of the tail is taken because it is conventional to do a so-called *two-sided t-test*: our null hypothesis is that $\mu=450$, and our alternative hypothesis is two-sided: $\mu$ is either less than $450$ or $\mu$ is larger than $450$. When we reject the null hypothesis, we are accepting this alternative, that  $\mu$ could be some value other than $450$. Notice that this alternative hypothesis is remarkably vague; we would reject the null hypothesis regardless of whether the sample mean turns out to be $600$ or $-600$, for example. The  practical implication is that the p-value gives us  the  strength of the evidence against the null hypothesis; it doesn't give us  evidence in favor of  a specific alternative. The p-value will reject the null hypothesis regardless of whether our sample mean is positive or negative in sign. In psychology and allied disciplines, whenever the p-value falls below $0.05$, it is common practice to write something along the lines that "there was reliable evidence for the predicted effect." This statement  is incorrect! We only ever have evidence against the null. By looking at the sample mean and its sign, we are making a very big leap that we  have evidence for the specific sample mean we happened to get. As we will see below, when power is low, the sample mean can be wildly far from the true mean that produced the data. 

One need not have a two-sided alternative; one could have defined the alternative to be one-sided (for example, that $mu>450$). In that case, one would compute only one side of the area under the curve. This kind of one-sided test is not normally done, but one can imagine a situation where a one-sided test is justified (for example, when only one sign of the effect is possible, or if there is a strong theoretical reason to expect only one particular sign---positive or negative---on an effect). That said, in their scientific career, only one of the authors of this book has ever had occasion to use a one-sided test. In this book, we will not use one-sided t-tests.

The p-value is always interpreted with reference to the pre-defined Type I error. Conventionally, we reject the null if $p<0.05$. This is because we set the Type I error probability at 0.05. Keep in mind that Type I error probability and the p-value are two distinct things. 
The Type I error probability is the probability of your incorrectly rejecting the null under repeated sampling. This is not the same thing as your p-value. The latter probability will be obtained from a particular experiment, and will vary from experiment to experiment; it is a random variable. By contrast, the Type I error probability is a value we fix in advance.

#### The distribution of the p-value under the null hypothesis

We have been talking about a continuous random variable as a dependent measure, and have learnt about the standard two-sided t-test, with a point null hypothesis. When we do such a test, we usually use the p-value to decide whether to reject the null hypothesis or not.

Sometimes, you will hear  statisticians (e.g., Andrew Gelman on his blog) criticize p-values by saying that the null hypothesis significance test is a "specific random number generator". What does that sentence mean? We explain this point here because it is important for understanding the meaning of the p-value.

Suppose that the null hypothesis is in fact true. We will now simulate the distribution of the p-value under repeated sampling:

```{r pvalnulltrue,cache=TRUE,echo=FALSE,fig.cap="The p-value has a uniform distribution when the null hypothesis is true; a demonstration using simulation."}
nsim <- 100000
pvals <- rep(NA, nsim)
for (i in 1:nsim) {
  ## Here, the Null Hypothesis is that mu = 0
  y <- rnorm(100)
  pvals[i] <- t.test(y)$p.value
}

hist(pvals,
  freq = FALSE,
  main = "The distribution of p-values\n when null is true"
,xlab = "p-values",
ylab = "density")
```

The distribution of the p-value is uniform---every value between 0 and 1 is equally likely.

We will now formally derive the above fact, that the distribution of the p-value is uniform under the null hypothesis. 

Consider the fact that the p-value is a random variable; call it $Z$. The p-value is the cumulative distribution function (CDF) of the random variable $T$, which itself is a transformation of the random variable $\bar{Y}$: 

$T=(\bar{X}-\mu)/(\sigma/\sqrt{n})$

This random variable $T$ has some CDF $F(T)$. It is possible to show that if a random variable $Z=F(T)$, i.e., if $Z$ is the 
CDF for the random variable $T$, then $Z$ has a uniform distribution ranging from 0 to 1, $Z \sim Uniform(0,1)$.

This is an amazing fact. To get a grip on this, let's first think about the fact that when a random variable $Z$ comes from a $Uniform(0,1)$ distribution, then $P(Z<z)=z$. Consider some examples:

- when $z=0$, then $P(Z<0)=0$;
- when $z=0.25$, then $P(Z<0.25)=0.25$; 
- when $z=0.5$, then $P(Z<0.5)=0.5$; 
- when $z=0.75$, then $P(Z<0.75)=0.75$; 
- when $z=1$, then $P(Z<1)=1$. 

Next, we will prove the above statement, that if a random variable $Z=F(T)$, i.e., if $Z$ is the 
CDF for a random variable $T$, then $Z \sim Uniform(0,1)$.
The proof is actually quite astonishing and even has a name:  it's called the *probability integral transform*.

Suppose that $Z$ is the CDF of a random variable $T$: $Z=F(T)$. Then, it follows that $P(Z\leq z)$ can be rewritten in terms of the CDF of T: $P(F(T)\leq z)$. Now, if we apply the inverse of the CDF ($F^{-1}$) to both the left and right sides of the inequality, we get $P(F^{-1}F(T)\leq F^{-1}(z))$.
But $F^{-1}F(T)$ gives us back $T$; this holds because if we have a one-to-one onto function $f(x)$, then applying the inverse $f^{-1}$ to this function gives us back $x$. 

The fact that $F^{-1}F(T)$ gives us back $T$ means that we can rewrite $P(F^{-1}F(T)\leq F^{-1}(z))$ as $P(T\leq F^{-1}(z) )$. But this probability is simply the CDF $F(F^{-1} (z))$, which simplifies to $z$. This shows that $P(Z\leq z) = z$; i.e., that the p-value has a uniform distribution under the null hypothesis. 

The above proof is restated below compactly:

\begin{equation}
\begin{split}
P(Z\leq z) =& P(F(T)\leq z)\\
=& P(F^{-1}F(T)\leq F^{-1}(z))\\
=& P(T\leq F^{-1}(z) ) \\
=& F(F^{-1} (z))\\
=& z\\
\end{split}
\end{equation}

It is for this reason that statisticians like Andrew Gelman periodically point out that "the null hypothesis significance test is a specific random number generator". 

The practical implication of this criticism of p-values is that when we do a single experiment and obtain a p-value under the assumption that the null is true, if the null were in fact true, then we are just using a random number generator to make a decision that the effect is present or absent. We are literally using the output of a uniform(0,1) distribution to decide to reject the null hypothesis:

<<>>=
pval <- runif(1, min = 0, max = 1)
if (pval < 0.05) {
  print("reject")
} else {
  "fail to reject"
}
@

This doesn't seem like a meaningful way to test a scientific hypothesis!

A broader implication is that we should not place our theory development exclusively at the feet of the p-value. As we discuss in this book, other considerations (such as replicability, uncertainty of the estimates, and power) are as or even more important.


### Type M and S error in the face of low power

Beyond Type I and II error, there are also two other kinds of error to be aware of. These are Type M and S error; both sources of error are closely related to statistical power.

The terms Type M and S error were introduced by @Gelman14, but the ideas have been in existence for some time [@hedges1984estimation],[@lane1978estimating]. @powerfailure refer to Type M and S error as the "winner's curse" and "the vibration of effects." In related work, @ioannidis2008most refers to the vibration ratio in the context of epidemiology.

Type S and M error can be illustrated with the following example.
Suppose your true effect size is believed to be $D=15$, 
then we can compute (apart from statistical power) the following error rates, which are defined as follows:

- **Type S error**: the probability that the sign of the effect is incorrect, given that the result is statistically significant.
- **Type M error**: the expectation of the ratio of the absolute magnitude of the effect to the hypothesized true effect size, given that the result is significant. 
Gelman and Carlin also call this the exaggeration ratio, which is perhaps more descriptive than "Type M error".

Suppose that a particular study has standard error $46$, and sample size $37$. And suppose that the true $\mu=15$, as in the example discussed above. Then, we can compute statistical power, Type S and M error through simulation in the following manner:

```{r}
## probable effect size, derived from past studies:
D <- 15
## SE from the study of interest:
se <- 46
stddev <- se * sqrt(37)
nsim <- 10000
drep <- rep(NA, nsim)
for (i in 1:nsim) {
  samp <- rnorm(37, mean = D, sd = stddev)
  drep[i] <- mean(samp)
}
```

Power can be computed by simply determining the proportion of times that the absolute  observed t-value  is larger than 2:

```{r}
## power: the proportion of cases where
## we reject the null hypothesis correctly:
(pow <- mean(ifelse(abs(drep / se) > 2, 1, 0)))
```

Power is quite low here (we deliberately chose an example with  low power to illustrate Type S and M error). 

Next, we figure out which of the samples are statistically significant (which simulated values yield $p<0.05$). As a criterion, we use a t-value of 2 to declare that we reject the null; we could have done this more precisely by working out an exact critical t-value.

```{r}
## which results in drep are significant at alpha=0.05?
signif <- which(abs(drep / se) > 2)
```

Type S error is the proportion of significant cases with the wrong sign (sign error), and Type M error is the ratio by which the true effect (of $\mu=15$) is exaggerated in those simulations that  happened to come out significant.

```{r}
## Type S error rate | signif:
(types_sig <- mean(drep[signif] < 0))
## Type M error rate | signif:
(typem_sig <- mean(abs(drep[signif]) / D))
```

In this scenario, when power is approximately 6%, whenever we get a significant effect, the probability of obtaining the wrong  sign is a  whopping `r round(types_sig*100)`% and the effect is likely to be `r typem_sig` times larger than its true magnitude. The practical implication is as follows.

When power is low, relying on the p-value (statistical significance) to declare an effect as being present will be misleading because the decision will be  based on an overestimate of the effect (Type M error), and even the sign of the effect could be wrong. This isn't just a theoretical point; it has real-world  consequences for theory  development. For an  example from psycholinguistics regarding this point, see @VasishthMertzenJaegerGelman2018.

Another useful way to  visualize Type M and S error  is through the so-called funnel plot. As shown in Figure \@ref(fig:funnel), estimates obtained from low-powered studies will tend to be exaggerated (the lower part of the funnel), and as power goes up, the effect estimates start to cluster tightly around the true value of the effect. 

```{r funnel,echo=FALSE,fig.cap="An illustration of a funnel plot. Shown are repeated samples of an effect estimate under different values of power, where the true value  of the effect is 15 (marked by the vertical line). Significant effects are shaded gray. The lower the power, the wider the fluctuation of the effect; under low power, it is the exaggerated effects that end up statistically significant, even though they are very biased relative to the true value. As power goes up, the effect estimates start to cluster around the true value, and significant effects are also accurate estimates of the effect. Thus, low power leads to exaggerated estimates of the effect, especially if the data are filtered by statistical significance."}
## funnel plot:
truemu <- 15
sampsize <- seq(10, 10000, by = 10)
n_expts <- length(sampsize)
means <- power <- sig <- rep(NA, n_expts)
for (i in 1:n_expts) {
  y <- rnorm(sampsize[i],
    mean = truemu,
    sd = 250
  )
  sig[i] <- ifelse(t.test(y)$p.value < 0.05, 1, 0)
  means[i] <- mean(y)
  power[i] <- power.t.test(
    d = truemu, sd = 250,
    n = sampsize[i]
  )$power
}

means_df <- data.frame(means, power, sig)

plot(jitter(means), jitter(power),
  main = "Funnel plot",
  xlim = range(c(min(means), max(means))),
  xlab = "effect", ylab = "power",
  # ylim=c(0,0.003),
  cex.lab = 1.8,
  cex.axis = 1.5, cex.main = 1.5
)
abline(v = 15)

sig_effects <- subset(means_df, sig == 1)

points(sig_effects$means, sig_effects$pow,
  pch = 21,
  bg = "#CACACA"
)
```

What is important to appreciate here is the fact that significant effects "point to the truth" just in case power is high; when power is low, either null results will frequently be found even if the null is false, and those results that turn out significant will be based on Type M error. 

In many fields, it is practically impossible to conduct a high-powered study. What should one do in this situation? When reporting results that are likely based on an underpowered study, the best approach is to openly acknowledge the power limitation, to attempt to conduct a direct replication of the effect to establish robustness, and to attempt to synthesize the evidence from existing knowlege [@cumming2014new].  

By direct replication, we mean that the study should be run multiple times with the same materials and design but new participants, to establish whether effect estimates in the original study and the replication study are consistent with each other. Direct replications stand in contrast to so-called conceptual replications, which are not exact repetitions of the original design, but involve some further or slightly different but related experimental manipulations. Conceptual replications are also a very useful tool for cross-validating the existence of an effect.

Direct  replications will always differ from the original study in some way or another---the lab may differ, the protocols might differ slightly, the experimenter is different, etc. Such between-study variability is obviously unavoidable in direct-replication attempts, but they are still worthwhile for establishing the existence of an effect. To make more clear the idea of establishing robustness through replication attempts, detailed examples of different kinds of replication  attempts of published studies will be presented in this book's example data-sets.


### Searching for significance

The NHST procedure is essentially a decision procedure: if $p<0.05$, we reject the null hypothesis; otherwise, we fail to reject the null. Because significant results are easier to publish than non-significant results, a common approach taken by researchers (including the first author of this book, when he was a graduate student) is to run the experiment and periodically check if statistical significance has been reached. The procedure can be described as follows:

- The experimenter gathers $n$ data points, then checks for significance (is $p<0.05$ or not?). 
- If the result is not significant, he gets more data (say, $n$ more data points). Then he checks for significance, and repeats.

Since time and money are limited, he might decide to stop collecting data after some multiple of $n$ have been collected. 

One can simulate different scenarios here. Suppose that $n$ is initially $15$.  
Under the standard assumptions, we set Type I error to be $0.05$. Let's suppose that the null hypothesis that $\mu=0$ is in fact true, and that standard deviation is 250. 

```{r}
## Standard properties of the t-test:
pvals <- NULL
tstat_standard <- NULL
n <- 15
nsim <- 10000
## assume a standard dev of 250:
stddev <- 250
mn <- 0
for (i in 1:nsim) {
  samp <- rnorm(n, mean = mn, sd = stddev)
  pvals[i] <- t.test(samp)$p.value
  tstat_standard[i] <- t.test(samp)$statistic
}
```

Type I error rate is about 5%, consistent with our expectations:

```{r}
round(mean(pvals < 0.05), 2)
```

But the situation quickly deteriorates as soon as we adopt the strategy outlined above. Below, we will also track the distribution of the t-statistic.

```{r}
pvals <- NULL
tstat <- NULL
## how many subjects can I run?
upper_bound <- n * 6

for (i in 1:nsim) {
  significant <- FALSE
  x <- rnorm(n, mean = mn, sd = stddev) ## take sample
  while (!significant & length(x) < upper_bound) {
    ## if not significant:
    if (t.test(x)$p.value > 0.05) {
      x <- append(x, rnorm(n, mean = mn, sd = stddev)) ## get more data
    } else {
      significant <- TRUE
    } ## otherwise stop:
  }
  pvals[i] <- t.test(x)$p.value
  tstat[i] <- t.test(x)$statistic
}
```

Now, Type I error rate is much higher than 5%:

```{r}
round(mean(pvals < 0.05), 2)
```

Figure \@ref(fig:stoppingrule) shows the distributions of the t-statistic in the standard case vs with the above stopping rule:

```{r stoppingrule,echo=FALSE,fig.cap="A comparison of the distribution of t-values with an a priori fixed stopping rule, versus a flexible stopping rule conditional on finding significance."}
plot(density(tstat_standard), main = "", xlab = "t-value")
lines(density(tstat), lty = 2)
```

What is important to realize here is that the inflation in Type I error we observed above was due to the fact that the t-distribution is no longer a t-distribution:  we have bumps in the tails when we use the flexible stopping rule, and these raise our Type I error. This demonstrates why one should fix one's sample size in advance, based on a power analysis. One should not deploy a stopping rule like the one above; if we used such a stopping rule, we are much more likely to incorrectly declare a result as statistically significant than our intended Type I error rate of 0.05. 

There can be compelling reasons to adopt the peek-and-run strategy; e.g., if one wants to avoid exposing patients to a treatment that might turn out to be harmful. In such situations, one can run an adaptive experimental trial by correcting for Type I error inflation [@pocock2013clinical]. In this book, we will aim to develop a workflow whereby the sample size is fixed through power analysis, in advance of running an experiment.

## The two-sample t-test vs. the paired t-test

In our running example above, we examined the case where we have a single vector of data $y$. This led to the one-sample t-test. 

Next, we consider a case where we have two vectors of data. The data-set below is from @johnson2011quantitative.  Shown below are F1 formant data (in Hertz) for different vowels produced by  male and female speakers of different languages. (In a speech wave, different bands of energy centered around particular frequencies are called formants.)

```{r}
library(lingpsych)
data("df_F1data")
F1data <- df_F1data
```

Notice that the male and female values can be seen as *dependent* or *paired*: each row belongs to the same vowel and language. Nevertheless, we can compare males' and females' F1 frequencies, completely ignoring this paired nature of the data. The t-test does not "know" whether these data are paired or not---it is the researcher's job to make sure that model assumptions are met. In this case, the assumption of the t-test is that the data are independent. 

Let's ignore the paired nature of the data for now, and treat the two vectors as independent vectors. Suppose that our null hypothesis is that there is no difference between the mean F1's for males ($\mu_m$) and females ($\mu_f$).
Now, our null hypothesis is $H_0: \mu_m = \mu_f$ or $H_0: \mu_m - \mu_f = \delta = 0$.

This kind of design calls for a two-sample t-test. 

The function call in R for a two-sample t-test is shown below. Note here that we are assuming that both the male and female F1 scores have equal variance.

```{r}
t.test(F1data$female, F1data$male,
  paired = FALSE,
  var.equal = TRUE
)
```

This t-test is computing the following t-statistic:

\begin{equation}
t=\frac{d-(\mu_m - \mu_f)}{SE} = \frac{d-0}{SE} 
\end{equation}

\noindent 
where $d$ is the difference between the two sample means; the rest of the terms we are familiar with. SE is the standard error of the sampling distribution of the difference between the means.

We will now do this calculation "by hand". The only new things are the formula for the SE calculation, and the degrees of freedom for t-distribution $(2\times n - 2)=36$.

The standard error for the difference in the means in the two-sample t-test is computed using this formula:

\begin{equation}
SE_\delta 
= \sqrt{\frac{\hat\sigma_m^2}{n_m} + \frac{\hat\sigma_f^2}{n_f}}
\end{equation}
  
Here, $\hat\sigma_m$ is an estimate of the standard deviation for males, and $\hat\sigma_f$  for the females; the $n$ are the respective sample sizes.
  
```{r}
n_m <- n_f <- 19
## difference of sample means:
d <- mean(F1data$female) - mean(F1data$male)
(SE <- sqrt(var(F1data$male) / n_m + var(F1data$female) / n_f))
(observed_t <- (d - 0) / SE)
## p-value:
2 * (1 - pt(observed_t, df = 36))
```
  
The output of the two-sample t-test and the hand-calculation above match up.

Now consider what will change once we take into account the fact that the data are paired. The two-sample  t-test now becomes a so-called paired t-test. 

For such  paired data,  the null hypothesis is as before: $H_0: \delta=0$. But since each row in the data-frame is paired (from the same vowel+language), we subtract the vector row-wise, and get a new *vector* $d$ (not a single number $d$ as in the two-sample t-test) with the row-wise differences. Then, we just do the familiar one-sample test we saw earlier:
  
```{r}
d <- F1data$female - F1data$male
t.test(d)
```

An alternative syntax for the paired t-test explicitly feeds the two paired vectors into the function, but one must explicitly specify that they are paired, otherwise the test is a two-sample (i.e., unpaired) t-test:

```{r}
t.test(F1data$female, F1data$male, paired = TRUE)
```
  
Incidentally, notice that the p-value in the paired t-test is statistically significant, unlike the two-sample t-test above. The null hypothesis is the same in both tests, but the significance level leads to different conclusions. 

Which analysis is correct, the two-sample t-test or the paired t-test? It all depends on your assumptions about what the data represent. If you consider the data paired, for the reasons given above, then a paired test is called for. If there is no pairing (here, domain knowledge is required), we can treat this as unpaired data. 

Next, we look at some perhaps subtle points about the paired t-test.

### Common mistakes involving the t-test

The paired t-test assumes that each row in the data-frame is independent of the other rows. This implies that the data-frame cannot have more than one row for a particular pair. In other words, the data-frame cannot have repeated measurements spread out across rows.  

For example, doing a paired t-test on this hypothetical data-frame would be incorrect:
  
\begin{table}[ht]
\centering
\begin{tabular}{rrrll}
\hline
female & male & vowel & language \\ 
\hline
391 & 339 & i & W.Apache \\ 
400 & 320 & i & W.Apache \\ 
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
\hline
\end{tabular}
\end{table}

Why? Because the assumption is that each row is independent of the others. This assumption is violated here (this is assuming that repeating the vowel from the same language will lead to some commonalities between the two repetitions).

Consider another hypothetical example. In the table below, from subject 1 we see two  data points each for condition a and for condition b. 
  
  \begin{table}[ht]
\centering
\begin{tabular}{rrrll}
\hline
condition a & condition b & subject & item \\ 
\hline
391 & 339 & 1 & 1 \\ 
400 & 320 & 1 & 2 \\ 
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
\hline
\end{tabular}
\end{table}

Here, we again have repeated measurements from subject 1. The independence assumption is violated.

How to proceed when we have repeated measurements from each subject or each item?
The solution is to aggregate the data so that each subject (or item) has only *one* value for each condition.

This aggregation allows us to meet the independence assumption of the t-test, but it has a potentially huge drawback: it pretends we have one measurement from each subject for each condition. 
Later on we will learn how to analyze unaggregated data, but if we want to do a paired t-test, we have no choice but to aggregate the data in this way.

A fully worked example will make this clear. We have repeated measures data on subject versus object relative clauses in English. Subject relative clauses are sentences like *The man who was standing near the doorway laughed*. Here, the phrase (called a relative clause) *who was standing near the doorway* modifies the noun phrase *man*; it is called a subject relative because the noun phrase *man* is the subject of the relative clause. By contrast, object relative clauses are sentences like *The man who was the woman was talking to near the doorway laughed*; here, the *man* is the grammatical object object of the relative clause *who was the woman was talking to near the doorway*.

The data are from a self-paced reading study reported in @grodner, their experiment 1. A theoretical prediction is that in English, object relatives are harder to read than subject relatives, in the relative clause verb region. We want to test this prediction.

First, load the data containing reading times from the region of interest (the relative clause verb):

```{r}
## From the library lingpsych:
data("df_gg05e1")
gg05e1 <- df_gg05e1
head(gg05e1)
```

We have repeated measurements for each condition from the subjects, and from items. You can establish this by using the `xtabs` command. 
Notice that there are no missing data points:

```{r}
t(xtabs(~ subject + condition, gg05e1))
t(xtabs(~ item + condition, gg05e1))
```
  
  
It is important to stress once more that it is the researcher's responsibility to make sure that the t-test's assumptions are  met. For example, one could fit a two-sample t-test to the data as provided. The two-sample t-test can be implemented using the syntax shown below:

```{r}
t.test(rawRT ~ condition, gg05e1)
```

This t-test is incorrect for several reasons, but the most egregious error here is that the data are paired (each  subject delivers data for both  conditions), and that property of the data is being ignored. 

Another common mistake is to do a paired t-test on the data without checking that the data are independent in the sense discussed above. Again, the t.test function will happily return a meaningless result:
  
```{r}
t.test(rawRT ~ condition, paired = TRUE, gg05e1)
```
  
Here, the degrees of freedom indicate that we have fit the incorrect model. There are 42 subjects and 16 items, and the presentation of items to subjects uses a Latin square design (each subject sees only one condition per item). The 335 degrees of freedom come from $42\times 8=336$ data points, minus one. Why do we say $42\times 8$ and not $42\times 16$? That is because each subject will return eight differences in reading time for each condition: each subject gives us eight subject-relative data points and eight object-relative data points.  

For each of the 42 subjects, the t-test function internally creates a vector of eight data points of subject relatives and subtracts the vector of eight data points of object relatives. That is how we end up with  $42\times 8=336$ data points. 

These 336 data points are assumed by the t-test to be independent of each other; but this cannot be the case because each subject delivers eight data points for each condition; these are obviously dependent (correlated) because they come from the same subject. 

What is needed is a *single* data-point for each subject and condition, and for each item and condition. In order to conduct the t-test, aggregation of the data by subjects and by items is necessary. 

Consider the by-subjects aggregation procedure below. Now we have only one data-point for each condition and subject:

```{r}
bysubj <- aggregate(rawRT ~ subject + condition,
  mean,
  data = gg05e1
)
t(xtabs(~ subject + condition, bysubj))
```


Notice that the data are correlated: the longer the subject relative clause data from a participant, the longer their object relative clause data:

```{r}
SRdata <- subset(bysubj, condition == "subjgap")$rawRT
ORdata <- subset(bysubj, condition == "objgap")$rawRT
plot(SRdata, ORdata)
abline(lm(ORdata ~ SRdata))
cor(SRdata, ORdata)
```

Returning to the t-test, by aggregating the data the  independence assumption of the t-test is met, and the degrees of freedom for this by-subjects analysis are now correct ($42-1=41$):

```{r}
t.test(rawRT ~ condition, bysubj, paired = TRUE)
```

Similar to the by-subjects aggregation done above, one could do a by-items aggregation and then a by-items t-test (What should be the degrees of freedom for the by-items analysis? There are 16 items in this data-set). This is left as an exercise for the reader. 

The paired t-test illustrated above is actually not the best way to analyze this data-set, because it ignores the fact that each subject  delivers not one but eight data points per condition. Each subject's repeated measurements will introduce a source of variance, but this source of variance is being suppressed in this t-test, leading to a possibly over-enthusiastic t-value. In order to take this variability into account, we must switch to the linear mixed model. But before we get to the linear mixed model, we have to consider the linear model. The next chapter turns to this topic.

## Exercises {#sec:SamplingDistrnexercises}

### Practice using ```qt``` {#sec:SamplingDistrnexercisesqt}

```{r echo=FALSE}
n <- 142
mu <- 123
sigma <- 70
sample.sd <- 50.885
sample.mean <- 145.242
estimated.se <- round(sample.sd / sqrt(n), 3)
crit.t <- abs(round(qt(0.025, df = n - 1), 3))
lower <- round(sample.mean - crit.t * estimated.se, digits = 3)
upper <- round(sample.mean + crit.t * estimated.se, digits = 3)
```

Take an independent random sample of size ```r n``` from a normal distribution
with mean ```r mu```, and standard deviation ```r sigma```. Next, we are going to pretend we don't know the population parameters (the mean and standard deviation). We compute the MLEs of the mean and standard deviation using the data and get the sample mean ```r sample.mean``` and the sample standard deviation ```r sample.sd```. 

- Compute the estimated standard error using the sample standard deviation provided above. 
- What are your degrees of freedom for the relevant t-distribution?
- Calculate the **absolute** critical t-value for a 95\% confidence interval using the relevant degrees of freedom you just wrote above.
- Next, compute the lower bound of the 95\% confidence interval using the estimated standard error and the critical t-value.
- Finally, compute the upper bound of the 95\% confidence interval using the estimated standard error and the critical t-value.

### Computing the p-value {#sec:SamplingDistrnexercisesPart1}

A paired t-test is done with data from 10 participants. The t-value from the test is 2.1. What is the p-value associated with a two-sided null hypothesis test? 

```{r echo=FALSE,eval=FALSE}
round(2 * pt(-2.1, df = 9), 3)
```

### Computing the t-value {#sec:SamplingDistrnexercisesPart2}

 If the p-value from a two-sided null hypothesis test had been 0.09, what  would be the associated absolute t-value (i.e., ignoring the sign on the t-value)?  The number of participants is 10, as above.

### Type I and II error {#sec:SamplingDistrnexercisesPart3}

Given that Type I error is 0.01; what is the highest value possible for Type II error?  


### Practice with the paired t-test     

In a self-paced reading study, @grodner investigated subjects vs. object relative clauses. They analyzed the reading times  at the relative clause verb. However, a reviewer objects that the whole sentence's reading times (total reading times) should be used to evaluate the difference between the two conditions, because one cannot know where the difficulty might arise. It isn't clear whether one should use mean reading times over the entire sentence, or total reading times (summing up all the reading times over the entire sentence). 
Carry out a by-subjects paired t-test on (a) the critical relative clause verb, versus (b) mean reading time over all words in the two sentence types, and (c) total reading times over all words in the two sentence types. Compare the t-value across the three tests, and decide what the appropriate dependent variable might be (Note: there is no correct answer here).

The data are loaded and pre-processed as follows. The code below gives you the reading times for the data at the relative clause verb. You will have to work out how to obtain mean or total reading times for the whole sentence in each condition.

```{r}
## load data from lingpsych package:
data("df_gg05e1_full")
## get data from relative clause verb:
df_gge1crit <- subset(
  df_gg05e1_full,
  (condition == "objgap" &
    word_position == 6) |
    (condition == "subjgap" & word_position == 4)
)
```
