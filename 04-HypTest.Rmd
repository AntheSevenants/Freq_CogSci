# Hypothesis testing using the likelihood ratio test

We started the book with the t-test. There, we had the following general procedure:

- Given independent and identically distributed data $y$, define a null hypothesis relating to the parameter $\mu$, which represents the: $H_0:  \mu=\mu_0$.
- Compute the sample mean $\bar{y}$ and the standard error SE. 
- Reject the null hypothesis if the absolute value of $(\bar{y}-\mu_0)/SE$ is larger than the critical t-value (approximately $2$ for sample sizes larger than 20).

The two main classes of t-test discussed in chapter 2, paired and unpaired, are basically just the same t-test. The paired/unpaired nature of the t-test affects the way the standard error is computed in paired vs. unpaired data.

Here, we turn to a closely related test: the *likelihood ratio test statistic*. The reason for considering this alternative test is that in reality we will almost never use the t-test, due to its many limitations.

## The likelihood ratio test: The theory

Suppose that $X_1,\dots, X_n$ are independent and normally distributed with mean $\mu$ and standard deviation $\sigma$ (assume for simplicity that $\sigma$ is known). 

Let the null hypothesis be $H_0: \mu=\mu_0$ and the alternative be $H_1: \mu\neq \mu_0$. Here, $\mu_0$ is a number, such as $0$.

The likelihood of the  data $y$ can be computed under the null model, in which $\mu=\mu_0$, and under the alternative model, in which $\mu$ has some specific alternative value. To make this concrete, imagine 10 data points being generated from a Normal(0,1).

```{r}
y <- rnorm(10)
```

We can compute the joint likelihood under a null hypothesis that $\mu=0$:

```{r}
likNULL <- prod(dnorm(y, mean = 0, sd = 1))
likNULL
```

On the log scale, we would need to add the log likelihoods of each data point:

```{r}
loglikNULL <- sum(dnorm(y, mean = 0, sd = 1, log = TRUE))
loglikNULL
```

Similarly, we can compute the log likelihood with $\mu$ equal to the maximum likelihood estimate of $\mu$, the sample mean. 

```{r}
loglikALT <- sum(dnorm(y, mean = mean(y), sd = 1, log = TRUE))
loglikALT
```

Essentially, the likelihood ratio test compares the ratio of likelihoods of the two models; on the log scale, the difference in log likelihood is taken. This is because a ratio on the log scale becomes a difference. The likelihood ratio test then chooses the model with the higher log likelihood, provided that the higher likelihood is high enough (we will just make this more precise).  

One can specify the test in general terms as follows. Suppose that the likelihood is with respect to some parameter $\theta$. We can evaluate the likelihood at  $\mu_0$, the null hypothesis value of the parameter, and evaluate the likelihood using the maximum likelihood estimate $\hat\theta$ of the parameter, as we did above. The likelihood ratio can then be written as follows:

\begin{equation}
\Lambda = \frac{max_{\theta\in \omega_0}(lik(\theta))}{max_{\theta\in \omega_1}(lik(\theta))}
\end{equation}

where, $\omega_0=\{\mu_0\}$ and $\omega_1=\{\forall \mu \mid \mu\neq \mu_0\}$. The function max just selects the maximum value of  any choices of parameter values; in the case of the null hypothesis there is only one value, $\mu_0$. In the case of the alternative model, the maximum likelihood estimate $\hat\theta$ is the maximum value.

Now, assuming for simplicity that the data are coming from a normal distribution, the numerator of the likelihood ratio statistic is:

\begin{equation}
lik(\theta=\mu_0) = \frac{1}{(\sigma\sqrt{2\pi})^n} 
           exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)
\end{equation}

For the denominator, the MLE $\bar{X}$ is taken as $\mu$:

\begin{equation}
lik(\theta=\bar{X}) =\frac{1}{(\sigma\sqrt{2\pi})^n} exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2 \right)
\end{equation}

The likelihood ratio statistic is then:

\begin{equation}
\Lambda = \frac{lik(\theta=\mu_0)}{lik(\theta=\bar{X})}=
\frac{\frac{1}{(\sigma\sqrt{2\pi})^n} 
           exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)}{\frac{1}{(\sigma\sqrt{2\pi})^n} 
           exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2  \right)}
\end{equation}

Canceling out common terms:

\begin{equation}
\Lambda = 
\frac{exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)}{
        exp\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2  \right)}
\end{equation}


Taking logs and pulling out the common term:

\begin{equation}
\begin{split}
\log \Lambda =& 
\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu_0)^2  \right)-\left( -\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \bar{X})^2  \right)\\
=& -\frac{1}{2\sigma^2} \left( \sum_{i=1}^n (X_i - \mu_0)^2  -  \sum_{i=1}^n (X_i - \bar{X})^2 \right)\\
\end{split}
\end{equation}

Now, it is a standard algebraic trick (see Box 4.1.1) to  rewrite  $\sum_{i=1}^n (X_i -\mu_0)^2$ as a sum of two terms:

\begin{equation}
\sum_{i=1}^n (X_i -\mu_0)^2 = \sum_{i=1}^n (X_i - \bar{X})^2 + n(\bar{X} - \mu_0)^2 
(\#eq:simplification)
\end{equation}

:::: {.blackbox data-latex=""}
::: {data-latex=""}
**An algebraic trick**
:::

We are going to establish that equality \@ref(eq:simplification) holds.

Consider this expression: $\sum_{i=1}^n (X_i -\mu_0)^2$. 

We can rewrite the terms inside the brackets in the following way:

\begin{equation}
\sum_{i=1}^n (X_i - \bar{X} + \bar{X} -\mu_0)^2
\end{equation}

Notice that $-\bar{X} + \bar{X}$ sums to 0, so the expression remains unchanged!

Now, collect together the two pairs of terms inside the brackets as follows:

\begin{equation}
\sum_{i=1}^n ((X_i - \bar{X}) + (\bar{X} -\mu_0))^2
\end{equation}

What we are doing here is that we are treating $X_i - \bar{X}$ as one term, say $a_i$ (we have to use the index $i$ because of the index inside the brackets), and $\bar{X} -\mu_0$ as another term, say $b$. So the above equation can be written simply as:

\begin{equation}
\sum_{i=1}^n (a_i + b)^2 = \sum_{i=1}^n (a_i^2 + b^2 + 2a_i b)
\end{equation}

Opening the brackets, we would rewrite this as:

\begin{equation}
\sum_{i=1}^n a_i^2 + \sum_{i=1}^nb^2 + 2\sum_{i=1}^n a_i b
\end{equation}

Instead of using $a_i$ and $b$, we use the full expressions now:

\begin{equation}
\sum_{i=1}^n ((X_i - \bar{X})^2 + (\bar{X} -\mu_0)^2 + 2 (X_i - \bar{X})(\bar{X} -\mu_0) )
\end{equation}

If we remove the outside-most brackets, we have to move the summation term $\sum_{i=1}^n$ to each of the expressions inside the brackets:

\begin{equation}
\sum_{i=1}^n (X_i - \bar{X})^2 +\sum_{i=1}^n (\bar{X} -\mu_0)^2 + 
2\sum_{i=1}^n (X_i - \bar{X})(\bar{X} -\mu_0) )
\end{equation}

But $\sum_{i=1}^n (\bar{X} -\mu_0)^2=n (\bar{X} -\mu_0)^2$, so we can rewrite the equation as:


\begin{equation}
\sum_{i=1}^n (X_i - \bar{X})^2 +n(\bar{X} -\mu_0)^2 + 
2\sum_{i=1}^n \times (X_i - \bar{X})(\bar{X} -\mu_0) )
\end{equation}

Also, $\sum_{i=1}^n 2 (X_i - \bar{X}) = 0$, because the sum of all the deviations of the data $x_i$ from the mean has to sum to 0. This follows because the definition of the mean is $\frac{\sum_i^n X_i}{n} = \bar{X}$, and that means that $\sum_i^n X_i = n\bar{X}$, or $X_1-\bar{X} + X_2 - \bar{X} \dots + X_n -\bar{X}=0$.

The fact that  $2 \sum_{i=1}^n  (X_i - \bar{X}) = 0$ allows us to simplify 

\begin{equation}
\sum_{i=1}^n (X_i - \bar{X})^2 +n(\bar{X} -\mu_0)^2)
\end{equation}

This proves that 

\begin{equation}
\sum_{i=1}^n (X_i -\mu_0)^2 = \sum_{i=1}^n (X_i - \bar{X})^2 + n(\bar{X} - \mu_0)^2 
\end{equation}

::::

Returning to equation \@ref(eq:simplification), if we rearrange the terms, we obtain:

\begin{equation}
\sum_{i=1}^n (X_i -\mu_0)^2 - \sum_{i=1}^n (X_i - \bar{X})^2 = n(\bar{X} - \mu_0)^2 
(\#eq:rearrange)
\end{equation}

Now, we just established above that $\log \Lambda$ is:

\begin{equation}
\log \Lambda= -\frac{1}{2\sigma^2} \left( \sum_{i=1}^n (X_i - \mu_0)^2  -  \sum_{i=1}^n (X_i - \bar{X})^2 \right)
(\#eq:loglambda)
\end{equation}

Consider the term in the brackets:

\begin{equation}
\left(\sum_{i=1}^n (X_i - \mu_0)^2  -  \sum_{i=1}^n (X_i - \bar{X})^2\right)
\end{equation}

Given equation \@ref(eq:rearrange), this can be rewritten as:

\begin{equation}
n(\bar{X} - \mu_0)^2
\end{equation}

Rewriting equation \@ref(#eq:loglambda) in this way gives us:

\begin{equation}
\log \Lambda = -\frac{1}{2\sigma^2}   n(\bar{X} - \mu_0)^2 
\end{equation}

Rearranging terms:

\begin{equation}
-2 \log \Lambda =    \frac{n(\bar{X} - \mu_0)^2 }{\sigma^2}
\end{equation}


Or even more transparently:

\begin{equation}
-2 \log \Lambda =    \frac{(\bar{X} - \mu_0)^2 }{\frac{\sigma^2}{n}}
\end{equation}

This should remind you of the t-test; the above equation is the square of the formula for the t-distribution.

Basically, just like in the t-test, what this is saying is that we reject the null when the distance between $\bar{X}$ and  $\mu_0$ (normalized by the square of the standard error) is large.  In other words, we reject the null hypothesis when negative two times the difference in log likelihood, is large.

Now we will  define what it means for $-2\log \Lambda$ to be large. There is a theorem in statistics that states that for large $n$, the distribution of $-2\log \Lambda$ approaches the chi-squared distribution, with degrees of freedom corresponding to the difference in the number of parameters between the two models being compared.

We will define the *likelihood ratio test statistic*, call it $LRT$, as follows. Here, $Lik(\theta)$ refers to the likelihood given some value  $\theta$ for the parameter, and 
$logLik(\theta)$  refers to the log likelihood. 

\begin{equation}
\begin{split}
LRT =& -2\times (Lik(\theta_0)/Lik(\theta_1)) \\
\log LRT =& -2\times \{logLik(\theta_0)-logLik(\theta_1)\}\\
\end{split}
\end{equation}

where $\theta_1$ and $\theta_0$ are the estimates of $\theta$ under the alternative and null hypotheses, respectively. The likelihood ratio test rejects $H_0$ if $\log LRT$ is sufficiently large. As the sample size approaches infinity, $\log LRT$ approaches the chi-squared  distribution:

\begin{equation}
\log LRT \rightarrow \chi_r^2  \hbox{ as }  n \rightarrow \infty
\end{equation}

Here, $r$ is called the degrees of freedom and is the difference in the number of parameters under the null and alternative hypotheses.

The above result is called *Wilks' theorem*. The proof of Wilks' theorem is fairly involved but you can find it  in Lehmann's textbook *Testing Statistical Hypotheses*.

Note that sometimes you will see the form:

\begin{equation}
 \log LRT = 2 \{logLik(\theta_1) - logLik(\theta_0)\}
\end{equation}

It should be clear that both statements are saying the same thing; in the second case, we are just subtracting the null hypothesis log likelihood from the alternative hypothesis log likelihood, so the negative sign disappears.

That's the theory. Let's see how the likelihood ratio test works for (a)  simulated data, and (b) our running example, the English relative clause data from  @grodner.

## A practical example using simulated data

A practical example will make the usage of this test clear.
Let's just simulate data from a linear model:

```{r}
x <- 1:10
y <- 10 + 20 * x + rnorm(10, sd = 10)
```

Here, the null hypothesis that the slope is 0 is  false (it has value 20). Now, we fit a null hypothesis model, without a slope:

```{r}
## null hypothesis model:
m0 <- lm(y ~ 1)
```

We will compare this model's log likelihood with that of the alternative model, which includes an estimate of the slope:

```{r}
## alternative hypothesis model:
m1 <- lm(y ~ x)
```

The difference in log likelihood, multiplied with -2, is:

```{r}
LogLRT <- -2 * (logLik(m0) - logLik(m1))
## observed value:
LogLRT[1]
```

The difference in the number of parameters in the two models is one, so $\log LRT$ has the distribution $\chi_1^2$. Is the observed value  $`r  round(LogLRT[1],2)`$  unexpected under this distribution? We can calculate the probability of obtaining the likelihood ratio statistic we observed above, or a  value more extreme, given the $\chi_1^2$ distribution. 


```{r}
pchisq(LogLRT[1], df = 1, lower.tail = FALSE)
```

Just like the critical  t-value in the t-test, the critical chi-squared value here is:

```{r}
## critical value:
qchisq(0.95, df = 1)
```

If minus two times the observed difference in log likelihood is larger than this critical value, we reject the null hypothesis.


Note that in the likelihood test above, we are comparing one nested model against another: the null hypothesis model is nested inside the alternative hypothesis model.  What this means is that the alternative hypothesis model contains all the parameters in the null hypothesis model (i.e., the intercept) plus another one (the slope).

## A real-life example: The English relative  clause data

The likelihood ratio test is also the way that hypothesis testing is done with the linear mixed model. Here is how it works. Let's look again at the @grodner English relative clause data. The null hypothesis here refers to the slope parameter. When we have the sum contrast coding, the intercept $\beta_0$ refers to the grand mean, and the slope $\beta_1$ is the amount by which subject and object relative clause mean reading times deviate from the grand mean.  Testing the null hypothesis that $\beta_1$ is 0 amounts to testing whether there is any difference in means between the two relative clause types. This becomes clear if we consider the following. 

Let object relatives be coded as $+1$ and subject relatives as $-1$. Then, the mean reading time $\mu_{or}$ for object relatives in the linear mixed model is: 

\begin{equation}
\mu_{or}=\beta_0 + \beta_1
\end{equation}

Similarly, the mean reading time $\mu_{sr}$ for subject relatives is:

\begin{equation}
\mu_{sr}=\beta_0 - \beta_1
\end{equation}

If the null hypothesis is that $\mu_{or}-\mu_{sr}=0$, then this amounts to saying that:

\begin{equation}
(\beta_0 + \beta_1)-(\beta_0 - \beta_1)=0
\end{equation}

Removing the brackets gives us:

\begin{equation}
\beta_0 + \beta_1-\beta_0 + \beta_1 = 0
\end{equation}

This yields the equation:

\begin{equation}
2\beta_1= 0
(\#eq:nullhypsumcontrast)
\end{equation}


Dividing both sides of the equation by 2, we get the null hypothesis that $\beta_1=0$.

Incidentally, if we had rescaled the contrast coding to be not $\pm 1$ but $\pm 1/2$, the parameter $\beta_1$ would represent exactly the difference between the two means, and null hypothesis in equation \@ref(eq:nullhypsumcontrast) would have come out to be  $\beta_1= 0$. This is why it is sometimes better to recode the contrasts as $\pm 1/2$ rather than $\pm 1$. See @SchadEtAlcontrasts for details; we will discuss this in the contrast coding chapter as well.

Let's load the data, set up the contrast coding, and fit the null versus the alternative models. We will fit varying intercept and varying slopes for subject and item, without correlations for items. We don't attempt to fit a model with by-items varying intercepts and slopes with a correlation  because we would get a singularity in the variance covariance matrix. 

```{r}
library(lingpsych)
data("df_gg05e1")
gg05e1 <- df_gg05e1

gg05e1$so <- ifelse(gg05e1$condition == "objgap", 1, -1)
gg05e1$logrt <- log(gg05e1$rawRT)

library(lme4)
m0 <- lmer(logrt ~ 1 + (1 + so | subject) + (1 + so || item), gg05e1)
m1 <- lmer(logrt ~ 1 + so + (1 + so | subject) + (1 + so || item), gg05e1)
```

Notice that we keep all random effects in the null model. We say that the null model is nested inside the full model.

Next, we compare the two models' log likelihoods. There is a function in the `lme4` package that achieves that: the `anova` function:

```{r}
anova(m0, m1)
```

You can confirm from the output that the `Chisq` value shown is minus two times the difference in log likelihood of the two models. The p-value is computed using the chi-squared distribution with one degree of freedom because in the two models the difference in the number of parameters is one:

```{r}
round(pchisq(6.15, df = 1, lower.tail = FALSE), 3)
```

It is common in the psycholinguistics literature to use the t-value from the linear mixed model output to conduct the hypothesis test on the slope:

```{r}
summary(m1)$coefficients
```

The most general method for hypothesis testing is the likelihood ratio test shown above. One can use the t-test output from the linear mixed model for hypothesis testing, but this should be done only when the data are balanced. If there is lack of balance (e.g., missing data for whatever reason), the likelihood ratio test is the best way to proceed. In any case, when we talk about the evidence against the null hypothesis, the likelihood ratio test is the only reasonable way to talk about what evidence we have. See @Royall for more discussion of this point. The essence of Royall's point is that the most reasonable way to talk about the evidence in favor of a particular model is with reference to, i.e., relative to, a baseline model. 

One can also use the likelihood ratio test to evaluate whether a variance component should be included or not. For example, is the correlation parameter justified for the subjects random effects? Recall that we had a correlation of 0.58. Is this statistically significant? One can test this in the following way:

```{r}
m1 <- lmer(logrt ~ 1 + so + (1 + so | subject) + (1 + so || item), gg05e1)
m1NoCorr <- lmer(logrt ~ 1 + so + (1 + so || subject) + (1 + so || item), gg05e1)
anova(m1, m1NoCorr)
```

The test indicates that we can reject the null hypothesis that the correlation parameter is 0.  We will return to this parameter in the chapter on simulation.

A final point: the likelihood ratio has essentially the same logic as the t-test, and that includes the fact that the focus is on the rejection of the null. The null cannot be accepted in the face of a null result, unless there is a good case to be made that power is sufficiently high (we will investigate this point later, in the simulation chapter). Also, since the likelihood ratio depends on defining the alternative model using the maximum likelihood estimate (the sample mean), it suffers from exactly the same problem as the t-test (Type M errors in the face of low power). We will also investigate this point in the simulation chapter. 

## Summary

## Further reading

## Exercises {#sec:HypTestExercises}

```{exercise, HypTestExercisesChinese}
Chinese relative clauses
```

Load the following two data-sets:

```{r}
data("df_gibsonwu")
gibsonwu <- df_gibsonwu
data("df_gibsonwu2")
gibsonwu2 <- df_gibsonwu2
```

The data are taken from two experiments that investigate (inter alia) the effect of relative clause type on reading time in Chinese. The data are from @gibsonwu and @VasishthetalPLoSOne2013 respectively. The second data-set is a direct replication attempt of the first.

Chinese relative clauses are interesting theoretically because they are prenominal: the relative clause appears before the head noun.

<!--
\begin{exe}
\ex  \label{ex:chineseRCs}
\begin{xlist}
\item Subject relative
\gll [GAP$_i$ yaoqing fuhao de] guanyuan$_i$ xinhuaibugui \\
GAP invite tycoon DE official {have bad intentions}\\
\glt `The official who invited the tycoon has bad intentions.’
\item Object relative 
\gll [fuhao yaoqing GAP$_i$ de] guanyuan$_i$ xinhuaibugui \\
tycoon invite GAP DE official { have bad intentions}\\
\glt `The official who the tycoon invited has bad intentions.’
\end{xlist}
\end{exe}
-->

As discussed in @gibsonwu, the consequence of Chinese relative clauses being prenominal is that the distance between the gap in relative clause and the head noun is larger in subject relatives than object relatives. @hsiao03 were the first to suggest that the larger distance in subject relatives leads to longer reading time at the head noun. Under this view, the prediction is that subject relatives are harder to process than object relatives. If this is true, this is interesting because in most other languages that have been studied, subject relatives are easier to process than object relatives; so Chinese will be a very unusual exception cross-linguistically.

The data provided are for the critical region (the head noun). The experiment method is self-paced reading, so we have reading times in milliseconds. 

The research hypothesis is whether the difference in reading times between object and subject relative clauses is negative. For both data-sets, investigate this question by  (a) fitting a paired t-test (by-subjects and by items), (b) fitting the most complex linear mixed model you  can to the data and then interpreting result using the t-value as well as the likelihood ratio test.  What can we conclude  about the research question?

```{exercise, HypTestExerciseAgrmt}
Agreement attraction in comprehension
```

Load the following data:

```{r}
data("df_dillonE1")
dillonE1 <- df_dillonE1
head(dillonE1)
```

The data are taken from an experiment that investigate (inter alia) the effect of number similarity between a noun and the auxiliary verb in sentences like the following. There are two levels to a factor called Int(erference): low and high. 

(a) low: The key to the cabinet *are* on the table
(b) high: The key to the *cabinets* *are* on the table

Here, in (b), the auxiliary verb *are* is predicted to be read faster than in (a), because the plural marking on the noun *cabinets* leads the reader to think that the sentence is grammatical. (Note that both sentences are ungrammatical.) This phenomenon, where the high condition is read faster than the low condition, is called **agreement attraction**.

The data provided are for the critical region (the auxiliary verb *are*). The experiment method is eyetracking; we have total reading times in milliseconds. 

The research  question is whether the difference in reading times between high and low conditions is negative. 

- First, figure out  which linear mixed model is  appropriate for these data (varying intercepts only? varying intercepts and slopes? with or without correlations?).
- Then, carry out a statistical test using (a) the paired t-test (using the t.test function), (b) the t-test of the linear mixed model, and (c) the likelihood ratio test. What is your conclusion? Is there evidence for agreement attraction in the data? 

```{exercise, HypTestExerciseGramCE}
The grammaticality illusion
```

Load the following two data-sets:

```{r}
data("df_english")
english <- df_english
data("df_dutch")
dutch <- df_dutch
```

In an offline accuracy rating study on English double center-embedding constructions, @gibsonthomas99 found that grammatical constructions (e.g., example a below) were no less acceptable than ungrammatical constructions (e.g., example b) where a middle verb phrase (e.g., was cleaning every week) was missing.

(a) The apartment that the maid who the service had sent over was cleaning every week was well decorated. 

(b) *The apartment that the maid who the service had sent over --- was well decorated

Based on these results from English, @gibsonthomas99 proposed that working-memory overload leads the comprehender to forget the prediction of the upcoming verb phrase (VP), which reduces working-memory load. This came to be known as the *VP-forgetting hypothesis*. The prediction is that in the word immediately following  the final verb, the grammatical condition (which is coded as +1 in the data-frames) should be harder to read than the ungrammatical condition (which is coded as -1). 

The data provided above test this hypothesis using self-paced reading for English [@VSLK08], and for Dutch [@FrankEtAl2015]. The data provided are for the critical region (the noun phrase, labeled NP1, following the final verb).  We have reading times in log milliseconds. 

Is there support for the VP-forgetting hypothesis cross-linguistically, from English and Dutch?
